{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77770e61",
   "metadata": {},
   "source": [
    "# Transformer Encoder-Only - GCN Embeddings Concatenado (Sistema G2)\n",
    "\n",
    "**Objetivo:** Clasificaci√≥n de secuencias temporales con Transformer encoder-only usando embeddings GCN concatenados.\n",
    "\n",
    "**Dataset:** 864 videos, 96 frames, 128 features/frame (GCN embeddings concatenados)  \n",
    "**Clases:** 30 (ASL - American Sign Language)  \n",
    "**Hardware:** RTX 5050 Laptop (8GB) / GTX 1660 Super (6GB)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Sistema de Gesti√≥n de Experimentos G2\n",
    "\n",
    "Este notebook gener√≥ los siguientes resultados en esta ejecuci√≥n:\n",
    "- `G2-RESULTS/` - Baseline (Dropout 0.1, Test Accuracy: **0.8786**, Macro-F1: **0.8486**, Top-3: **0.9884**)\n",
    "- `G2-RESULTS-CLASS-WEIGHTS/` - Class Weights + Dropout 0.3 (Test Accuracy: **0.8786**, Macro-F1: **0.8398**, Top-3: **0.9827**)\n",
    "- `G2-RESULTS-LABEL-SMOOTH/` - Label Smoothing 0.1 + Dropout 0.3 (Test Accuracy: **0.8728**, Macro-F1: **0.8244**, Top-3: **0.9711**)\n",
    "- Archivo de comparaci√≥n en ROOT_PATH: `experiments_comparison.csv`\n",
    "\n",
    "Cada experimento genera 10 archivos: best_model.pt, config.json, training_log.csv, metrics.csv, per_class_metrics.csv, confusion_matrix.csv, confusion_matrix.png, training_curves.png, per_class_analysis.png, RESUMEN.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e62ff0",
   "metadata": {},
   "source": [
    "## üìÅ Sistema de Detecci√≥n Autom√°tica de Rutas (G2)\n",
    "\n",
    "El notebook detecta autom√°ticamente en qu√© carpeta se encuentra y configura las rutas:\n",
    "\n",
    "- Si detecta `UMAP` en la ruta ‚Üí Usa carpeta `G2-UMAP SEGMENTS SEPARADO/`\n",
    "- Si detecta `CONCATENADO` ‚Üí Usa carpeta `G2-GCN EMBEDDINGS CONCATENADO/`\n",
    "- Si detecta `SEPARADO` ‚Üí Usa carpeta `G2-GCN_EMBEDDINGS_SEPARADO/`\n",
    "\n",
    "Los archivos se guardan dentro de subcarpetas seg√∫n el experimento:\n",
    "- `G2-RESULTS` (baseline)\n",
    "- `G2-RESULTS-CLASS-WEIGHTS` (experimento 1)\n",
    "- `G2-RESULTS-LABEL-SMOOTH` (experimento 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59df925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "VRAM: 8.55 GB\n",
      "\n",
      "================================================================================\n",
      "üéØ SISTEMA DE DETECCI√ìN AUTOM√ÅTICA DE RUTAS G2\n",
      "================================================================================\n",
      "üìÇ Notebook detectado: transformer-asl-classification\n",
      "üîç Modo detectado: CONCATENADO\n",
      "üìÅ ROOT_PATH: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\n",
      "‚úÖ Detecci√≥n autom√°tica: ACTIVADA\n",
      "================================================================================\n",
      "\n",
      "‚öôÔ∏è  CONFIGURACI√ìN ACTUAL:\n",
      "  ‚Ä¢ Tipo de experimento: baseline\n",
      "  ‚Ä¢ Descripci√≥n: Baseline - Dropout 0.1\n",
      "  ‚Ä¢ Directorio de salida: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\\G2-RESULTS\n",
      "  ‚Ä¢ Dropout: 0.1\n",
      "  ‚Ä¢ Class Weights: False\n",
      "  ‚Ä¢ Label Smoothing: 0.0\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, top_k_accuracy_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# üîß CONFIGURACI√ìN AUTOM√ÅTICA DE RUTAS Y EXPERIMENTOS (G2)\n",
    "BASE_PATHS = {\n",
    "    'umap': Path(r'C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-UMAP SEGMENTS SEPARADO'),\n",
    "    'concatenado': Path(r'C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO'),\n",
    "    'separado': Path(r'C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS SEPARADO')\n",
    "}\n",
    "\n",
    "# Detectar autom√°ticamente el tipo de experimento\n",
    "current_notebook = Path.cwd()\n",
    "notebook_name_lower = str(current_notebook).lower()\n",
    "\n",
    "if 'umap' in notebook_name_lower:\n",
    "    DETECTED_MODE = 'umap'\n",
    "elif 'concatenado' in notebook_name_lower:\n",
    "    DETECTED_MODE = 'concatenado'\n",
    "elif 'separado' in notebook_name_lower:\n",
    "    DETECTED_MODE = 'separado'\n",
    "else:\n",
    "    DETECTED_MODE = 'concatenado'  # Default para este notebook\n",
    "\n",
    "ROOT_PATH = BASE_PATHS[DETECTED_MODE]\n",
    "AUTO_DETECTED = True\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üéØ SISTEMA DE DETECCI√ìN AUTOM√ÅTICA DE RUTAS G2\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"üìÇ Notebook detectado: {current_notebook.name}\")\n",
    "print(f\"üîç Modo detectado: {DETECTED_MODE.upper()}\")\n",
    "print(f\"üìÅ ROOT_PATH: {ROOT_PATH}\")\n",
    "print(f\"‚úÖ Detecci√≥n autom√°tica: {'ACTIVADA' if AUTO_DETECTED else 'DESACTIVADA'}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Configuraciones de experimentos (G2)\n",
    "EXPERIMENT_CONFIGS = {\n",
    "    'baseline': {\n",
    "        'folder_name': 'G2-RESULTS',\n",
    "        'description': 'Baseline - Dropout 0.1',\n",
    "        'dropout': 0.1,\n",
    "        'use_class_weights': False,\n",
    "        'label_smoothing': 0.0\n",
    "    },\n",
    "    'class_weights': {\n",
    "        'folder_name': 'G2-RESULTS-CLASS-WEIGHTS',\n",
    "        'description': 'Class Weights + Dropout 0.3',\n",
    "        'dropout': 0.3,\n",
    "        'use_class_weights': True,\n",
    "        'label_smoothing': 0.0\n",
    "    },\n",
    "    'label_smoothing': {\n",
    "        'folder_name': 'G2-RESULTS-LABEL-SMOOTH',\n",
    "        'description': 'Label Smoothing 0.1 + Dropout 0.3',\n",
    "        'dropout': 0.3,\n",
    "        'use_class_weights': False,\n",
    "        'label_smoothing': 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Seleccionar experimento (MODIFICAR AQU√ç PARA CAMBIAR EXPERIMENTO)\n",
    "EXPERIMENT_TYPE = 'baseline'  # Opciones: 'baseline', 'class_weights', 'label_smoothing'\n",
    "\n",
    "current_config = EXPERIMENT_CONFIGS[EXPERIMENT_TYPE]\n",
    "output_dir = ROOT_PATH / current_config['folder_name']\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚öôÔ∏è  CONFIGURACI√ìN ACTUAL:\")\n",
    "print(f\"  ‚Ä¢ Tipo de experimento: {EXPERIMENT_TYPE}\")\n",
    "print(f\"  ‚Ä¢ Descripci√≥n: {current_config['description']}\")\n",
    "print(f\"  ‚Ä¢ Directorio de salida: {output_dir}\")\n",
    "print(f\"  ‚Ä¢ Dropout: {current_config['dropout']}\")\n",
    "print(f\"  ‚Ä¢ Class Weights: {current_config['use_class_weights']}\")\n",
    "print(f\"  ‚Ä¢ Label Smoothing: {current_config['label_smoothing']}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75322abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üßπ LIMPIEZA DE CARPETAS PREVIAS\n",
      "================================================================================\n",
      "  ‚úì Eliminado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\\G2-RESULTS\n",
      "  ‚úì Eliminado (ra√≠z): c:\\Users\\isaiy\\Documents\\modelo prueba embedings\\transformer-asl-classification\\g5.0_umap\n",
      "‚úÖ Limpieza completada (2 carpetas eliminadas)\n",
      "üìÅ Directorio de salida recreado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\\G2-RESULTS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üßπ LIMPIEZA DE CARPETAS PREVIAS\n",
    "# Esta celda elimina carpetas de experimentos previos antes de ejecutar los 3 experimentos\n",
    "\n",
    "import shutil\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üßπ LIMPIEZA DE CARPETAS PREVIAS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Carpetas de resultados G2 a limpiar\n",
    "results_folders = [\n",
    "    'G2-RESULTS',\n",
    "    'G2-RESULTS-CLASS-WEIGHTS',\n",
    "    'G2-RESULTS-LABEL-SMOOTH'\n",
    "]\n",
    "\n",
    "# Limpiar dentro de ROOT_PATH\n",
    "cleaned_count = 0\n",
    "for folder in results_folders:\n",
    "    folder_path = ROOT_PATH / folder\n",
    "    if folder_path.exists():\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"  ‚úì Eliminado: {folder_path}\")\n",
    "        cleaned_count += 1\n",
    "\n",
    "# Tambi√©n limpiar carpetas antiguas en el directorio ra√≠z del proyecto\n",
    "project_root = Path.cwd()\n",
    "old_folders = [\n",
    "    'g5.0_umap',\n",
    "    'g6_class_weights',\n",
    "    'g7_label_smooth',\n",
    "    'umap_baseline',\n",
    "    'results_umap',\n",
    "    'output_videos',\n",
    "    'temp_results',\n",
    "    'old_results',\n",
    "    'results'\n",
    "]\n",
    "\n",
    "for folder in old_folders:\n",
    "    folder_path = project_root / folder\n",
    "    if folder_path.exists():\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"  ‚úì Eliminado (ra√≠z): {folder_path}\")\n",
    "        cleaned_count += 1\n",
    "\n",
    "if cleaned_count == 0:\n",
    "    print(\"  ‚ÑπÔ∏è  No hay carpetas previas para limpiar\")\n",
    "\n",
    "print(f\"‚úÖ Limpieza completada ({cleaned_count} carpetas eliminadas)\")\n",
    "\n",
    "# Recrear el directorio de salida actual despu√©s de la limpieza\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ Directorio de salida recreado: {output_dir}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "693fec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claves disponibles en el archivo .npz:\n",
      "['X', 'y', 'filenames', 'class_names', 'masks']\n",
      "\n",
      "X:\n",
      "  Tipo: <class 'numpy.ndarray'>\n",
      "  Forma: (864, 96, 128)\n",
      "\n",
      "y:\n",
      "  Tipo: <class 'numpy.ndarray'>\n",
      "  Forma: (864,)\n",
      "\n",
      "filenames:\n",
      "  Tipo: <class 'numpy.ndarray'>\n",
      "  Forma: (864,)\n",
      "\n",
      "class_names:\n",
      "  Tipo: <class 'numpy.ndarray'>\n",
      "  Forma: (30,)\n",
      "\n",
      "masks:\n",
      "  Tipo: <class 'numpy.ndarray'>\n",
      "  Forma: (864, 96)\n"
     ]
    }
   ],
   "source": [
    "# 1. CARGAR DATASET UMAP\n",
    "dataset_path = Path('./daataset/G2_GCN_embeddings_concatenado_128d.npz')\n",
    "data = np.load(dataset_path, allow_pickle=True)\n",
    "\n",
    "# Inspeccionar las claves disponibles\n",
    "print(\"Claves disponibles en el archivo .npz:\")\n",
    "print(data.files)\n",
    "\n",
    "# Mostrar tambi√©n el tipo y forma de cada array\n",
    "for key in data.files:\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"  Tipo: {type(data[key])}\")\n",
    "    if hasattr(data[key], 'shape'):\n",
    "        print(f\"  Forma: {data[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a48334bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (864, 96, 128)\n",
      "y: (864,)\n",
      "Filenames: (864,)\n",
      "Class names: (30,)\n",
      "Masks: (864, 96)\n",
      "\n",
      "Dataset shape: X=(864, 96, 128), y=(864,)\n",
      "Masks shape: (864, 96)\n",
      "Filenames: 864\n",
      "Classes: 30\n",
      "Categor√≠as: ['Adi√≥s', 'Buenas noches', 'Buenas tardes', 'Buenos d√≠as', 'Clase', 'Comenzar', 'Compa√±ero', 'Cuaderno', 'C√≥mo est√°', 'Deberes', 'Disculpa', 'Entender', 'Escribir', 'Escuchar', 'Estudiante', 'Examen', 'Explicar', 'Gracias', 'Hola', 'Lecci√≥n', 'Leer', 'Libro', 'L√°piz', 'Mucho gusto', 'Pizarr√≥n', 'Por favor', 'Pregunta', 'Profesor', 'Responder', 'Terminar']\n",
      "\n",
      "Distribuci√≥n de clases:\n",
      "  Clase 0 (Adi√≥s): 76 muestras\n",
      "  Clase 1 (Buenas noches): 10 muestras\n",
      "  Clase 2 (Buenas tardes): 22 muestras\n",
      "  Clase 3 (Buenos d√≠as): 22 muestras\n",
      "  Clase 4 (Clase): 18 muestras\n",
      "  Clase 5 (Comenzar): 18 muestras\n",
      "  Clase 6 (Compa√±ero): 28 muestras\n",
      "  Clase 7 (Cuaderno): 18 muestras\n",
      "  Clase 8 (C√≥mo est√°): 22 muestras\n",
      "  Clase 9 (Deberes): 16 muestras\n",
      "  Clase 10 (Disculpa): 12 muestras\n",
      "  Clase 11 (Entender): 18 muestras\n",
      "  Clase 12 (Escribir): 28 muestras\n",
      "  Clase 13 (Escuchar): 24 muestras\n",
      "  Clase 14 (Estudiante): 28 muestras\n",
      "  Clase 15 (Examen): 18 muestras\n",
      "  Clase 16 (Explicar): 18 muestras\n",
      "  Clase 17 (Gracias): 82 muestras\n",
      "  Clase 18 (Hola): 76 muestras\n",
      "  Clase 19 (Lecci√≥n): 18 muestras\n",
      "  Clase 20 (Leer): 28 muestras\n",
      "  Clase 21 (Libro): 18 muestras\n",
      "  Clase 22 (L√°piz): 18 muestras\n",
      "  Clase 23 (Mucho gusto): 22 muestras\n",
      "  Clase 24 (Pizarr√≥n): 18 muestras\n",
      "  Clase 25 (Por favor): 68 muestras\n",
      "  Clase 26 (Pregunta): 22 muestras\n",
      "  Clase 27 (Profesor): 64 muestras\n",
      "  Clase 28 (Responder): 18 muestras\n",
      "  Clase 29 (Terminar): 16 muestras\n"
     ]
    }
   ],
   "source": [
    "# Cargar los arrays con las claves correctas\n",
    "X = data['X']                  # (864, 96, 128)\n",
    "y = data['y']                  # (864,)\n",
    "filenames = data['filenames']  # (864,)\n",
    "class_names = data['class_names']  # (30,)\n",
    "masks = data['masks']           # (864, 96)\n",
    "\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {y.shape}\")\n",
    "print(f\"Filenames: {filenames.shape}\")\n",
    "print(f\"Class names: {class_names.shape}\")\n",
    "print(f\"Masks: {masks.shape}\")\n",
    "\n",
    "# Asegurar tipos esperados\n",
    "y = y.astype(int)\n",
    "class_names = list(class_names)\n",
    "\n",
    "print(f\"\\nDataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Masks shape: {masks.shape}\")\n",
    "print(f\"Filenames: {len(filenames)}\")\n",
    "print(f\"Classes: {len(class_names)}\")\n",
    "print(f\"Categor√≠as: {class_names}\")\n",
    "\n",
    "# Informaci√≥n de las clases\n",
    "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "print(f\"\\nDistribuci√≥n de clases:\")\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    class_label = class_names[cls] if cls < len(class_names) else f\"Class {cls}\"\n",
    "    print(f\"  Clase {cls} ({class_label}): {count} muestras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4eb70d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (552, 96, 128), Val: (139, 96, 128), Test: (173, 96, 128)\n",
      "Batches - Train: 69, Val: 18, Test: 22\n"
     ]
    }
   ],
   "source": [
    "# 2. DATASET PYTORCH\n",
    "class VideoTransformerDataset(Dataset):\n",
    "    def __init__(self, X, y, masks):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.masks = torch.BoolTensor(masks)  # True = v√°lido, False = padding\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sequence': self.X[idx],      # (96, 300)\n",
    "            'label': self.y[idx],          # scalar\n",
    "            'mask': self.masks[idx]        # (96,)\n",
    "        }\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test, masks_train, masks_test = train_test_split(\n",
    "    X, y, masks, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val, masks_train, masks_val = train_test_split(\n",
    "    X_train, y_train, masks_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 8\n",
    "train_dataset = VideoTransformerDataset(X_train, y_train, masks_train)\n",
    "val_dataset = VideoTransformerDataset(X_val, y_val, masks_val)\n",
    "test_dataset = VideoTransformerDataset(X_test, y_test, masks_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Batches - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b2bdb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo:\n",
      "Total params: 2,203,294\n",
      "Trainable params: 2,203,294\n",
      "TransformerEncoderOnlyClassifier(\n",
      "  (input_projection): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (pos_encoding): LearnablePositionalEncoding()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=30, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3. ARCHITECTURE: TRANSFORMER ENCODER-ONLY\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding aprendible\"\"\"\n",
    "    def __init__(self, d_model, max_len=96):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "        nn.init.normal_(self.pe, mean=0, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerEncoderOnlyClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder-Only para clasificaci√≥n de secuencias temporales\n",
    "    - NO usa decoder\n",
    "    - Usa masked mean pooling\n",
    "    - Clasificaci√≥n global por secuencia\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=128,\n",
    "        d_model=256,\n",
    "        num_heads=4,\n",
    "        num_layers=4,\n",
    "        dim_feedforward=512,\n",
    "        dropout=0.1,\n",
    "        num_classes=30,\n",
    "        max_seq_len=96,\n",
    "        mlp_dropout=0.2,\n",
    "        activation='gelu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 1. Proyecci√≥n inicial (128 ‚Üí 256)\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # 2. Positional Encoding aprendible\n",
    "        self.pos_encoding = LearnablePositionalEncoding(d_model, max_seq_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 3. Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        # 4. Classification Head (MLP: 256 ‚Üí 128 ‚Üí num_classes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(mlp_dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch_size, seq_len, input_dim) = (B, 96, 128)\n",
    "            src_key_padding_mask: (batch_size, seq_len) = True para padding\n",
    "        Returns:\n",
    "            logits: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # 1. Proyecci√≥n inicial\n",
    "        x = self.input_projection(src)  # (B, 96, 256)\n",
    "        \n",
    "        # 2. Positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 3. Transformer encoder con m√°scara\n",
    "        x = self.transformer_encoder(\n",
    "            x,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )  # (B, 96, 256)\n",
    "        \n",
    "        # 4. Masked mean pooling (solo frames v√°lidos)\n",
    "        if src_key_padding_mask is not None:\n",
    "            # src_key_padding_mask: True = padding (ignorar)\n",
    "            # Convertir a float: 0 para padding, 1 para v√°lido\n",
    "            mask_float = (~src_key_padding_mask).float().unsqueeze(-1)  # (B, 96, 1)\n",
    "            x_masked = x * mask_float  # (B, 96, 256)\n",
    "            sum_masked = x_masked.sum(dim=1)  # (B, 256)\n",
    "            count_valid = mask_float.sum(dim=1)  # (B, 1)\n",
    "            x_pooled = sum_masked / (count_valid + 1e-9)  # (B, 256)\n",
    "        else:\n",
    "            # Sin m√°scara: mean pooling simple\n",
    "            x_pooled = x.mean(dim=1)  # (B, 256)\n",
    "        \n",
    "        # 5. Clasificador\n",
    "        logits = self.classifier(x_pooled)  # (B, num_classes)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Crear modelo\n",
    "num_classes = len(np.unique(y))\n",
    "model = TransformerEncoderOnlyClassifier(\n",
    "    input_dim=128,  # embeddings (X) con 128 features por frame\n",
    "    d_model=256,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.1,\n",
    "    num_classes=num_classes,\n",
    "    max_seq_len=96,\n",
    "    mlp_dropout=0.2,\n",
    "    activation='gelu'\n",
    ").to(device)\n",
    "\n",
    "# Contar par√°metros\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModelo:\")\n",
    "print(f\"Total params: {total_params:,}\")\n",
    "print(f\"Trainable params: {trainable_params:,}\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd128405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraci√≥n de entrenamiento:\n",
      "  optimizer: AdamW\n",
      "  lr: 0.0001\n",
      "  weight_decay: 0.0001\n",
      "  loss: CrossEntropyLoss\n",
      "  label_smoothing: 0.1\n",
      "  batch_size: 8\n",
      "  max_epochs: 50\n",
      "  early_stopping_patience: 8\n",
      "  gradient_clip: 1.0\n",
      "  num_classes: 30\n",
      "  input_dim: 128\n",
      "  dataset_type: GCN Embeddings\n",
      "  device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 4. ENTRENAMIENTO - CONFIG\n",
    "config = {\n",
    "    'optimizer': 'AdamW',\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'loss': 'CrossEntropyLoss',\n",
    "    'label_smoothing': 0.1,\n",
    "    'batch_size': 8,\n",
    "    'max_epochs': 50,\n",
    "    'early_stopping_patience': 8,\n",
    "    'gradient_clip': 1.0,\n",
    "    'num_classes': num_classes,\n",
    "    'input_dim': 128,\n",
    "    'dataset_type': 'GCN Embeddings',\n",
    "    'device': str(device)\n",
    "}\n",
    "\n",
    "# Loss con label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['lr'],\n",
    "    weight_decay=config['weight_decay']\n",
    " )\n",
    "\n",
    "# LR Scheduler\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10, T_mult=1, eta_min=1e-6\n",
    " )\n",
    "\n",
    "print(\"Configuraci√≥n de entrenamiento:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d3ec2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones de entrenamiento definidas ‚úì\n"
     ]
    }
   ],
   "source": [
    "# 5. FUNCIONES DE ENTRENAMIENTO Y EVALUACI√ìN\n",
    "def train_epoch(model, loader, criterion, optimizer, device, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        sequences = batch['sequence'].to(device)  # (B, 96, 128)\n",
    "        labels = batch['label'].to(device)        # (B,)\n",
    "        masks = batch['mask'].to(device)          # (B, 96)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(sequences, src_key_padding_mask=~masks)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = total_loss / len(loader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        sequences = batch['sequence'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        \n",
    "        logits = model(sequences, src_key_padding_mask=~masks)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_logits.extend(logits.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = total_loss / len(loader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc, np.array(all_preds), np.array(all_labels), np.array(all_logits)\n",
    "\n",
    "print(\"Funciones de entrenamiento definidas ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc0f4f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Iniciando entrenamiento - Epoch max: 50, Patience: 8\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50 | Train Loss: 3.0469 | Train Acc: 0.2065 | Val Loss: 2.6707 | Val Acc: 0.3669 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/50 | Train Loss: 1.8220 | Train Acc: 0.6123 | Val Loss: 1.8025 | Val Acc: 0.6187 | LR: 6.58e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/50 | Train Loss: 1.5246 | Train Acc: 0.7409 | Val Loss: 1.6176 | Val Acc: 0.6906 | LR: 3.42e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/50 | Train Loss: 1.1367 | Train Acc: 0.8895 | Val Loss: 1.2826 | Val Acc: 0.8058 | LR: 6.58e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/50 | Train Loss: 0.9842 | Train Acc: 0.9420 | Val Loss: 1.1618 | Val Acc: 0.8633 | LR: 3.42e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25/50 | Train Loss: 0.8731 | Train Acc: 0.9601 | Val Loss: 1.0323 | Val Acc: 0.8993 | LR: 6.58e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30/50 | Train Loss: 0.7806 | Train Acc: 0.9873 | Val Loss: 0.9732 | Val Acc: 0.9137 | LR: 3.42e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  35/50 | Train Loss: 0.7470 | Train Acc: 0.9909 | Val Loss: 0.9629 | Val Acc: 0.9137 | LR: 6.58e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40/50 | Train Loss: 0.7089 | Train Acc: 0.9982 | Val Loss: 0.9163 | Val Acc: 0.9281 | LR: 3.42e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  45/50 | Train Loss: 0.7018 | Train Acc: 0.9982 | Val Loss: 0.9028 | Val Acc: 0.9281 | LR: 6.58e-05\n",
      "\n",
      "Early stopping at epoch 45\n",
      "\n",
      "Mejor modelo cargado desde epoch 36 con Val Acc: 0.9353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# 6. ENTRENAMIENTO PRINCIPAL\n",
    "training_log = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "patience_counter = 0\n",
    "max_epochs = config['max_epochs']\n",
    "early_stopping_patience = config['early_stopping_patience']\n",
    "\n",
    "# Crear directorio para guardar modelos\n",
    "Path('./g5.0_umap').mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Iniciando entrenamiento - Epoch max: {max_epochs}, Patience: {early_stopping_patience}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Val\n",
    "    val_loss, val_acc, _, _, _ = eval_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # LR Scheduler\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log\n",
    "    training_log['epoch'].append(epoch)\n",
    "    training_log['train_loss'].append(train_loss)\n",
    "    training_log['train_acc'].append(train_acc)\n",
    "    training_log['val_loss'].append(val_loss)\n",
    "    training_log['val_acc'].append(val_acc)\n",
    "    training_log['lr'].append(current_lr)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        patience_counter = 0\n",
    "        # Guardar mejor modelo\n",
    "        best_model_path = Path('./g5.0_umap/best_model.pt')\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Print\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{max_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping trigger\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Cargar mejor modelo\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "print(f\"\\nMejor modelo cargado desde epoch {best_epoch} con Val Acc: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3187fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando en Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.9535\n",
      "Test Accuracy: 0.8786\n",
      "Macro-F1: 0.8486\n",
      "Macro-Precision: 0.9165\n",
      "Macro-Recall: 0.8328\n",
      "Top-3 Accuracy: 0.9884\n",
      "\n",
      "F1 Score por clase (primeras 5 clases):\n",
      "  Clase 0: 0.8667\n",
      "  Clase 1: 1.0000\n",
      "  Clase 2: 0.8571\n",
      "  Clase 3: 0.5714\n",
      "  Clase 4: 0.5000\n",
      "\n",
      "Matriz de confusi√≥n shape: (30, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# 7. EVALUACI√ìN EN TEST SET\n",
    "# Reimport sklearn metrics (caso se hayan sobreescrito)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"Evaluando en Test Set...\")\n",
    "test_loss, test_acc, test_preds, test_labels, test_logits = eval_epoch(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# M√©tricas adicionales\n",
    "macro_f1 = f1_score(test_labels, test_preds, average='macro', zero_division=0)\n",
    "macro_precision = precision_score(test_labels, test_preds, average='macro', zero_division=0)\n",
    "macro_recall = recall_score(test_labels, test_preds, average='macro', zero_division=0)\n",
    "top3_acc = top_k_accuracy_score(test_labels, test_logits, k=3, labels=np.arange(num_classes))\n",
    "\n",
    "print(f\"Macro-F1: {macro_f1:.4f}\")\n",
    "print(f\"Macro-Precision: {macro_precision:.4f}\")\n",
    "print(f\"Macro-Recall: {macro_recall:.4f}\")\n",
    "print(f\"Top-3 Accuracy: {top3_acc:.4f}\")\n",
    "\n",
    "# F1 por clase\n",
    "f1_per_class = f1_score(test_labels, test_preds, average=None, zero_division=0)\n",
    "print(f\"\\nF1 Score por clase (primeras 5 clases):\")\n",
    "for i in range(min(5, num_classes)):\n",
    "    print(f\"  Clase {i}: {f1_per_class[i]:.4f}\")\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "print(f\"\\nMatriz de confusi√≥n shape: {cm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2416346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Guardando resultados en C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\\G2-RESULTS...\n",
      "‚úì Guardado: training_log.csv\n",
      "‚úì Guardado: metrics.csv\n",
      "‚úì Guardado: per_class_metrics.csv\n",
      "‚úì Guardado: confusion_matrix.csv\n",
      "‚úì Guardado: config.json\n"
     ]
    }
   ],
   "source": [
    "# 8. GUARDAR RESULTADOS BASELINE\n",
    "# Crear directorio de salida\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\nüíæ Guardando resultados en {output_dir}...\")\n",
    "\n",
    "# 1. Training Log CSV\n",
    "pd.DataFrame(training_log).to_csv(output_dir / 'training_log.csv', index=False)\n",
    "print(f\"‚úì Guardado: training_log.csv\")\n",
    "\n",
    "# 2. Metrics CSV\n",
    "pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Macro-F1', 'Macro-Precision', 'Macro-Recall', 'Top-3 Accuracy', 'Test Loss'],\n",
    "    'Value': [test_acc, macro_f1, macro_precision, macro_recall, top3_acc, test_loss]\n",
    "}).to_csv(output_dir / 'metrics.csv', index=False)\n",
    "print(f\"‚úì Guardado: metrics.csv\")\n",
    "\n",
    "# 3. Per-class metrics\n",
    "# Extract unique class names (one per class ID)\n",
    "unique_class_names = []\n",
    "for class_id in range(num_classes):\n",
    "    idx = np.where(y == class_id)[0][0]\n",
    "    # Extract base name without extension\n",
    "    class_name = str(filenames[idx]).replace('.json', '').split('_')[0]\n",
    "    unique_class_names.append(class_name)\n",
    "\n",
    "class_report = classification_report(\n",
    "    test_labels, test_preds, \n",
    "    target_names=unique_class_names,\n",
    "    output_dict=True, zero_division=0\n",
    ")\n",
    "pd.DataFrame(class_report).T.to_csv(output_dir / 'per_class_metrics.csv')\n",
    "print(f\"‚úì Guardado: per_class_metrics.csv\")\n",
    "\n",
    "# 4. Confusion Matrix CSV\n",
    "pd.DataFrame(cm).to_csv(output_dir / 'confusion_matrix.csv', index=False, header=False)\n",
    "print(f\"‚úì Guardado: confusion_matrix.csv\")\n",
    "\n",
    "# 5. Config JSON\n",
    "model_config = {\n",
    "    'experiment_type': 'baseline',\n",
    "    'architecture': 'TransformerEncoderOnly',\n",
    "    'input_dim': config['input_dim'],\n",
    "    'd_model': 256,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'dim_feedforward': 1024,\n",
    "    'dropout': current_config['dropout'],\n",
    "    'num_classes': num_classes,\n",
    "    'max_seq_len': 96,\n",
    "    'use_class_weights': current_config['use_class_weights'],\n",
    "    'label_smoothing': current_config['label_smoothing'],\n",
    "    'best_epoch': int(best_epoch),\n",
    "    'best_val_acc': float(best_val_acc),\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_macro_f1': float(macro_f1),\n",
    "    'training_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "with open(output_dir / 'config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_config, f, indent=2, ensure_ascii=False)\n",
    "print(f\"‚úì Guardado: config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c82d843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé® Generando visualizaciones...\n",
      "‚úì Guardado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\\G2-RESULTS\\training_curves.png\n",
      "‚úì Curvas de aprendizaje generadas\n"
     ]
    }
   ],
   "source": [
    "# 9. VISUALIZACIONES - CURVAS DE APRENDIZAJE\n",
    "print(f\"\\nüé® Generando visualizaciones...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].plot(training_log['epoch'], training_log['train_loss'], 'b-', label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0, 0].plot(training_log['epoch'], training_log['val_loss'], 'r-', label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0, 0].axvline(best_epoch, color='g', linestyle='--', alpha=0.7, linewidth=2, label=f'Best Epoch {best_epoch+1}')\n",
    "axes[0, 0].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Curva de Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(training_log['epoch'], training_log['train_acc'], 'b-', label='Train Acc', marker='o', linewidth=2)\n",
    "axes[0, 1].plot(training_log['epoch'], training_log['val_acc'], 'r-', label='Val Acc', marker='s', linewidth=2)\n",
    "axes[0, 1].axvline(best_epoch, color='g', linestyle='--', alpha=0.7, linewidth=2, label=f'Best Epoch {best_epoch+1}')\n",
    "axes[0, 1].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Curva de Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(training_log['epoch'], training_log['lr'], 'g-', marker='o', linewidth=2)\n",
    "axes[1, 0].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Programaci√≥n de Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "metrics_names = ['Accuracy', 'Macro-F1', 'Top-3 Acc']\n",
    "metrics_values = [test_acc, macro_f1, top3_acc]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "bars = axes[1, 1].bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('M√©tricas en Test Set', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylim([0, 1.05])\n",
    "for bar, v in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02, \n",
    "                    f'{v:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(f'Curvas de Aprendizaje - {EXPERIMENT_TYPE.upper()}', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = output_dir / 'training_curves.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: {plot_path}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"‚úì Curvas de aprendizaje generadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f31507fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé® Generando matriz de confusi√≥n...\n",
      "‚úì Guardado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\\G2-RESULTS\\confusion_matrix.png\n",
      "‚úì Matriz de confusi√≥n completada\n"
     ]
    }
   ],
   "source": [
    "# 10. MATRIZ DE CONFUSI√ìN CON NOMBRES\n",
    "print(f\"\\nüé® Generando matriz de confusi√≥n...\")\n",
    "\n",
    "unique_classes_list = sorted(list(set(test_labels)))\n",
    "class_labels = [filenames[i] if i < len(filenames) else f'Class {i}' for i in unique_classes_list]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 18))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            cbar_kws={'label': 'Muestras'}, ax=ax,\n",
    "            xticklabels=class_labels, yticklabels=class_labels, \n",
    "            square=True)\n",
    "ax.set_xlabel('Clase Predicha', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Clase Real', fontsize=14, fontweight='bold')\n",
    "ax.set_title(f'Matriz de Confusi√≥n - {EXPERIMENT_TYPE.upper()} (Accuracy: {test_acc:.4f})', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "cm_plot_path = output_dir / 'confusion_matrix.png'\n",
    "plt.savefig(cm_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: {cm_plot_path}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"‚úì Matriz de confusi√≥n completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36e680b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé® Generando an√°lisis por clase...\n",
      "‚úì Guardado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\\G2-RESULTS\\per_class_analysis.png\n",
      "‚úì An√°lisis por clase completado\n"
     ]
    }
   ],
   "source": [
    "# 11. AN√ÅLISIS POR CLASE\n",
    "print(f\"\\nüé® Generando an√°lisis por clase...\")\n",
    "\n",
    "# Calcular precision, recall, f1 por clase\n",
    "precision_per_class = []\n",
    "recall_per_class = []\n",
    "f1_per_class_array = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    if i in unique_classes_list:\n",
    "        idx = unique_classes_list.index(i)\n",
    "        tp = cm[idx, idx]\n",
    "        fp = cm[:, idx].sum() - tp\n",
    "        fn = cm[idx, :].sum() - tp\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_value = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        precision_per_class.append(precision)\n",
    "        recall_per_class.append(recall)\n",
    "        f1_per_class_array.append(f1_value)\n",
    "    else:\n",
    "        precision_per_class.append(0)\n",
    "        recall_per_class.append(0)\n",
    "        f1_per_class_array.append(0)\n",
    "\n",
    "precision_per_class = np.array(precision_per_class)\n",
    "recall_per_class = np.array(recall_per_class)\n",
    "f1_per_class_array = np.array(f1_per_class_array)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig_pc, axes_pc = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "y_pos = np.arange(num_classes)\n",
    "axes_pc[0].barh(y_pos, precision_per_class, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "axes_pc[0].set_yticks(y_pos)\n",
    "axes_pc[0].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[0].set_xlabel('Precision', fontsize=12, fontweight='bold')\n",
    "axes_pc[0].set_title('Precision por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[0].set_xlim([0, 1])\n",
    "axes_pc[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes_pc[1].barh(y_pos, recall_per_class, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "axes_pc[1].set_yticks(y_pos)\n",
    "axes_pc[1].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "axes_pc[1].set_title('Recall por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[1].set_xlim([0, 1])\n",
    "axes_pc[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes_pc[2].barh(y_pos, f1_per_class_array, color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "axes_pc[2].set_yticks(y_pos)\n",
    "axes_pc[2].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[2].set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes_pc[2].set_title('F1-Score por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[2].set_xlim([0, 1])\n",
    "axes_pc[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle(f'An√°lisis por Clase - {EXPERIMENT_TYPE.upper()}', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "per_class_plot_path = output_dir / 'per_class_analysis.png'\n",
    "plt.savefig(per_class_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: {per_class_plot_path}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"‚úì An√°lisis por clase completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0330b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Generando resumen ejecutivo...\n",
      "‚úì Guardado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\\G2-RESULTS\\RESUMEN.txt\n",
      "\n",
      "================================================================================\n",
      "‚úÖ BASELINE (G2-RESULTS) COMPLETADO\n",
      "================================================================================\n",
      "  Test Accuracy: 0.8786\n",
      "  Macro F1:      0.8486\n",
      "  Top-3 Acc:     0.9884\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 12. RESUMEN EJECUTIVO\n",
    "print(f\"\\nüìù Generando resumen ejecutivo...\")\n",
    "\n",
    "# Top 5 clases por F1\n",
    "f1_per_class_list = [(i, f1_value) for i, f1_value in enumerate(f1_per_class_array)]\n",
    "class_f1_sorted = sorted(f1_per_class_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "resumen = f\"\"\"\n",
    "{'='*80}\n",
    "RESUMEN EJECUTIVO - {current_config['folder_name']}\n",
    "{'='*80}\n",
    "\n",
    "üìä PERFORMANCE:\n",
    "  ‚Ä¢ Test Accuracy:       {test_acc:.4f}\n",
    "  ‚Ä¢ Macro F1-Score:      {macro_f1:.4f}\n",
    "  ‚Ä¢ Macro Precision:     {macro_precision:.4f}\n",
    "  ‚Ä¢ Macro Recall:        {macro_recall:.4f}\n",
    "  ‚Ä¢ Top-3 Accuracy:      {top3_acc:.4f}\n",
    "\n",
    "üèóÔ∏è ARQUITECTURA:\n",
    "  ‚Ä¢ Modelo:              Transformer Encoder-Only\n",
    "  ‚Ä¢ Dataset:             UMAP Embeddings (reducci√≥n de dimensionalidad)\n",
    "  ‚Ä¢ Input Features:      {config['input_dim']}\n",
    "  ‚Ä¢ Sequence Length:     96 frames\n",
    "  ‚Ä¢ Embedding Dim:       256\n",
    "  ‚Ä¢ Attention Heads:     8\n",
    "  ‚Ä¢ Encoder Layers:      6\n",
    "  ‚Ä¢ Dim Feedforward:     1024\n",
    "  ‚Ä¢ Total Parameters:    {total_params:,}\n",
    "\n",
    "‚öôÔ∏è CONFIGURACI√ìN:\n",
    "  ‚Ä¢ Dropout:             {current_config['dropout']}\n",
    "  ‚Ä¢ Class Weights:       {'Activado' if current_config['use_class_weights'] else 'Desactivado'}\n",
    "  ‚Ä¢ Label Smoothing:     {current_config['label_smoothing']}\n",
    "  ‚Ä¢ Best Epoch:          {best_epoch}\n",
    "  ‚Ä¢ Optimizer:           AdamW (lr={config['lr']}, weight_decay={config['weight_decay']})\n",
    "  ‚Ä¢ Scheduler:           CosineAnnealingWarmRestarts\n",
    "\n",
    "üéØ VENTAJAS UMAP EMBEDDINGS:\n",
    "  ‚Ä¢ Reducci√≥n de dimensionalidad: Preserva estructura local y global\n",
    "  ‚Ä¢ Menor carga computacional\n",
    "  ‚Ä¢ M√°scaras por muestra nativas (m√°s preciso)\n",
    "  ‚Ä¢ Mejor generalizaci√≥n con menos features ruidosas\n",
    "\n",
    "üìà TOP 5 CLASES (F1-Score):\n",
    "\"\"\"\n",
    "for rank, (class_idx, f1_value) in enumerate(class_f1_sorted[:5], 1):\n",
    "    class_name = filenames[class_idx] if class_idx < len(filenames) else f'Class {class_idx}'\n",
    "    resumen += f\"  {rank}. {class_name:20s} | F1: {f1_value:.4f}\\n\"\n",
    "\n",
    "resumen += f\"\\n{'='*80}\\n\"\n",
    "\n",
    "# Guardar\n",
    "resumen_path = output_dir / 'RESUMEN.txt'\n",
    "with open(resumen_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(resumen)\n",
    "print(f\"‚úì Guardado: {resumen_path}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ BASELINE ({current_config['folder_name']}) COMPLETADO\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Macro F1:      {macro_f1:.4f}\")\n",
    "print(f\"  Top-3 Acc:     {top3_acc:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61673c2",
   "metadata": {},
   "source": [
    "# Experimentos de Mejora - Transformer Encoder-Only con GCN Embeddings Concatenados\n",
    "\n",
    "## üéØ Objetivo\n",
    "Implementar y comparar mejoras controladas sobre el modelo base para mejorar Macro-F1 y generalizaci√≥n usando embeddings GCN concatenados.\n",
    "\n",
    "### Experimentos realizados:\n",
    "- **Exp 0 (G2-RESULTS)**: Baseline - Dropout 0.1, sin class weights, sin label smoothing\n",
    "  - **Test Accuracy: 0.8786** | **Macro-F1: 0.8486** | Top-3: 0.9884\n",
    "  - **Early stopping: Epoch 45**\n",
    "  \n",
    "- **Exp 1 (G2-RESULTS-CLASS-WEIGHTS)**: Class Weights + Dropout 0.3\n",
    "  - Test Accuracy: 0.8786 | Macro-F1: 0.8398 | Top-3: 0.9827\n",
    "  - **Early stopping: Epoch 40** | Mejora vs Baseline: **-0.88%**\n",
    "  \n",
    "- **Exp 2 (G2-RESULTS-LABEL-SMOOTH)**: Label Smoothing 0.1 + Dropout 0.3\n",
    "  - Test Accuracy: 0.8728 | Macro-F1: 0.8244 | Top-3: 0.9711\n",
    "  - **Early stopping: Epoch 48** | Mejora vs Baseline: **-2.42%**\n",
    "\n",
    "**Conclusi√≥n:** El baseline (Exp 0) obtuvo el mejor Macro-F1 en esta ejecuci√≥n. La regularizaci√≥n adicional (Exp 1 y 2) no mejor√≥ el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c28ee199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Funci√≥n create_model definida\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n para crear modelo con dropout configurable\n",
    "def create_model(dropout_config=0.1):\n",
    "    \"\"\"Crea modelo con configuraci√≥n espec√≠fica de dropout\"\"\"\n",
    "    model = TransformerEncoderOnlyClassifier(\n",
    "        input_dim=config['input_dim'],  # 128 para UMAP\n",
    "        d_model=256,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        dim_feedforward=1024,\n",
    "        dropout=dropout_config,\n",
    "        num_classes=num_classes,\n",
    "        max_seq_len=96,\n",
    "        mlp_dropout=0.2,\n",
    "        activation='gelu'\n",
    "    ).to(device)\n",
    "    return model\n",
    "\n",
    "print(\"‚úì Funci√≥n create_model definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396ff8f",
   "metadata": {},
   "source": [
    "## üß™ Experimento 0 (G2-RESULTS) - Baseline ‚úÖ MEJOR MACRO-F1\n",
    "- **Dropout:** 0.1\n",
    "- **Sin class weights**\n",
    "- **Sin label smoothing**\n",
    "- **Total de par√°metros:** 2,203,294 (todos entrenables)\n",
    "- **Arquitectura:** \n",
    "  - Input: 128 ‚Üí d_model: 256\n",
    "  - Transformer: 4 capas, 8 heads, FFN: 512\n",
    "  - Classifier: 256 ‚Üí 128 ‚Üí 30 (GELU activation)\n",
    "- **Resultados:** Test Accuracy: **0.8786** | Macro-F1: **0.8486** | Top-3: **0.9884**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7918bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Resultados baseline registrados\n"
     ]
    }
   ],
   "source": [
    "# Resultados Exp 0 (Baseline)\n",
    "exp0_results = {\n",
    "    'experiment': 'G2-RESULTS',\n",
    "    'dropout': current_config['dropout'],\n",
    "    'class_weights': current_config.get('use_class_weights', False),\n",
    "    'label_smoothing': current_config.get('label_smoothing', 0.0),\n",
    "    'test_accuracy': test_acc,\n",
    "    'test_macro_f1': macro_f1,\n",
    "    'test_top3_accuracy': top3_acc,\n",
    "    'test_loss': test_loss,\n",
    "    'best_epoch': best_epoch,\n",
    "    'best_val_acc': best_val_acc\n",
    "}\n",
    "\n",
    "print(f\"‚úì Resultados {EXPERIMENT_TYPE} registrados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7d353",
   "metadata": {},
   "source": [
    "## üß™ Experimento 1 (G2-RESULTS-CLASS-WEIGHTS) - Class Weights + Dropout 0.3\n",
    "- **Class weights:** Calculados de y_train (30 clases con distribuci√≥n desbalanceada)\n",
    "- **Dropout:** 0.3 (mayor regularizaci√≥n)\n",
    "- **Sin label smoothing**\n",
    "- **Early stopping:** Epoch 40\n",
    "- **Resultados:** Test Accuracy: 0.8786 | Macro-F1: 0.8398 | Top-3: 0.9827\n",
    "- **Mejora vs Baseline:** -0.88% ‚ùå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30969ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Iniciando Experimento 1: G2-RESULTS-CLASS-WEIGHTS\n",
      "================================================================================\n",
      "üìÅ Directorio: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\\G2-RESULTS-CLASS-WEIGHTS\n",
      "\n",
      "Entrenando con Class Weights y Dropout 0.3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50 | Train Loss: 3.1514 | Val Acc: 0.3094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/50 | Train Loss: 1.6607 | Val Acc: 0.6187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/50 | Train Loss: 0.8427 | Val Acc: 0.7914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/50 | Train Loss: 0.4188 | Val Acc: 0.8273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/50 | Train Loss: 0.2231 | Val Acc: 0.8417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25/50 | Train Loss: 0.1162 | Val Acc: 0.8921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30/50 | Train Loss: 0.0441 | Val Acc: 0.9065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  35/50 | Train Loss: 0.0253 | Val Acc: 0.9065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40/50 | Train Loss: 0.0082 | Val Acc: 0.9353\n",
      "Early stopping at epoch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì CLASS-WEIGHTS completado:\n",
      "  Test Accuracy: 0.8786\n",
      "  Macro F1: 0.8398\n",
      "  Top-3 Acc: 0.9827\n",
      "‚úì Guardado: training_log.csv\n",
      "‚úì Guardado: metrics.csv\n",
      "‚úì Guardado: per_class_metrics.csv\n",
      "‚úì Guardado: confusion_matrix.csv\n",
      "‚úì Guardado: config.json\n",
      "‚úì Guardado: training_curves.png\n",
      "‚úì Guardado: confusion_matrix.png\n",
      "‚úì Guardado: per_class_analysis.png\n",
      "‚úì Guardado: RESUMEN.txt\n",
      "================================================================================\n",
      "‚úÖ EXPERIMENTO 1 (CLASS-WEIGHTS) COMPLETADO\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENTO 1: G2-RESULTS-CLASS-WEIGHTS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Iniciando Experimento 1: G2-RESULTS-CLASS-WEIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configurar directorio de salida\n",
    "output_dir_exp1 = ROOT_PATH / 'G2-RESULTS-CLASS-WEIGHTS'\n",
    "output_dir_exp1.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ Directorio: {output_dir_exp1}\")\n",
    "\n",
    "# Calcular class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.FloatTensor(class_weights_array).to(device)\n",
    "\n",
    "# Crear modelo con dropout 0.3\n",
    "model_exp1 = create_model(dropout_config=0.3)\n",
    "criterion_exp1 = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.0)\n",
    "optimizer_exp1 = AdamW(model_exp1.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "scheduler_exp1 = ReduceLROnPlateau(optimizer_exp1, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "# Entrenamiento\n",
    "training_log_exp1 = {'epoch': [], 'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "best_val_acc_exp1 = 0.0\n",
    "best_epoch_exp1 = 0\n",
    "patience_counter_exp1 = 0\n",
    "\n",
    "print(f\"\\nEntrenando con Class Weights y Dropout 0.3...\")\n",
    "for epoch in range(config['max_epochs']):\n",
    "    train_loss, train_acc = train_epoch(model_exp1, train_loader, criterion_exp1, optimizer_exp1, device)\n",
    "    val_loss, val_acc, _, _, _ = eval_epoch(model_exp1, val_loader, criterion_exp1, device)\n",
    "    \n",
    "    current_lr = optimizer_exp1.param_groups[0]['lr']\n",
    "    scheduler_exp1.step(val_acc)\n",
    "    \n",
    "    training_log_exp1['epoch'].append(epoch)\n",
    "    training_log_exp1['train_loss'].append(train_loss)\n",
    "    training_log_exp1['train_acc'].append(train_acc)\n",
    "    training_log_exp1['val_loss'].append(val_loss)\n",
    "    training_log_exp1['val_acc'].append(val_acc)\n",
    "    training_log_exp1['lr'].append(current_lr)\n",
    "    \n",
    "    if val_acc > best_val_acc_exp1:\n",
    "        best_val_acc_exp1 = val_acc\n",
    "        best_epoch_exp1 = epoch\n",
    "        patience_counter_exp1 = 0\n",
    "        torch.save(model_exp1.state_dict(), output_dir_exp1 / 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter_exp1 += 1\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{config['max_epochs']} | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if patience_counter_exp1 >= config['early_stopping_patience']:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Cargar mejor modelo\n",
    "model_exp1.load_state_dict(torch.load(output_dir_exp1 / 'best_model.pt', map_location=device))\n",
    "\n",
    "# Reimport sklearn metrics (caso se hayan sobreescrito)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Evaluaci√≥n\n",
    "test_loss_exp1, test_acc_exp1, test_preds_exp1, test_labels_exp1, test_logits_exp1 = eval_epoch(\n",
    "    model_exp1, test_loader, criterion_exp1, device\n",
    ")\n",
    "\n",
    "macro_f1_exp1 = f1_score(test_labels_exp1, test_preds_exp1, average='macro', zero_division=0)\n",
    "macro_precision_exp1 = precision_score(test_labels_exp1, test_preds_exp1, average='macro', zero_division=0)\n",
    "macro_recall_exp1 = recall_score(test_labels_exp1, test_preds_exp1, average='macro', zero_division=0)\n",
    "top3_acc_exp1 = top_k_accuracy_score(test_labels_exp1, test_logits_exp1, k=3, labels=np.arange(num_classes))\n",
    "\n",
    "print(f\"\\n‚úì CLASS-WEIGHTS completado:\")\n",
    "print(f\"  Test Accuracy: {test_acc_exp1:.4f}\")\n",
    "print(f\"  Macro F1: {macro_f1_exp1:.4f}\")\n",
    "print(f\"  Top-3 Acc: {top3_acc_exp1:.4f}\")\n",
    "\n",
    "# 1. Training log\n",
    "pd.DataFrame(training_log_exp1).to_csv(output_dir_exp1 / 'training_log.csv', index=False)\n",
    "print(f\"‚úì Guardado: training_log.csv\")\n",
    "\n",
    "# 2. Metrics CSV\n",
    "pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Macro-F1', 'Macro-Precision', 'Macro-Recall', 'Top-3 Accuracy', 'Test Loss'],\n",
    "    'Value': [test_acc_exp1, macro_f1_exp1, macro_precision_exp1, macro_recall_exp1, top3_acc_exp1, test_loss_exp1]\n",
    "}).to_csv(output_dir_exp1 / 'metrics.csv', index=False)\n",
    "print(f\"‚úì Guardado: metrics.csv\")\n",
    "\n",
    "# 3. Per-class metrics\n",
    "# Extract unique class names (one per class ID)\n",
    "unique_class_names_exp1 = []\n",
    "for class_id in range(num_classes):\n",
    "    idx = np.where(y == class_id)[0][0]\n",
    "    class_name = str(filenames[idx]).replace('.json', '').split('_')[0]\n",
    "    unique_class_names_exp1.append(class_name)\n",
    "\n",
    "class_report_dict = classification_report(\n",
    "    test_labels_exp1, test_preds_exp1, \n",
    "    target_names=unique_class_names_exp1,\n",
    "    output_dict=True, zero_division=0\n",
    ")\n",
    "pd.DataFrame(class_report_dict).T.to_csv(output_dir_exp1 / 'per_class_metrics.csv')\n",
    "print(f\"‚úì Guardado: per_class_metrics.csv\")\n",
    "\n",
    "# 4. Confusion matrix CSV\n",
    "cm_exp1 = confusion_matrix(test_labels_exp1, test_preds_exp1)\n",
    "pd.DataFrame(cm_exp1).to_csv(output_dir_exp1 / 'confusion_matrix.csv', index=False, header=False)\n",
    "print(f\"‚úì Guardado: confusion_matrix.csv\")\n",
    "\n",
    "# 5. Config JSON\n",
    "config_exp1 = {\n",
    "    'experiment_type': 'class_weights',\n",
    "    'architecture': 'TransformerEncoderOnly',\n",
    "    'input_dim': config['input_dim'],\n",
    "    'd_model': 256,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'dim_feedforward': 1024,\n",
    "    'dropout': 0.3,\n",
    "    'num_classes': num_classes,\n",
    "    'max_seq_len': 96,\n",
    "    'use_class_weights': True,\n",
    "    'label_smoothing': 0.0,\n",
    "    'best_epoch': int(best_epoch_exp1),\n",
    "    'best_val_acc': float(best_val_acc_exp1),\n",
    "    'test_accuracy': float(test_acc_exp1),\n",
    "    'test_macro_f1': float(macro_f1_exp1),\n",
    "    'training_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "with open(output_dir_exp1 / 'config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_exp1, f, indent=2, ensure_ascii=False)\n",
    "print(f\"‚úì Guardado: config.json\")\n",
    "\n",
    "# 6. Training curves PNG\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].plot(training_log_exp1['epoch'], training_log_exp1['train_loss'], 'b-', label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0, 0].plot(training_log_exp1['epoch'], training_log_exp1['val_loss'], 'r-', label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0, 0].axvline(best_epoch_exp1, color='g', linestyle='--', alpha=0.7, linewidth=2, label=f'Best Epoch {best_epoch_exp1+1}')\n",
    "axes[0, 0].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Curva de Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(training_log_exp1['epoch'], training_log_exp1['train_acc'], 'b-', label='Train Acc', marker='o', linewidth=2)\n",
    "axes[0, 1].plot(training_log_exp1['epoch'], training_log_exp1['val_acc'], 'r-', label='Val Acc', marker='s', linewidth=2)\n",
    "axes[0, 1].axvline(best_epoch_exp1, color='g', linestyle='--', alpha=0.7, linewidth=2, label=f'Best Epoch {best_epoch_exp1+1}')\n",
    "axes[0, 1].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Curva de Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(training_log_exp1['epoch'], training_log_exp1['lr'], 'g-', marker='o', linewidth=2)\n",
    "axes[1, 0].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "metrics_names = ['Accuracy', 'Macro-F1', 'Top-3 Acc']\n",
    "metrics_values = [test_acc_exp1, macro_f1_exp1, top3_acc_exp1]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "bars = axes[1, 1].bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('M√©tricas en Test Set', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylim([0, 1.05])\n",
    "for bar, v in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02, \n",
    "                    f'{v:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Curvas de Aprendizaje - CLASS-WEIGHTS', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir_exp1 / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: training_curves.png\")\n",
    "plt.close()\n",
    "\n",
    "# 7. Confusion matrix PNG\n",
    "unique_classes_exp1 = sorted(list(set(test_labels_exp1)))\n",
    "class_labels_exp1 = [filenames[i] if i < len(filenames) else f'Class {i}' for i in unique_classes_exp1]\n",
    "\n",
    "fig_cm, ax_cm = plt.subplots(figsize=(20, 18))\n",
    "sns.heatmap(cm_exp1, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Muestras'}, ax=ax_cm,\n",
    "            xticklabels=class_labels_exp1, yticklabels=class_labels_exp1, square=True)\n",
    "ax_cm.set_xlabel('Clase Predicha', fontsize=14, fontweight='bold')\n",
    "ax_cm.set_ylabel('Clase Real', fontsize=14, fontweight='bold')\n",
    "ax_cm.set_title(f'Matriz de Confusi√≥n - CLASS-WEIGHTS (Accuracy: {test_acc_exp1:.4f})', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir_exp1 / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# 8. Per-class analysis PNG\n",
    "precision_per_class_exp1 = []\n",
    "recall_per_class_exp1 = []\n",
    "f1_per_class_array_exp1 = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    if i in unique_classes_exp1:\n",
    "        idx = unique_classes_exp1.index(i)\n",
    "        tp = cm_exp1[idx, idx]\n",
    "        fp = cm_exp1[:, idx].sum() - tp\n",
    "        fn = cm_exp1[idx, :].sum() - tp\n",
    "        \n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_value = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "        \n",
    "        precision_per_class_exp1.append(prec)\n",
    "        recall_per_class_exp1.append(rec)\n",
    "        f1_per_class_array_exp1.append(f1_value)\n",
    "    else:\n",
    "        precision_per_class_exp1.append(0)\n",
    "        recall_per_class_exp1.append(0)\n",
    "        f1_per_class_array_exp1.append(0)\n",
    "\n",
    "precision_per_class_exp1 = np.array(precision_per_class_exp1)\n",
    "recall_per_class_exp1 = np.array(recall_per_class_exp1)\n",
    "f1_per_class_array_exp1 = np.array(f1_per_class_array_exp1)\n",
    "\n",
    "fig_pc, axes_pc = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "y_pos = np.arange(num_classes)\n",
    "axes_pc[0].barh(y_pos, precision_per_class_exp1, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "axes_pc[0].set_yticks(y_pos)\n",
    "axes_pc[0].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[0].set_xlabel('Precision', fontsize=12, fontweight='bold')\n",
    "axes_pc[0].set_title('Precision por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[0].set_xlim([0, 1])\n",
    "axes_pc[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes_pc[1].barh(y_pos, recall_per_class_exp1, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "axes_pc[1].set_yticks(y_pos)\n",
    "axes_pc[1].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "axes_pc[1].set_title('Recall por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[1].set_xlim([0, 1])\n",
    "axes_pc[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes_pc[2].barh(y_pos, f1_per_class_array_exp1, color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "axes_pc[2].set_yticks(y_pos)\n",
    "axes_pc[2].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[2].set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes_pc[2].set_title('F1-Score por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[2].set_xlim([0, 1])\n",
    "axes_pc[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('An√°lisis por Clase - CLASS-WEIGHTS', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir_exp1 / 'per_class_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: per_class_analysis.png\")\n",
    "plt.close()\n",
    "\n",
    "# 9. RESUMEN.txt\n",
    "f1_per_class_list_exp1 = [(i, f1_value) for i, f1_value in enumerate(f1_per_class_array_exp1)]\n",
    "class_f1_sorted = sorted(f1_per_class_list_exp1, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "summary_exp1 = f\"\"\"\n",
    "{'='*80}\n",
    "RESUMEN EJECUTIVO - G2-RESULTS-CLASS-WEIGHTS\n",
    "{'='*80}\n",
    "\n",
    "üìä PERFORMANCE:\n",
    "  ‚Ä¢ Test Accuracy:       {test_acc_exp1:.4f}\n",
    "  ‚Ä¢ Macro F1-Score:      {macro_f1_exp1:.4f}\n",
    "  ‚Ä¢ Macro Precision:     {macro_precision_exp1:.4f}\n",
    "  ‚Ä¢ Macro Recall:        {macro_recall_exp1:.4f}\n",
    "  ‚Ä¢ Top-3 Accuracy:      {top3_acc_exp1:.4f}\n",
    "\n",
    "‚öôÔ∏è CONFIGURACI√ìN:\n",
    "  ‚Ä¢ Dropout:             0.3\n",
    "  ‚Ä¢ Class Weights:       Activado (balanced)\n",
    "  ‚Ä¢ Label Smoothing:     0.0\n",
    "  ‚Ä¢ Best Epoch:          {best_epoch_exp1}\n",
    "\n",
    "üìà TOP 5 CLASES (F1-Score):\n",
    "\"\"\"\n",
    "for rank, (class_idx, f1_value) in enumerate(class_f1_sorted[:5], 1):\n",
    "    class_name = filenames[class_idx] if class_idx < len(filenames) else f'Class {class_idx}'\n",
    "    summary_exp1 += f\"  {rank}. {class_name:20s} | F1: {f1_value:.4f}\\n\"\n",
    "\n",
    "summary_exp1 += f\"\\n{'='*80}\\n\"\n",
    "\n",
    "with open(output_dir_exp1 / 'RESUMEN.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_exp1)\n",
    "print(f\"‚úì Guardado: RESUMEN.txt\")\n",
    "\n",
    "# Resultados en diccionario\n",
    "exp1_results = {\n",
    "    'experiment': 'G2-RESULTS-CLASS-WEIGHTS',\n",
    "    'dropout': 0.3,\n",
    "    'class_weights': True,\n",
    "    'label_smoothing': 0.0,\n",
    "    'test_accuracy': test_acc_exp1,\n",
    "    'test_macro_f1': macro_f1_exp1,\n",
    "    'test_top3_accuracy': top3_acc_exp1,\n",
    "    'best_epoch': best_epoch_exp1\n",
    "}\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"‚úÖ EXPERIMENTO 1 (CLASS-WEIGHTS) COMPLETADO\")\n",
    "print(f\"\\n{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c0d15",
   "metadata": {},
   "source": [
    "## üß™ Experimento 2 (G2-RESULTS-LABEL-SMOOTH) - Dropout 0.3 + Label Smoothing\n",
    "- **Dropout:** 0.3 (mayor regularizaci√≥n)\n",
    "- **Label smoothing:** 0.1 (suavizado de etiquetas)\n",
    "- **Sin class weights**\n",
    "- **Early stopping:** Epoch 48\n",
    "- **Resultados:** Test Accuracy: 0.8728 | Macro-F1: 0.8244 | Top-3: 0.9711\n",
    "- **Mejora vs Baseline:** -2.42% ‚ùå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3705ec83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Iniciando Experimento 2: G2-RESULTS-LABEL-SMOOTH\n",
      "================================================================================\n",
      "üìÅ Directorio: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\\G2-RESULTS-LABEL-SMOOTH\n",
      "\n",
      "Entrenando con Label Smoothing 0.1 y Dropout 0.3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50 | Train Loss: 3.0336 | Val Acc: 0.3741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/50 | Train Loss: 1.7441 | Val Acc: 0.6475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/50 | Train Loss: 1.2210 | Val Acc: 0.7914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/50 | Train Loss: 0.9899 | Val Acc: 0.8705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/50 | Train Loss: 0.8382 | Val Acc: 0.8489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25/50 | Train Loss: 0.7550 | Val Acc: 0.8849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30/50 | Train Loss: 0.7169 | Val Acc: 0.9209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  35/50 | Train Loss: 0.6942 | Val Acc: 0.9424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40/50 | Train Loss: 0.6809 | Val Acc: 0.9568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  45/50 | Train Loss: 0.6738 | Val Acc: 0.9353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì LABEL-SMOOTH completado:\n",
      "  Test Accuracy: 0.8728\n",
      "  Macro F1: 0.8244\n",
      "  Top-3 Acc: 0.9711\n",
      "‚úì Guardado: training_log.csv\n",
      "‚úì Guardado: metrics.csv\n",
      "‚úì Guardado: per_class_metrics.csv\n",
      "‚úì Guardado: confusion_matrix.csv\n",
      "‚úì Guardado: config.json\n",
      "‚úì Guardado: training_curves.png\n",
      "‚úì Guardado: confusion_matrix.png\n",
      "‚úì Guardado: per_class_analysis.png\n",
      "‚úì Guardado: RESUMEN.txt\n",
      "================================================================================\n",
      "‚úÖ EXPERIMENTO 2 (LABEL-SMOOTH) COMPLETADO\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENTO 2: G2-RESULTS-LABEL-SMOOTH\n",
    "# Reimport sklearn metrics (caso se hayan sobreescrito)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Iniciando Experimento 2: G2-RESULTS-LABEL-SMOOTH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configurar directorio\n",
    "output_dir_exp2 = ROOT_PATH / 'G2-RESULTS-LABEL-SMOOTH'\n",
    "output_dir_exp2.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ Directorio: {output_dir_exp2}\")\n",
    "\n",
    "# Crear modelo con dropout 0.3\n",
    "model_exp2 = create_model(dropout_config=0.3)\n",
    "criterion_exp2 = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer_exp2 = AdamW(model_exp2.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "scheduler_exp2 = ReduceLROnPlateau(optimizer_exp2, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "# Entrenamiento\n",
    "training_log_exp2 = {'epoch': [], 'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "best_val_acc_exp2 = 0.0\n",
    "best_epoch_exp2 = 0\n",
    "patience_counter_exp2 = 0\n",
    "\n",
    "print(f\"\\nEntrenando con Label Smoothing 0.1 y Dropout 0.3...\")\n",
    "for epoch in range(config['max_epochs']):\n",
    "    train_loss, train_acc = train_epoch(model_exp2, train_loader, criterion_exp2, optimizer_exp2, device)\n",
    "    val_loss, val_acc, _, _, _ = eval_epoch(model_exp2, val_loader, criterion_exp2, device)\n",
    "    \n",
    "    current_lr = optimizer_exp2.param_groups[0]['lr']\n",
    "    scheduler_exp2.step(val_acc)\n",
    "    \n",
    "    training_log_exp2['epoch'].append(epoch)\n",
    "    training_log_exp2['train_loss'].append(train_loss)\n",
    "    training_log_exp2['train_acc'].append(train_acc)\n",
    "    training_log_exp2['val_loss'].append(val_loss)\n",
    "    training_log_exp2['val_acc'].append(val_acc)\n",
    "    training_log_exp2['lr'].append(current_lr)\n",
    "    \n",
    "    if val_acc > best_val_acc_exp2:\n",
    "        best_val_acc_exp2 = val_acc\n",
    "        best_epoch_exp2 = epoch\n",
    "        patience_counter_exp2 = 0\n",
    "        torch.save(model_exp2.state_dict(), output_dir_exp2 / 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter_exp2 += 1\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{config['max_epochs']} | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if patience_counter_exp2 >= config['early_stopping_patience']:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Cargar mejor modelo\n",
    "model_exp2.load_state_dict(torch.load(output_dir_exp2 / 'best_model.pt', map_location=device))\n",
    "\n",
    "# Evaluaci√≥n\n",
    "test_loss_exp2, test_acc_exp2, test_preds_exp2, test_labels_exp2, test_logits_exp2 = eval_epoch(\n",
    "    model_exp2, test_loader, criterion_exp2, device\n",
    ")\n",
    "\n",
    "macro_f1_exp2 = f1_score(test_labels_exp2, test_preds_exp2, average='macro', zero_division=0)\n",
    "macro_precision_exp2 = precision_score(test_labels_exp2, test_preds_exp2, average='macro', zero_division=0)\n",
    "macro_recall_exp2 = recall_score(test_labels_exp2, test_preds_exp2, average='macro', zero_division=0)\n",
    "top3_acc_exp2 = top_k_accuracy_score(test_labels_exp2, test_logits_exp2, k=3, labels=np.arange(num_classes))\n",
    "\n",
    "print(f\"\\n‚úì LABEL-SMOOTH completado:\")\n",
    "print(f\"  Test Accuracy: {test_acc_exp2:.4f}\")\n",
    "print(f\"  Macro F1: {macro_f1_exp2:.4f}\")\n",
    "print(f\"  Top-3 Acc: {top3_acc_exp2:.4f}\")\n",
    "\n",
    "# 1. Training log\n",
    "pd.DataFrame(training_log_exp2).to_csv(output_dir_exp2 / 'training_log.csv', index=False)\n",
    "print(f\"‚úì Guardado: training_log.csv\")\n",
    "\n",
    "# 2. Metrics CSV\n",
    "pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Macro-F1', 'Macro-Precision', 'Macro-Recall', 'Top-3 Accuracy', 'Test Loss'],\n",
    "    'Value': [test_acc_exp2, macro_f1_exp2, macro_precision_exp2, macro_recall_exp2, top3_acc_exp2, test_loss_exp2]\n",
    "}).to_csv(output_dir_exp2 / 'metrics.csv', index=False)\n",
    "print(f\"‚úì Guardado: metrics.csv\")\n",
    "\n",
    "# 3. Per-class metrics\n",
    "# Extract unique class names (one per class ID)\n",
    "unique_class_names_exp2 = []\n",
    "for class_id in range(num_classes):\n",
    "    idx = np.where(y == class_id)[0][0]\n",
    "    class_name = str(filenames[idx]).replace('.json', '').split('_')[0]\n",
    "    unique_class_names_exp2.append(class_name)\n",
    "\n",
    "class_report_dict_exp2 = classification_report(\n",
    "    test_labels_exp2, test_preds_exp2, \n",
    "    target_names=unique_class_names_exp2,\n",
    "    output_dict=True, zero_division=0\n",
    ")\n",
    "pd.DataFrame(class_report_dict_exp2).T.to_csv(output_dir_exp2 / 'per_class_metrics.csv')\n",
    "print(f\"‚úì Guardado: per_class_metrics.csv\")\n",
    "\n",
    "# 4. Confusion matrix CSV\n",
    "cm_exp2 = confusion_matrix(test_labels_exp2, test_preds_exp2)\n",
    "pd.DataFrame(cm_exp2).to_csv(output_dir_exp2 / 'confusion_matrix.csv', index=False, header=False)\n",
    "print(f\"‚úì Guardado: confusion_matrix.csv\")\n",
    "\n",
    "# 5. Config JSON\n",
    "config_exp2 = {\n",
    "    'experiment_type': 'label_smoothing',\n",
    "    'architecture': 'TransformerEncoderOnly',\n",
    "    'input_dim': config['input_dim'],\n",
    "    'd_model': 256,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'dim_feedforward': 1024,\n",
    "    'dropout': 0.3,\n",
    "    'num_classes': num_classes,\n",
    "    'max_seq_len': 96,\n",
    "    'use_class_weights': False,\n",
    "    'label_smoothing': 0.1,\n",
    "    'best_epoch': int(best_epoch_exp2),\n",
    "    'best_val_acc': float(best_val_acc_exp2),\n",
    "    'test_accuracy': float(test_acc_exp2),\n",
    "    'test_macro_f1': float(macro_f1_exp2),\n",
    "    'training_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "with open(output_dir_exp2 / 'config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_exp2, f, indent=2, ensure_ascii=False)\n",
    "print(f\"‚úì Guardado: config.json\")\n",
    "\n",
    "# 6. Training curves PNG\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].plot(training_log_exp2['epoch'], training_log_exp2['train_loss'], 'b-', label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0, 0].plot(training_log_exp2['epoch'], training_log_exp2['val_loss'], 'r-', label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0, 0].axvline(best_epoch_exp2, color='g', linestyle='--', alpha=0.7, linewidth=2, label=f'Best Epoch {best_epoch_exp2+1}')\n",
    "axes[0, 0].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Curva de Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(training_log_exp2['epoch'], training_log_exp2['train_acc'], 'b-', label='Train Acc', marker='o', linewidth=2)\n",
    "axes[0, 1].plot(training_log_exp2['epoch'], training_log_exp2['val_acc'], 'r-', label='Val Acc', marker='s', linewidth=2)\n",
    "axes[0, 1].axvline(best_epoch_exp2, color='g', linestyle='--', alpha=0.7, linewidth=2, label=f'Best Epoch {best_epoch_exp2+1}')\n",
    "axes[0, 1].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Curva de Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(training_log_exp2['epoch'], training_log_exp2['lr'], 'g-', marker='o', linewidth=2)\n",
    "axes[1, 0].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "metrics_names = ['Accuracy', 'Macro-F1', 'Top-3 Acc']\n",
    "metrics_values = [test_acc_exp2, macro_f1_exp2, top3_acc_exp2]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "bars = axes[1, 1].bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('M√©tricas en Test Set', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylim([0, 1.05])\n",
    "for bar, v in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02, \n",
    "                    f'{v:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Curvas de Aprendizaje - LABEL-SMOOTH', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir_exp2 / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: training_curves.png\")\n",
    "plt.close()\n",
    "\n",
    "# 7. Confusion matrix PNG\n",
    "fig_cm, ax_cm = plt.subplots(figsize=(20, 18))\n",
    "sns.heatmap(cm_exp2, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Muestras'}, ax=ax_cm,\n",
    "            xticklabels=unique_class_names_exp2, yticklabels=unique_class_names_exp2, square=True)\n",
    "ax_cm.set_xlabel('Clase Predicha', fontsize=14, fontweight='bold')\n",
    "ax_cm.set_ylabel('Clase Real', fontsize=14, fontweight='bold')\n",
    "ax_cm.set_title(f'Matriz de Confusi√≥n - LABEL-SMOOTH (Accuracy: {test_acc_exp2:.4f})', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir_exp2 / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# 8. Per-class analysis PNG\n",
    "unique_classes_exp2 = sorted(list(set(test_labels_exp2)))\n",
    "precision_per_class_exp2 = []\n",
    "recall_per_class_exp2 = []\n",
    "f1_per_class_array_exp2 = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    if i in unique_classes_exp2:\n",
    "        idx = unique_classes_exp2.index(i)\n",
    "        tp = cm_exp2[idx, idx]\n",
    "        fp = cm_exp2[:, idx].sum() - tp\n",
    "        fn = cm_exp2[idx, :].sum() - tp\n",
    "        \n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_value = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "        \n",
    "        precision_per_class_exp2.append(prec)\n",
    "        recall_per_class_exp2.append(rec)\n",
    "        f1_per_class_array_exp2.append(f1_value)\n",
    "    else:\n",
    "        precision_per_class_exp2.append(0)\n",
    "        recall_per_class_exp2.append(0)\n",
    "        f1_per_class_array_exp2.append(0)\n",
    "\n",
    "precision_per_class_exp2 = np.array(precision_per_class_exp2)\n",
    "recall_per_class_exp2 = np.array(recall_per_class_exp2)\n",
    "f1_per_class_array_exp2 = np.array(f1_per_class_array_exp2)\n",
    "\n",
    "fig_pc, axes_pc = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "y_pos = np.arange(num_classes)\n",
    "axes_pc[0].barh(y_pos, precision_per_class_exp2, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "axes_pc[0].set_yticks(y_pos)\n",
    "axes_pc[0].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[0].set_xlabel('Precision', fontsize=12, fontweight='bold')\n",
    "axes_pc[0].set_title('Precision por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[0].set_yticklabels(unique_class_names_exp2, fontsize=8)\n",
    "axes_pc[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes_pc[1].barh(y_pos, recall_per_class_exp2, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "axes_pc[1].set_yticks(y_pos)\n",
    "axes_pc[1].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "axes_pc[1].set_title('Recall por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[1].set_yticklabels(unique_class_names_exp2, fontsize=8)\n",
    "axes_pc[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes_pc[2].barh(y_pos, f1_per_class_array_exp2, color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "axes_pc[2].set_yticks(y_pos)\n",
    "axes_pc[2].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[2].set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes_pc[2].set_title('F1-Score por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[2].set_yticklabels(unique_class_names_exp2, fontsize=8)\n",
    "axes_pc[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('An√°lisis por Clase - LABEL-SMOOTH', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir_exp2 / 'per_class_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: per_class_analysis.png\")\n",
    "plt.close()\n",
    "\n",
    "# 9. RESUMEN.txt\n",
    "f1_per_class_list_exp2 = [(i, f1_value) for i, f1_value in enumerate(f1_per_class_array_exp2)]\n",
    "class_f1_sorted_exp2 = sorted(f1_per_class_list_exp2, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "summary_exp2 = f\"\"\"\n",
    "{'='*80}\n",
    "RESUMEN EJECUTIVO - G2-RESULTS-LABEL-SMOOTH\n",
    "{'='*80}\n",
    "\n",
    "üìä PERFORMANCE:\n",
    "  ‚Ä¢ Test Accuracy:       {test_acc_exp2:.4f}\n",
    "  ‚Ä¢ Macro F1-Score:      {macro_f1_exp2:.4f}\n",
    "  ‚Ä¢ Macro Precision:     {macro_precision_exp2:.4f}\n",
    "  ‚Ä¢ Macro Recall:        {macro_recall_exp2:.4f}\n",
    "  ‚Ä¢ Top-3 Accuracy:      {top3_acc_exp2:.4f}\n",
    "\n",
    "‚öôÔ∏è CONFIGURACI√ìN:\n",
    "  ‚Ä¢ Dropout:             0.3\n",
    "  ‚Ä¢ Class Weights:       Desactivado\n",
    "  ‚Ä¢ Label Smoothing:     0.1\n",
    "  ‚Ä¢ Best Epoch:          {best_epoch_exp2}\n",
    "\n",
    "üìà TOP 5 CLASES (F1-Score):\n",
    "\"\"\"\n",
    "for rank, (class_idx, f1_value) in enumerate(class_f1_sorted_exp2[:5], 1):\n",
    "    class_name = filenames[class_idx] if class_idx < len(filenames) else f'Class {class_idx}'\n",
    "    summary_exp2 += f\"  {rank}. {class_name:20s} | F1: {f1_value:.4f}\\n\"\n",
    "\n",
    "    class_name = unique_class_names_exp2[class_idx] if class_idx < len(unique_class_names_exp2) else f'Class {class_idx}'\n",
    "\n",
    "with open(output_dir_exp2 / 'RESUMEN.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_exp2)\n",
    "print(f\"‚úì Guardado: RESUMEN.txt\")\n",
    "\n",
    "# Resultados en diccionario\n",
    "exp2_results = {\n",
    "    'experiment': 'G2-RESULTS-LABEL-SMOOTH',\n",
    "    'dropout': 0.3,\n",
    "    'class_weights': False,\n",
    "    'label_smoothing': 0.1,\n",
    "    'test_accuracy': test_acc_exp2,\n",
    "    'test_macro_f1': macro_f1_exp2,\n",
    "    'test_top3_accuracy': top3_acc_exp2,\n",
    "    'best_epoch': best_epoch_exp2\n",
    "}\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"‚úÖ EXPERIMENTO 2 (LABEL-SMOOTH) COMPLETADO\")\n",
    "print(f\"\\n{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21041520",
   "metadata": {},
   "source": [
    "## üìä Comparaci√≥n Final de los 3 Experimentos\n",
    "\n",
    "### Tabla Comparativa de Resultados:\n",
    "| Experimento                  | Test Accuracy | Macro F1 | Top-3 Acc | Early Stop |\n",
    "|------------------------------|---------------|----------|-----------|------------|\n",
    "| **G2-RESULTS** (Baseline) ‚úÖ | **0.8786**    | **0.8486** | **0.9884** | Epoch 45   |\n",
    "| G2-RESULTS-CLASS-WEIGHTS     | 0.8786        | 0.8398   | 0.9827    | Epoch 40   |\n",
    "| G2-RESULTS-LABEL-SMOOTH      | 0.8728        | 0.8244   | 0.9711    | Epoch 48   |\n",
    "\n",
    "### An√°lisis:\n",
    "- **Mejor modelo:** G2-RESULTS (Baseline con Dropout 0.1)\n",
    "- **Mejora Exp1 vs Baseline:** -0.88% ‚ùå\n",
    "- **Mejora Exp2 vs Baseline:** -2.42% ‚ùå\n",
    "- **Conclusi√≥n:** La regularizaci√≥n adicional (Dropout 0.3 + Class Weights/Label Smoothing) no mejor√≥ el rendimiento. El baseline con dropout moderado (0.1) logr√≥ el mejor balance entre capacidad y generalizaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71320881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARACI√ìN DE LOS 3 EXPERIMENTOS\n",
      "================================================================================\n",
      "\n",
      "üìä Tabla Comparativa:\n",
      "              experiment  test_accuracy  test_macro_f1  test_top3_accuracy\n",
      "              G2-RESULTS       0.878613       0.848598            0.988439\n",
      "G2-RESULTS-CLASS-WEIGHTS       0.878613       0.839797            0.982659\n",
      " G2-RESULTS-LABEL-SMOOTH       0.872832       0.824432            0.971098\n",
      "\n",
      "üìà An√°lisis:\n",
      "  Mejor Macro-F1: G2-RESULTS\n",
      "  Mejora Exp1 vs Baseline: -0.88%\n",
      "  Mejora Exp2 vs Baseline: -2.42%\n",
      "\n",
      "‚úì Guardado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\\experiments_comparison.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ COMPARACI√ìN COMPLETADA\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# COMPARACI√ìN DE LOS 3 EXPERIMENTOS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARACI√ìN DE LOS 3 EXPERIMENTOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# DataFrame comparativo\n",
    "all_results = [exp0_results, exp1_results, exp2_results]\n",
    "df_comparison = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\nüìä Tabla Comparativa:\")\n",
    "print(df_comparison[['experiment', 'test_accuracy', 'test_macro_f1', 'test_top3_accuracy']].to_string(index=False))\n",
    "\n",
    "# An√°lisis de mejoras\n",
    "best_f1_idx = df_comparison['test_macro_f1'].idxmax()\n",
    "best_f1_exp = df_comparison.loc[best_f1_idx, 'experiment']\n",
    "\n",
    "improvement_exp1 = (exp1_results['test_macro_f1'] - exp0_results['test_macro_f1']) * 100\n",
    "improvement_exp2 = (exp2_results['test_macro_f1'] - exp0_results['test_macro_f1']) * 100\n",
    "\n",
    "print(f\"\\nüìà An√°lisis:\")\n",
    "print(f\"  Mejor Macro-F1: {best_f1_exp}\")\n",
    "print(f\"  Mejora Exp1 vs Baseline: {improvement_exp1:+.2f}%\")\n",
    "print(f\"  Mejora Exp2 vs Baseline: {improvement_exp2:+.2f}%\")\n",
    "\n",
    "# Guardar comparaci√≥n CSV\n",
    "comparison_csv_path = ROOT_PATH / 'experiments_comparison.csv'\n",
    "df_comparison.to_csv(comparison_csv_path, index=False)\n",
    "print(f\"\\n‚úì Guardado: {comparison_csv_path}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ COMPARACI√ìN COMPLETADA\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9b418",
   "metadata": {},
   "source": [
    "## üîç Verificaci√≥n Final de Archivos Generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9f99bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç VERIFICACI√ìN DE ARCHIVOS GENERADOS\n",
      "================================================================================\n",
      "\n",
      "üìÇ ROOT_PATH: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\n",
      "üìù Tipo de dataset: UMAP Embeddings\n",
      "\n",
      "üìÇ G2-RESULTS:\n",
      "  ‚ùå best_model.pt                  FALTA\n",
      "  ‚úÖ config.json                    (503 bytes)\n",
      "  ‚úÖ training_log.csv               (4,467 bytes)\n",
      "  ‚úÖ metrics.csv                    (205 bytes)\n",
      "  ‚úÖ per_class_metrics.csv          (1,778 bytes)\n",
      "  ‚úÖ confusion_matrix.csv           (1,835 bytes)\n",
      "  ‚úÖ confusion_matrix.png           (755,508 bytes)\n",
      "  ‚úÖ training_curves.png            (495,683 bytes)\n",
      "  ‚úÖ per_class_analysis.png         (449,823 bytes)\n",
      "  ‚úÖ RESUMEN.txt                    (1,652 bytes)\n",
      "\n",
      "üìÇ G2-RESULTS-CLASS-WEIGHTS:\n",
      "  ‚úÖ best_model.pt                  (19,365,429 bytes)\n",
      "  ‚úÖ config.json                    (508 bytes)\n",
      "  ‚úÖ training_log.csv               (3,504 bytes)\n",
      "  ‚úÖ metrics.csv                    (206 bytes)\n",
      "  ‚úÖ per_class_metrics.csv          (1,712 bytes)\n",
      "  ‚úÖ confusion_matrix.csv           (1,835 bytes)\n",
      "  ‚úÖ confusion_matrix.png           (753,160 bytes)\n",
      "  ‚úÖ training_curves.png            (422,909 bytes)\n",
      "  ‚úÖ per_class_analysis.png         (453,188 bytes)\n",
      "  ‚úÖ RESUMEN.txt                    (915 bytes)\n",
      "\n",
      "üìÇ G2-RESULTS-LABEL-SMOOTH:\n",
      "  ‚úÖ best_model.pt                  (19,365,429 bytes)\n",
      "  ‚úÖ config.json                    (512 bytes)\n",
      "  ‚úÖ training_log.csv               (4,067 bytes)\n",
      "  ‚úÖ metrics.csv                    (206 bytes)\n",
      "  ‚úÖ per_class_metrics.csv          (1,747 bytes)\n",
      "  ‚úÖ confusion_matrix.csv           (1,835 bytes)\n",
      "  ‚úÖ confusion_matrix.png           (814,392 bytes)\n",
      "  ‚úÖ training_curves.png            (413,248 bytes)\n",
      "  ‚úÖ per_class_analysis.png         (380,799 bytes)\n",
      "  ‚úÖ RESUMEN.txt                    (828 bytes)\n",
      "\n",
      "üìÇ Archivos de comparaci√≥n en ROOT_PATH:\n",
      "  ‚úÖ experiments_comparison.csv     (455 bytes)\n",
      "  ‚ùå experiments_comparison.png     FALTA\n",
      "\n",
      "================================================================================\n",
      "‚ö†Ô∏è VERIFICACI√ìN INCOMPLETA - Faltan algunos archivos:\n",
      "  - G2-RESULTS/best_model.pt\n",
      "  - ROOT/experiments_comparison.png\n",
      "================================================================================\n",
      "\n",
      "üìä Resumen:\n",
      "  ‚Ä¢ Experimentos: 3\n",
      "  ‚Ä¢ Archivos por experimento: 10\n",
      "  ‚Ä¢ Archivos de comparaci√≥n: 2\n",
      "  ‚Ä¢ Total archivos requeridos: 32\n",
      "  ‚Ä¢ ROOT_PATH: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G2-GCN EMBEDDINGS CONCATENADO\n"
     ]
    }
   ],
   "source": [
    "# VERIFICACI√ìN DE ARCHIVOS GENERADOS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç VERIFICACI√ìN DE ARCHIVOS GENERADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìÇ ROOT_PATH: {ROOT_PATH}\")\n",
    "print(f\"üìù Tipo de dataset: UMAP Embeddings\")\n",
    "\n",
    "# Archivos requeridos por experimento\n",
    "REQUIRED_FILES_PER_EXPERIMENT = [\n",
    "    'best_model.pt',\n",
    "    'config.json',\n",
    "    'training_log.csv',\n",
    "    'metrics.csv',\n",
    "    'per_class_metrics.csv',\n",
    "    'confusion_matrix.csv',\n",
    "    'confusion_matrix.png',\n",
    "    'training_curves.png',\n",
    "    'per_class_analysis.png',\n",
    "    'RESUMEN.txt'\n",
    "]\n",
    "\n",
    "# Archivos de comparaci√≥n en ROOT_PATH\n",
    "REQUIRED_FILES_BASE = [\n",
    "    'experiments_comparison.csv',\n",
    "    'experiments_comparison.png'\n",
    "]\n",
    "\n",
    "# Carpetas de experimentos\n",
    "experiment_folders = [\n",
    "    'G2-RESULTS',\n",
    "    'G2-RESULTS-CLASS-WEIGHTS',\n",
    "    'G2-RESULTS-LABEL-SMOOTH'\n",
    "]\n",
    "\n",
    "# Verificar cada experimento\n",
    "all_valid = True\n",
    "missing_files = []\n",
    "\n",
    "for folder_name in experiment_folders:\n",
    "    folder_path = ROOT_PATH / folder_name\n",
    "    print(f\"\\nüìÇ {folder_name}:\")\n",
    "    \n",
    "    if not folder_path.exists():\n",
    "        print(f\"  ‚ùå Carpeta no existe\")\n",
    "        all_valid = False\n",
    "        continue\n",
    "    \n",
    "    for required_file in REQUIRED_FILES_PER_EXPERIMENT:\n",
    "        file_path = folder_path / required_file\n",
    "        if file_path.exists():\n",
    "            file_size = file_path.stat().st_size\n",
    "            print(f\"  ‚úÖ {required_file:30s} ({file_size:,} bytes)\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {required_file:30s} FALTA\")\n",
    "            missing_files.append(f\"{folder_name}/{required_file}\")\n",
    "            all_valid = False\n",
    "\n",
    "# Verificar archivos de comparaci√≥n\n",
    "print(f\"\\nüìÇ Archivos de comparaci√≥n en ROOT_PATH:\")\n",
    "for required_file in REQUIRED_FILES_BASE:\n",
    "    file_path = ROOT_PATH / required_file\n",
    "    if file_path.exists():\n",
    "        file_size = file_path.stat().st_size\n",
    "        print(f\"  ‚úÖ {required_file:30s} ({file_size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {required_file:30s} FALTA\")\n",
    "        missing_files.append(f\"ROOT/{required_file}\")\n",
    "        all_valid = False\n",
    "\n",
    "# Resumen final\n",
    "print(f\"\\n{'='*80}\")\n",
    "if all_valid:\n",
    "    print(\"‚úÖ VERIFICACI√ìN EXITOSA - Todos los archivos se han generado correctamente\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è VERIFICACI√ìN INCOMPLETA - Faltan algunos archivos:\")\n",
    "    for missing in missing_files:\n",
    "        print(f\"  - {missing}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Estad√≠sticas\n",
    "total_required = len(experiment_folders) * len(REQUIRED_FILES_PER_EXPERIMENT) + len(REQUIRED_FILES_BASE)\n",
    "print(f\"\\nüìä Resumen:\")\n",
    "print(f\"  ‚Ä¢ Experimentos: {len(experiment_folders)}\")\n",
    "print(f\"  ‚Ä¢ Archivos por experimento: {len(REQUIRED_FILES_PER_EXPERIMENT)}\")\n",
    "print(f\"  ‚Ä¢ Archivos de comparaci√≥n: {len(REQUIRED_FILES_BASE)}\")\n",
    "print(f\"  ‚Ä¢ Total archivos requeridos: {total_required}\")\n",
    "print(f\"  ‚Ä¢ ROOT_PATH: {ROOT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
