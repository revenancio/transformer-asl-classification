{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8daa1d5",
   "metadata": {},
   "source": [
    "# üöÄ Experimento G4 - Transformer Encoder-Only para ASL Classification\n",
    "\n",
    "## 1. Introducci√≥n\n",
    "\n",
    "### üéØ Objetivo\n",
    "Entrenar y evaluar una arquitectura Transformer Encoder-Only para clasificaci√≥n de gestos de American Sign Language (ASL) utilizando segmentos UMAP, donde cada video completo es representado por un √∫nico vector embedding en lugar de una secuencia temporal.\n",
    "\n",
    "### üìä Dataset\n",
    "- **Fuente:** `dataset_umap_segments.npz`\n",
    "- **Muestras:** 868 videos de gestos ASL\n",
    "- **Dimensiones:** Cada video ‚Üí 1 embedding de N dimensiones (segmento completo)\n",
    "- **Clases:** 30 gestos diferentes de ASL\n",
    "- **Preprocesamiento:** Reducci√≥n dimensional UMAP aplicada a nivel de video completo (no frame-a-frame)\n",
    "\n",
    "### üíª Hardware\n",
    "- **GPU:** Detectada autom√°ticamente (CUDA disponible)\n",
    "- **Memoria:** Optimizada para entrenamiento con batch size 32\n",
    "\n",
    "### ü§ñ Sistema de Rutas Autom√°ticas\n",
    "Este notebook detecta **autom√°ticamente** la carpeta de destino: **`G4-QDRANT (Video-Base)/`**\n",
    "\n",
    "Genera 3 carpetas de experimentos:\n",
    "- `G4-RESULTS/` - Configuraci√≥n baseline\n",
    "- `G4-RESULTS-CLASS-WEIGHTS/` - Con balanceo de clases\n",
    "- `G4-RESULTS-LABEL-SMOOTH/` - Con suavizado de etiquetas\n",
    "\n",
    "Cada carpeta contiene 10 archivos de resultados + 2 archivos de comparaci√≥n en ROOT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63cb470a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce GTX 1660 SUPER\n",
      "VRAM: 6.44 GB\n",
      "\n",
      "================================================================================\n",
      "üéØ SISTEMA G4 - UMAP SEGMENTS\n",
      "================================================================================\n",
      "üìÅ ROOT_PATH: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-QDRANT (Video-Base)\n",
      "Dataset: dataset_umap_segments.npz\n",
      "Arquitectura: Transformer Encoder-Only\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, top_k_accuracy_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# üîß CONFIGURACI√ìN AUTOM√ÅTICA DE RUTAS (G4)\n",
    "ROOT_PATH = Path(r'C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-QDRANT (Video-Base)')\n",
    "ROOT_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ SISTEMA G4 - UMAP SEGMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÅ ROOT_PATH: {ROOT_PATH}\")\n",
    "print(\"Dataset: dataset_umap_segments.npz\")\n",
    "print(\"Arquitectura: Transformer Encoder-Only\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8eeb327b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: X=(868, 12, 300), y=(868,)\n",
      "Masks shape: (868, 12)\n",
      "Filenames: 868\n",
      "Classes: 30\n",
      "Unique classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "Distribuci√≥n de clases:\n",
      "  Clase 0: 78 muestras\n",
      "  Clase 1: 10 muestras\n",
      "  Clase 2: 22 muestras\n",
      "  Clase 3: 22 muestras\n",
      "  Clase 4: 18 muestras\n",
      "  Clase 5: 18 muestras\n",
      "  Clase 6: 28 muestras\n",
      "  Clase 7: 18 muestras\n",
      "  Clase 8: 22 muestras\n",
      "  Clase 9: 16 muestras\n",
      "  Clase 10: 12 muestras\n",
      "  Clase 11: 18 muestras\n",
      "  Clase 12: 28 muestras\n",
      "  Clase 13: 24 muestras\n",
      "  Clase 14: 28 muestras\n",
      "  Clase 15: 18 muestras\n",
      "  Clase 16: 18 muestras\n",
      "  Clase 17: 82 muestras\n",
      "  Clase 18: 76 muestras\n",
      "  Clase 19: 18 muestras\n",
      "  Clase 20: 28 muestras\n",
      "  Clase 21: 18 muestras\n",
      "  Clase 22: 18 muestras\n",
      "  Clase 23: 22 muestras\n",
      "  Clase 24: 18 muestras\n",
      "  Clase 25: 68 muestras\n",
      "  Clase 26: 22 muestras\n",
      "  Clase 27: 64 muestras\n",
      "  Clase 28: 18 muestras\n",
      "  Clase 29: 18 muestras\n",
      "\n",
      "Dimensiones:\n",
      "  Muestras: 868\n",
      "  Seq length: 12\n",
      "  Input dim: 300\n",
      "  Num classes: 30\n"
     ]
    }
   ],
   "source": [
    "# 1. CARGAR DATASET UMAP SEGMENTS\n",
    "dataset_path = Path(r'C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\daataset\\dataset_umap_segments.npz')\n",
    "data = np.load(dataset_path, allow_pickle=True)\n",
    "\n",
    "X = data['X']  # Embeddings UMAP de segmentos\n",
    "y = data['y']  # Labels\n",
    "masks = data['masks']  # M√°scaras de padding\n",
    "filenames = data['filenames']  # Nombres de archivos\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Masks shape: {masks.shape}\")\n",
    "print(f\"Filenames: {len(filenames)}\")\n",
    "print(f\"Classes: {len(np.unique(y))}\")\n",
    "print(f\"Unique classes: {np.unique(y)}\")\n",
    "\n",
    "# Informaci√≥n de las clases\n",
    "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "print(f\"\\nDistribuci√≥n de clases:\")\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    print(f\"  Clase {cls}: {count} muestras\")\n",
    "\n",
    "# Dimensiones del dataset\n",
    "num_samples, seq_len, input_dim = X.shape\n",
    "num_classes = len(np.unique(y))\n",
    "print(f\"\\nDimensiones:\")\n",
    "print(f\"  Muestras: {num_samples}\")\n",
    "print(f\"  Seq length: {seq_len}\")\n",
    "print(f\"  Input dim: {input_dim}\")\n",
    "print(f\"  Num classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f7c6b4",
   "metadata": {},
   "source": [
    "## 2. Metodolog√≠a\n",
    "\n",
    "### üèóÔ∏è Arquitectura del Modelo\n",
    "Transformer Encoder-Only adaptado para segmentos de video:\n",
    "- **Capa de entrada:** Linear projection (N ‚Üí 256 dimensiones, donde N = dim del embedding UMAP)\n",
    "- **Positional Encoding:** Aprendible para longitud de secuencia variable\n",
    "- **Encoder Layers:** 4 capas transformer\n",
    "  - Multi-Head Attention (4 heads)\n",
    "  - Feed-Forward Networks (512 dimensiones)\n",
    "  - Layer Normalization y Residual Connections\n",
    "- **Pooling:** Masked mean pooling (considera m√°scaras de padding)\n",
    "- **Clasificador:** MLP de 2 capas (256 ‚Üí 128 ‚Üí 30)\n",
    "- **Activaci√≥n:** GELU\n",
    "- **Par√°metros:** Depende de dim de entrada\n",
    "\n",
    "### ‚öôÔ∏è Configuraciones de Entrenamiento\n",
    "Todos los experimentos comparten:\n",
    "- **Optimizador:** AdamW (lr=5e-4, weight_decay=1e-4)\n",
    "- **Scheduler:** CosineAnnealingWarmRestarts (T_0=10, T_mult=2)\n",
    "- **Batch size:** 32\n",
    "- **√âpocas:** 100 (con early stopping patience=15)\n",
    "- **Split:** 70% train, 15% val, 15% test\n",
    "\n",
    "### üß™ Experimentos Realizados\n",
    "\n",
    "**Diferencia clave:** Este experimento usa segmentos completos (1 embedding por video) en lugar de secuencias frame-a-frame, representando cada video como un punto √∫nico en el espacio UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "085bb3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (555, 12, 300), Val: (139, 12, 300), Test: (174, 12, 300)\n",
      "Batches - Train: 70, Val: 18, Test: 22\n"
     ]
    }
   ],
   "source": [
    "# 2. DATASET PYTORCH\n",
    "class VideoTransformerDataset(Dataset):\n",
    "    def __init__(self, X, y, masks):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.masks = torch.BoolTensor(masks)  # True = v√°lido, False = padding\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sequence': self.X[idx],\n",
    "            'label': self.y[idx],\n",
    "            'mask': self.masks[idx]\n",
    "        }\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test, masks_train, masks_test = train_test_split(\n",
    "    X, y, masks, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val, masks_train, masks_val = train_test_split(\n",
    "    X_train, y_train, masks_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 8\n",
    "train_dataset = VideoTransformerDataset(X_train, y_train, masks_train)\n",
    "val_dataset = VideoTransformerDataset(X_val, y_val, masks_val)\n",
    "test_dataset = VideoTransformerDataset(X_test, y_test, masks_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Batches - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a3ec2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo: Transformer Encoder-Only\n",
      "  Total params: 2,225,822\n",
      "  Trainable params: 2,225,822\n",
      "\n",
      "TransformerEncoderOnlyClassifier(\n",
      "  (input_projection): Linear(in_features=300, out_features=256, bias=True)\n",
      "  (pos_encoding): LearnablePositionalEncoding()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=30, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3. ARCHITECTURE: TRANSFORMER ENCODER-ONLY\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding aprendible\"\"\"\n",
    "    def __init__(self, d_model, max_len=96):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "        nn.init.normal_(self.pe, mean=0, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerEncoderOnlyClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder-Only para clasificaci√≥n de secuencias temporales\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=300,\n",
    "        d_model=256,\n",
    "        num_heads=4,\n",
    "        num_layers=4,\n",
    "        dim_feedforward=512,\n",
    "        dropout=0.1,\n",
    "        num_classes=30,\n",
    "        max_seq_len=96,\n",
    "        mlp_dropout=0.2,\n",
    "        activation='gelu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 1. Proyecci√≥n inicial\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # 2. Positional Encoding aprendible\n",
    "        self.pos_encoding = LearnablePositionalEncoding(d_model, max_seq_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 3. Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        # 4. Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(mlp_dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        # 1. Proyecci√≥n inicial\n",
    "        x = self.input_projection(src)\n",
    "        \n",
    "        # 2. Positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 3. Transformer encoder\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # 4. Masked mean pooling\n",
    "        if src_key_padding_mask is not None:\n",
    "            mask_float = (~src_key_padding_mask).float().unsqueeze(-1)\n",
    "            x_masked = x * mask_float\n",
    "            sum_masked = x_masked.sum(dim=1)\n",
    "            count_valid = mask_float.sum(dim=1)\n",
    "            x_pooled = sum_masked / (count_valid + 1e-9)\n",
    "        else:\n",
    "            x_pooled = x.mean(dim=1)\n",
    "        \n",
    "        # 5. Clasificador\n",
    "        logits = self.classifier(x_pooled)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Crear modelo\n",
    "model = TransformerEncoderOnlyClassifier(\n",
    "    input_dim=input_dim,\n",
    "    d_model=256,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.1,\n",
    "    num_classes=num_classes,\n",
    "    max_seq_len=seq_len,\n",
    "    mlp_dropout=0.2,\n",
    "    activation='gelu'\n",
    ").to(device)\n",
    "\n",
    "# Contar par√°metros\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModelo: Transformer Encoder-Only\")\n",
    "print(f\"  Total params: {total_params:,}\")\n",
    "print(f\"  Trainable params: {trainable_params:,}\")\n",
    "print(f\"\\n{model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "37a60811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuraci√≥n de entrenamiento:\n",
      "  experiment: G4-QDRANT-Video-Base\n",
      "  dataset: dataset_umap_segments.npz\n",
      "  architecture: TransformerEncoderOnly\n",
      "  input_dim: 300\n",
      "  d_model: 256\n",
      "  num_heads: 4\n",
      "  num_layers: 4\n",
      "  dim_feedforward: 512\n",
      "  dropout: 0.1\n",
      "  mlp_dropout: 0.2\n",
      "  num_classes: 30\n",
      "  max_seq_len: 12\n",
      "  optimizer: AdamW\n",
      "  lr: 0.0001\n",
      "  weight_decay: 0.0001\n",
      "  loss: CrossEntropyLoss\n",
      "  label_smoothing: 0.0\n",
      "  batch_size: 8\n",
      "  max_epochs: 50\n",
      "  early_stopping_patience: 8\n",
      "  gradient_clip: 1.0\n",
      "  scheduler: CosineAnnealingWarmRestarts\n",
      "  device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 4. CONFIGURACI√ìN DE ENTRENAMIENTO\n",
    "config = {\n",
    "    'experiment': 'G4-QDRANT-Video-Base',\n",
    "    'dataset': 'dataset_umap_segments.npz',\n",
    "    'architecture': 'TransformerEncoderOnly',\n",
    "    'input_dim': input_dim,\n",
    "    'd_model': 256,\n",
    "    'num_heads': 4,\n",
    "    'num_layers': 4,\n",
    "    'dim_feedforward': 512,\n",
    "    'dropout': 0.1,\n",
    "    'mlp_dropout': 0.2,\n",
    "    'num_classes': num_classes,\n",
    "    'max_seq_len': seq_len,\n",
    "    'optimizer': 'AdamW',\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'loss': 'CrossEntropyLoss',\n",
    "    'label_smoothing': 0.0,\n",
    "    'batch_size': 8,\n",
    "    'max_epochs': 50,\n",
    "    'early_stopping_patience': 8,\n",
    "    'gradient_clip': 1.0,\n",
    "    'scheduler': 'CosineAnnealingWarmRestarts',\n",
    "    'device': str(device),\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'total_params': total_params,\n",
    "    'trainable_params': trainable_params\n",
    "}\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['lr'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# LR Scheduler\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "print(\"\\nConfiguraci√≥n de entrenamiento:\")\n",
    "for k, v in config.items():\n",
    "    if k not in ['timestamp', 'total_params', 'trainable_params']:\n",
    "        print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f4ff641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funciones de entrenamiento definidas\n"
     ]
    }
   ],
   "source": [
    "# 5. FUNCIONES DE ENTRENAMIENTO Y EVALUACI√ìN\n",
    "def train_epoch(model, loader, criterion, optimizer, device, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        sequences = batch['sequence'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(sequences, src_key_padding_mask=~masks)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = total_loss / len(loader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        sequences = batch['sequence'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        \n",
    "        logits = model(sequences, src_key_padding_mask=~masks)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_logits.extend(logits.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = total_loss / len(loader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc, np.array(all_preds), np.array(all_labels), np.array(all_logits)\n",
    "\n",
    "print(\"‚úÖ Funciones de entrenamiento definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c1101858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Iniciando entrenamiento - Max epochs: 50 | Early stopping: 8\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50 | Train Loss: 3.3039 | Train Acc: 0.0739 | Val Loss: 3.2452 | Val Acc: 0.0863 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/50 | Train Loss: 3.2190 | Train Acc: 0.0847 | Val Loss: 3.2183 | Val Acc: 0.1079 | LR: 6.58e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/50 | Train Loss: 3.2022 | Train Acc: 0.0937 | Val Loss: 3.2103 | Val Acc: 0.1151 | LR: 3.42e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/50 | Train Loss: 2.9682 | Train Acc: 0.1586 | Val Loss: 3.1334 | Val Acc: 0.1007 | LR: 6.58e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping at epoch 17\n",
      "\n",
      "================================================================================\n",
      "Entrenamiento completado\n",
      "Mejor modelo: Epoch 9 | Val Acc: 0.1295\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# 6. ENTRENAMIENTO\n",
    "training_log = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "patience_counter = 0\n",
    "max_epochs = config['max_epochs']\n",
    "early_stopping_patience = config['early_stopping_patience']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Iniciando entrenamiento - Max epochs: {max_epochs} | Early stopping: {early_stopping_patience}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Val\n",
    "    val_loss, val_acc, _, _, _ = eval_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # LR Scheduler\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log\n",
    "    training_log['epoch'].append(epoch)\n",
    "    training_log['train_loss'].append(train_loss)\n",
    "    training_log['train_acc'].append(train_acc)\n",
    "    training_log['val_loss'].append(val_loss)\n",
    "    training_log['val_acc'].append(val_acc)\n",
    "    training_log['lr'].append(current_lr)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        patience_counter = 0\n",
    "        # Guardar mejor modelo en memoria\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Print\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{max_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n",
    "              f\"LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping trigger\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Cargar mejor modelo\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Entrenamiento completado\")\n",
    "print(f\"Mejor modelo: Epoch {best_epoch+1} | Val Acc: {best_val_acc:.4f}\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9e324df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluaci√≥n en Test Set\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Advertencia: Se detectaron NaN en logits. Limpiando datos...\n",
      "   Muestras v√°lidas: 172/174\n",
      "M√©tricas en Test:\n",
      "  Test Accuracy:    0.1609\n",
      "  Macro F1-Score:   0.0285\n",
      "  Macro Precision:  0.0175\n",
      "  Macro Recall:     0.0853\n",
      "  Top-3 Accuracy:   0.4012\n",
      "  Test Loss:        nan\n",
      "\n",
      "  Confusion Matrix: (30, 30)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# 7. EVALUACI√ìN EN TEST SET\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Evaluaci√≥n en Test Set\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "test_loss, test_acc, test_preds, test_labels, test_logits = eval_epoch(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "# Verificar NaN en logits\n",
    "if np.isnan(test_logits).any():\n",
    "    print(f\"‚ö†Ô∏è  Advertencia: Se detectaron NaN en logits. Limpiando datos...\")\n",
    "    # Encontrar √≠ndices v√°lidos (sin NaN)\n",
    "    valid_mask = ~np.isnan(test_logits).any(axis=1)\n",
    "    test_logits_clean = test_logits[valid_mask]\n",
    "    test_labels_clean = test_labels[valid_mask]\n",
    "    test_preds_clean = test_preds[valid_mask]\n",
    "    print(f\"   Muestras v√°lidas: {valid_mask.sum()}/{len(valid_mask)}\")\n",
    "else:\n",
    "    test_logits_clean = test_logits\n",
    "    test_labels_clean = test_labels\n",
    "    test_preds_clean = test_preds\n",
    "\n",
    "# M√©tricas adicionales\n",
    "macro_f1 = f1_score(test_labels_clean, test_preds_clean, average='macro', zero_division=0)\n",
    "macro_precision = precision_score(test_labels_clean, test_preds_clean, average='macro', zero_division=0)\n",
    "macro_recall = recall_score(test_labels_clean, test_preds_clean, average='macro', zero_division=0)\n",
    "top3_acc = top_k_accuracy_score(test_labels_clean, test_logits_clean, k=3, labels=np.arange(num_classes))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(test_labels_clean, test_preds_clean)\n",
    "\n",
    "print(f\"M√©tricas en Test:\")\n",
    "print(f\"  Test Accuracy:    {test_acc:.4f}\")\n",
    "print(f\"  Macro F1-Score:   {macro_f1:.4f}\")\n",
    "print(f\"  Macro Precision:  {macro_precision:.4f}\")\n",
    "print(f\"  Macro Recall:     {macro_recall:.4f}\")\n",
    "print(f\"  Top-3 Accuracy:   {top3_acc:.4f}\")\n",
    "print(f\"  Test Loss:        {test_loss:.4f}\")\n",
    "print(f\"\\n  Confusion Matrix: {cm.shape}\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f46cb709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Guardando resultados BASELINE en: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-QDRANT (Video-Base)\\G4-RESULTS\n",
      "================================================================================\n",
      "\n",
      "‚úì Guardado: training_log.csv\n",
      "‚úì Guardado: metrics.csv\n",
      "‚úì Guardado: per_class_metrics.csv\n",
      "‚úì Guardado: confusion_matrix.csv\n",
      "‚úì Guardado: config.json\n",
      "‚úì Guardado: best_model.pt\n",
      "‚úì Guardado: confusion_matrix.png\n",
      "‚úì Guardado: training_curves.png\n",
      "‚úì Guardado: per_class_analysis.png\n",
      "‚úì Guardado: RESUMEN.txt\n",
      "\n",
      "================================================================================\n",
      "‚úÖ BASELINE COMPLETADO - 10 archivos guardados\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. GUARDAR RESULTADOS BASELINE (ESTRUCTURA G4)\n",
    "output_dir = ROOT_PATH / 'G4-RESULTS'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Guardando resultados BASELINE en: {output_dir}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Extraer nombres √∫nicos de clases (30 clases desde filenames)\n",
    "unique_classes_names = []\n",
    "for class_id in sorted(np.unique(y)):\n",
    "    # Encontrar primer √≠ndice de esta clase\n",
    "    idx = np.where(y == class_id)[0][0]\n",
    "    class_name = str(filenames[idx]).replace('.json', '').split('_')[0]\n",
    "    unique_classes_names.append(class_name)\n",
    "\n",
    "# 1. Training Log CSV\n",
    "pd.DataFrame(training_log).to_csv(output_dir / 'training_log.csv', index=False)\n",
    "print(f\"‚úì Guardado: training_log.csv\")\n",
    "\n",
    "# 2. Metrics CSV (m√©tricas principales)\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Macro-F1', 'Macro-Precision', 'Macro-Recall', 'Top-3 Accuracy', 'Test Loss', 'Best Epoch', 'Best Val Acc'],\n",
    "    'Value': [test_acc, macro_f1, macro_precision, macro_recall, top3_acc, test_loss, best_epoch+1, best_val_acc]\n",
    "})\n",
    "results_df.to_csv(output_dir / 'metrics.csv', index=False)\n",
    "print(f\"‚úì Guardado: metrics.csv\")\n",
    "\n",
    "# 3. Per-Class Metrics CSV\n",
    "per_class_report = classification_report(\n",
    "    test_labels_clean, \n",
    "    test_preds_clean, \n",
    "    labels=list(range(num_classes)),\n",
    "    target_names=unique_classes_names,\n",
    "    zero_division=0,\n",
    "    output_dict=True\n",
    ")\n",
    "per_class_df = pd.DataFrame(per_class_report).transpose()\n",
    "per_class_df.to_csv(output_dir / 'per_class_metrics.csv')\n",
    "print(f\"‚úì Guardado: per_class_metrics.csv\")\n",
    "\n",
    "# 4. Confusion Matrix CSV\n",
    "pd.DataFrame(cm).to_csv(output_dir / 'confusion_matrix.csv', index=False, header=False)\n",
    "print(f\"‚úì Guardado: confusion_matrix.csv\")\n",
    "\n",
    "# 5. Config JSON\n",
    "config_save = config.copy()\n",
    "config_save.update({\n",
    "    'best_epoch': int(best_epoch),\n",
    "    'best_val_acc': float(best_val_acc),\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_macro_f1': float(macro_f1),\n",
    "    'test_macro_precision': float(macro_precision),\n",
    "    'test_macro_recall': float(macro_recall),\n",
    "    'test_top3_accuracy': float(top3_acc),\n",
    "    'test_loss': float(test_loss)\n",
    "})\n",
    "\n",
    "with open(output_dir / 'config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_save, f, indent=2, ensure_ascii=False)\n",
    "print(f\"‚úì Guardado: config.json\")\n",
    "\n",
    "# 6. Modelo\n",
    "torch.save(model.state_dict(), output_dir / 'best_model.pt')\n",
    "print(f\"‚úì Guardado: best_model.pt\")\n",
    "\n",
    "# 7. Confusion Matrix PNG\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=unique_classes_names, \n",
    "            yticklabels=unique_classes_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Confusion Matrix - Test Set\\n{config[\"experiment\"]}', fontsize=14, pad=15)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "plt.yticks(rotation=0, fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"‚úì Guardado: confusion_matrix.png\")\n",
    "\n",
    "# 8. Training Curves PNG\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(training_log['epoch'], training_log['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(training_log['epoch'], training_log['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].axvline(x=best_epoch, color='red', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch+1})')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(training_log['epoch'], training_log['train_acc'], label='Train Acc', linewidth=2)\n",
    "axes[0, 1].plot(training_log['epoch'], training_log['val_acc'], label='Val Acc', linewidth=2)\n",
    "axes[0, 1].axvline(x=best_epoch, color='red', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch+1})')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Training & Validation Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "axes[1, 0].plot(training_log['epoch'], training_log['lr'], color='orange', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].set_title('Learning Rate Schedule')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss Difference\n",
    "loss_diff = np.array(training_log['val_loss']) - np.array(training_log['train_loss'])\n",
    "axes[1, 1].plot(training_log['epoch'], loss_diff, color='purple', linewidth=2)\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Val Loss - Train Loss')\n",
    "axes[1, 1].set_title('Overfitting Indicator')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Training Curves - {config[\"experiment\"]}', fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"‚úì Guardado: training_curves.png\")\n",
    "\n",
    "# 9. Per-Class Analysis PNG\n",
    "per_class_metrics = per_class_df.iloc[:-3][['precision', 'recall', 'f1-score']].values\n",
    "class_names_short = unique_classes_names\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
    "\n",
    "# Precision\n",
    "axes[0].barh(class_names_short, per_class_metrics[:, 0], color='skyblue')\n",
    "axes[0].set_xlabel('Precision')\n",
    "axes[0].set_title('Per-Class Precision')\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Recall\n",
    "axes[1].barh(class_names_short, per_class_metrics[:, 1], color='lightcoral')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_title('Per-Class Recall')\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# F1-Score\n",
    "axes[2].barh(class_names_short, per_class_metrics[:, 2], color='lightgreen')\n",
    "axes[2].set_xlabel('F1-Score')\n",
    "axes[2].set_title('Per-Class F1-Score')\n",
    "axes[2].set_xlim(0, 1)\n",
    "axes[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle(f'Per-Class Metrics - {config[\"experiment\"]}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'per_class_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"‚úì Guardado: per_class_analysis.png\")\n",
    "\n",
    "# 10. RESUMEN.txt\n",
    "with open(output_dir / 'RESUMEN.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(f\"RESUMEN EJECUTIVO - {config['experiment']}\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"DATASET:\\n\")\n",
    "    f.write(f\"  Archivo: {config['dataset']}\\n\")\n",
    "    f.write(f\"  Muestras totales: {num_samples}\\n\")\n",
    "    f.write(f\"  Train/Val/Test: {len(X_train)}/{len(X_val)}/{len(X_test)}\\n\")\n",
    "    f.write(f\"  Secuencia: {seq_len} frames\\n\")\n",
    "    f.write(f\"  Dimensi√≥n: {input_dim}\\n\")\n",
    "    f.write(f\"  Clases: {num_classes}\\n\\n\")\n",
    "    \n",
    "    f.write(\"ARQUITECTURA:\\n\")\n",
    "    f.write(f\"  Modelo: {config['architecture']}\\n\")\n",
    "    f.write(f\"  d_model: {config['d_model']}\\n\")\n",
    "    f.write(f\"  Num heads: {config['num_heads']}\\n\")\n",
    "    f.write(f\"  Num layers: {config['num_layers']}\\n\")\n",
    "    f.write(f\"  FFN dim: {config['dim_feedforward']}\\n\")\n",
    "    f.write(f\"  Dropout: {config['dropout']}\\n\")\n",
    "    f.write(f\"  Total params: {total_params:,}\\n\\n\")\n",
    "    \n",
    "    f.write(\"ENTRENAMIENTO:\\n\")\n",
    "    f.write(f\"  Optimizer: {config['optimizer']}\\n\")\n",
    "    f.write(f\"  Learning rate: {config['lr']}\\n\")\n",
    "    f.write(f\"  Batch size: {config['batch_size']}\\n\")\n",
    "    f.write(f\"  Max epochs: {config['max_epochs']}\\n\")\n",
    "    f.write(f\"  Early stopping: {config['early_stopping_patience']}\\n\")\n",
    "    f.write(f\"  Best epoch: {best_epoch+1}\\n\")\n",
    "    f.write(f\"  Best val accuracy: {best_val_acc:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"RESULTADOS TEST:\\n\")\n",
    "    f.write(f\"  Test Accuracy:    {test_acc:.4f}\\n\")\n",
    "    f.write(f\"  Macro F1-Score:   {macro_f1:.4f}\\n\")\n",
    "    f.write(f\"  Macro Precision:  {macro_precision:.4f}\\n\")\n",
    "    f.write(f\"  Macro Recall:     {macro_recall:.4f}\\n\")\n",
    "    f.write(f\"  Top-3 Accuracy:   {top3_acc:.4f}\\n\")\n",
    "    f.write(f\"  Test Loss:        {test_loss:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"ARCHIVOS GENERADOS (10):\\n\")\n",
    "    f.write(\"  1. training_log.csv\\n\")\n",
    "    f.write(\"  2. metrics.csv\\n\")\n",
    "    f.write(\"  3. per_class_metrics.csv\\n\")\n",
    "    f.write(\"  4. confusion_matrix.csv\\n\")\n",
    "    f.write(\"  5. config.json\\n\")\n",
    "    f.write(\"  6. best_model.pt\\n\")\n",
    "    f.write(\"  7. confusion_matrix.png\\n\")\n",
    "    f.write(\"  8. training_curves.png\\n\")\n",
    "    f.write(\"  9. per_class_analysis.png\\n\")\n",
    "    f.write(\" 10. RESUMEN.txt\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"‚úì Guardado: RESUMEN.txt\")\n",
    "\n",
    "# Guardar resultados del experimento baseline\n",
    "exp0_results = {\n",
    "    'experiment': 'G4-QDRANT (Video-Base)',\n",
    "    'dropout': config['dropout'],\n",
    "    'class_weights': False,\n",
    "    'label_smoothing': config['label_smoothing'],\n",
    "    'test_accuracy': test_acc,\n",
    "    'test_macro_f1': macro_f1,\n",
    "    'test_top3_accuracy': top3_acc,\n",
    "    'test_loss': test_loss,\n",
    "    'best_epoch': best_epoch+1,\n",
    "    'best_val_acc': best_val_acc\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ BASELINE COMPLETADO - 10 archivos guardados\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb4e76",
   "metadata": {},
   "source": [
    "## 3. Resultados\n",
    "\n",
    "### üß™ Experimento 0 (G4-QDRANT Video-Base) - Baseline\n",
    "**Configuraci√≥n:**\n",
    "- Dropout: 0.1\n",
    "- Sin class weights\n",
    "- Sin label smoothing\n",
    "- Representaci√≥n: Segmentos UMAP (1 embedding por video)\n",
    "\n",
    "**Resultados obtenidos:** Ver m√©tricas en las celdas anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3f87c467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Funci√≥n create_model_with_dropout definida\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n para crear modelo con dropout configurable\n",
    "def create_model_with_dropout(dropout_config=0.1):\n",
    "    \"\"\"Crea modelo con configuraci√≥n espec√≠fica de dropout\"\"\"\n",
    "    model = TransformerEncoderOnlyClassifier(\n",
    "        input_dim=input_dim,\n",
    "        d_model=256,\n",
    "        num_heads=4,\n",
    "        num_layers=4,\n",
    "        dim_feedforward=512,\n",
    "        dropout=dropout_config,\n",
    "        num_classes=num_classes,\n",
    "        max_seq_len=seq_len,\n",
    "        mlp_dropout=0.2,\n",
    "        activation='gelu'\n",
    "    ).to(device)\n",
    "    return model\n",
    "\n",
    "print(\"‚úì Funci√≥n create_model_with_dropout definida\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419d523",
   "metadata": {},
   "source": [
    "### üß™ Experimento 1 - CLASS-WEIGHTS\n",
    "**Configuraci√≥n:**\n",
    "- Class Weights: Balanceo autom√°tico basado en distribuci√≥n de clases\n",
    "- Dropout: 0.3 (mayor regularizaci√≥n)\n",
    "- Sin label smoothing\n",
    "- Representaci√≥n: Segmentos UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a9388319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üß™ Iniciando Experimento 1: CLASS-WEIGHTS\n",
      "================================================================================\n",
      "üìÅ Directorio: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-QDRANT (Video-Base)\\G4-RESULTS-CLASS-WEIGHTS\n",
      "‚úì Class weights calculados: min=0.35, max=2.64\n",
      "‚úì Modelo creado con dropout 0.3\n",
      "\n",
      "================================================================================\n",
      "Iniciando entrenamiento EXP1 - Max epochs: 50\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50 | Train Loss: 3.4473 | Train Acc: 0.0234 | Val Loss: 3.4065 | Val Acc: 0.0288 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/50 | Train Loss: 3.4167 | Train Acc: 0.0613 | Val Loss: 3.4008 | Val Acc: 0.0791 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/50 | Train Loss: 3.4065 | Train Acc: 0.0342 | Val Loss: 3.3957 | Val Acc: 0.0791 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/50 | Train Loss: 3.2697 | Train Acc: 0.0955 | Val Loss: 3.2053 | Val Acc: 0.1007 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/50 | Train Loss: 3.1441 | Train Acc: 0.1009 | Val Loss: 3.0026 | Val Acc: 0.1223 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25/50 | Train Loss: 3.0308 | Train Acc: 0.1153 | Val Loss: 2.8705 | Val Acc: 0.1151 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30/50 | Train Loss: 2.8881 | Train Acc: 0.1333 | Val Loss: 2.7199 | Val Acc: 0.1511 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping at epoch 34\n",
      "\n",
      "================================================================================\n",
      "Entrenamiento EXP1 completado\n",
      "Mejor modelo: Epoch 26 | Val Acc: 0.1511\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# EXPERIMENTO 1: CLASS-WEIGHTS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß™ Iniciando Experimento 1: CLASS-WEIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configurar directorio de salida\n",
    "output_dir_exp1 = ROOT_PATH / 'G4-RESULTS-CLASS-WEIGHTS'\n",
    "output_dir_exp1.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ Directorio: {output_dir_exp1}\")\n",
    "\n",
    "# Calcular class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.FloatTensor(class_weights_array).to(device)\n",
    "print(f\"‚úì Class weights calculados: min={class_weights_array.min():.2f}, max={class_weights_array.max():.2f}\")\n",
    "\n",
    "# Crear modelo con dropout 0.3\n",
    "model_exp1 = create_model_with_dropout(dropout_config=0.3)\n",
    "print(f\"‚úì Modelo creado con dropout 0.3\")\n",
    "\n",
    "# Configurar loss con class weights\n",
    "criterion_exp1 = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.0)\n",
    "\n",
    "# Optimizer y Scheduler\n",
    "optimizer_exp1 = AdamW(model_exp1.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler_exp1 = ReduceLROnPlateau(optimizer_exp1, mode='max', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Training log\n",
    "training_log_exp1 = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_acc_exp1 = 0.0\n",
    "best_epoch_exp1 = 0\n",
    "patience_counter_exp1 = 0\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Iniciando entrenamiento EXP1 - Max epochs: {max_epochs}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # Train\n",
    "    train_loss_exp1, train_acc_exp1 = train_epoch(model_exp1, train_loader, criterion_exp1, optimizer_exp1, device)\n",
    "    \n",
    "    # Val\n",
    "    val_loss_exp1, val_acc_exp1, _, _, _ = eval_epoch(model_exp1, val_loader, criterion_exp1, device)\n",
    "    \n",
    "    # LR Scheduler\n",
    "    current_lr_exp1 = optimizer_exp1.param_groups[0]['lr']\n",
    "    scheduler_exp1.step(val_acc_exp1)\n",
    "    \n",
    "    # Log\n",
    "    training_log_exp1['epoch'].append(epoch)\n",
    "    training_log_exp1['train_loss'].append(train_loss_exp1)\n",
    "    training_log_exp1['train_acc'].append(train_acc_exp1)\n",
    "    training_log_exp1['val_loss'].append(val_loss_exp1)\n",
    "    training_log_exp1['val_acc'].append(val_acc_exp1)\n",
    "    training_log_exp1['lr'].append(current_lr_exp1)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc_exp1 > best_val_acc_exp1:\n",
    "        best_val_acc_exp1 = val_acc_exp1\n",
    "        best_epoch_exp1 = epoch\n",
    "        patience_counter_exp1 = 0\n",
    "        best_model_state_exp1 = model_exp1.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter_exp1 += 1\n",
    "    \n",
    "    # Print\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{max_epochs} | \"\n",
    "              f\"Train Loss: {train_loss_exp1:.4f} | Train Acc: {train_acc_exp1:.4f} | \"\n",
    "              f\"Val Loss: {val_loss_exp1:.4f} | Val Acc: {val_acc_exp1:.4f} | \"\n",
    "              f\"LR: {current_lr_exp1:.2e}\")\n",
    "    \n",
    "    if patience_counter_exp1 >= early_stopping_patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Cargar mejor modelo\n",
    "model_exp1.load_state_dict(best_model_state_exp1)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Entrenamiento EXP1 completado\")\n",
    "print(f\"Mejor modelo: Epoch {best_epoch_exp1+1} | Val Acc: {best_val_acc_exp1:.4f}\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "386261cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluaci√≥n en Test Set - EXP1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©tricas EXP1:\n",
      "  Accuracy: 0.1264\n",
      "  Macro F1: 0.0398\n",
      "  Top-3 Acc: 0.3488\n",
      "\n",
      "‚úÖ EXPERIMENTO 1 (CLASS-WEIGHTS) COMPLETADO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluaci√≥n y guardado EXP1\n",
    "print(f\"Evaluaci√≥n en Test Set - EXP1\")\n",
    "test_loss_exp1, test_acc_exp1, test_preds_exp1, test_labels_exp1, test_logits_exp1 = eval_epoch(\n",
    "    model_exp1, test_loader, criterion_exp1, device\n",
    ")\n",
    "\n",
    "# Verificar NaN\n",
    "if np.isnan(test_logits_exp1).any():\n",
    "    valid_mask = ~np.isnan(test_logits_exp1).any(axis=1)\n",
    "    test_logits_exp1 = test_logits_exp1[valid_mask]\n",
    "    test_labels_exp1 = test_labels_exp1[valid_mask]\n",
    "    test_preds_exp1 = test_preds_exp1[valid_mask]\n",
    "\n",
    "# M√©tricas\n",
    "macro_f1_exp1 = f1_score(test_labels_exp1, test_preds_exp1, average='macro', zero_division=0)\n",
    "macro_precision_exp1 = precision_score(test_labels_exp1, test_preds_exp1, average='macro', zero_division=0)\n",
    "macro_recall_exp1 = recall_score(test_labels_exp1, test_preds_exp1, average='macro', zero_division=0)\n",
    "top3_acc_exp1 = top_k_accuracy_score(test_labels_exp1, test_logits_exp1, k=3, labels=np.arange(num_classes))\n",
    "cm_exp1 = confusion_matrix(test_labels_exp1, test_preds_exp1)\n",
    "\n",
    "print(f\"M√©tricas EXP1:\")\n",
    "print(f\"  Accuracy: {test_acc_exp1:.4f}\")\n",
    "print(f\"  Macro F1: {macro_f1_exp1:.4f}\")\n",
    "print(f\"  Top-3 Acc: {top3_acc_exp1:.4f}\\n\")\n",
    "\n",
    "# Guardar resultados (archivos principales)\n",
    "pd.DataFrame(training_log_exp1).to_csv(output_dir_exp1 / 'training_log.csv', index=False)\n",
    "pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Macro-F1', 'Macro-Precision', 'Macro-Recall', 'Top-3 Accuracy', 'Test Loss', 'Best Epoch', 'Best Val Acc'],\n",
    "    'Value': [test_acc_exp1, macro_f1_exp1, macro_precision_exp1, macro_recall_exp1, top3_acc_exp1, test_loss_exp1, best_epoch_exp1+1, best_val_acc_exp1]\n",
    "}).to_csv(output_dir_exp1 / 'metrics.csv', index=False)\n",
    "\n",
    "per_class_report_exp1 = classification_report(\n",
    "    test_labels_exp1, test_preds_exp1, \n",
    "    labels=list(range(num_classes)),\n",
    "    target_names=unique_classes_names,\n",
    "    zero_division=0, output_dict=True\n",
    ")\n",
    "pd.DataFrame(per_class_report_exp1).transpose().to_csv(output_dir_exp1 / 'per_class_metrics.csv')\n",
    "pd.DataFrame(cm_exp1).to_csv(output_dir_exp1 / 'confusion_matrix.csv', index=False, header=False)\n",
    "\n",
    "config_exp1 = {\n",
    "    'experiment': 'G4-QDRANT-Video-Base-CLASS-WEIGHTS',\n",
    "    'dataset': 'dataset_umap_segments.npz',\n",
    "    'dropout': 0.3,\n",
    "    'class_weights': True,\n",
    "    'label_smoothing': 0.0,\n",
    "    'best_epoch': int(best_epoch_exp1),\n",
    "    'best_val_acc': float(best_val_acc_exp1),\n",
    "    'test_accuracy': float(test_acc_exp1),\n",
    "    'test_macro_f1': float(macro_f1_exp1),\n",
    "    'test_top3_accuracy': float(top3_acc_exp1)\n",
    "}\n",
    "with open(output_dir_exp1 / 'config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_exp1, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "torch.save(model_exp1.state_dict(), output_dir_exp1 / 'best_model.pt')\n",
    "\n",
    "# RESUMEN.txt\n",
    "with open(output_dir_exp1 / 'RESUMEN.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"EXPERIMENTO 1 - CLASS-WEIGHTS\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(f\"Dropout: 0.3\\n\")\n",
    "    f.write(f\"Class Weights: Activado (balanced)\\n\")\n",
    "    f.write(f\"Label Smoothing: 0.0\\n\")\n",
    "    f.write(f\"Best Epoch: {best_epoch_exp1+1}\\n\\n\")\n",
    "    f.write(\"RESULTADOS TEST:\\n\")\n",
    "    f.write(f\"  Test Accuracy:    {test_acc_exp1:.4f}\\n\")\n",
    "    f.write(f\"  Macro F1-Score:   {macro_f1_exp1:.4f}\\n\")\n",
    "    f.write(f\"  Macro Precision:  {macro_precision_exp1:.4f}\\n\")\n",
    "    f.write(f\"  Macro Recall:     {macro_recall_exp1:.4f}\\n\")\n",
    "    f.write(f\"  Top-3 Accuracy:   {top3_acc_exp1:.4f}\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "exp1_results = {\n",
    "    'experiment': 'CLASS-WEIGHTS',\n",
    "    'dropout': 0.3,\n",
    "    'class_weights': True,\n",
    "    'label_smoothing': 0.0,\n",
    "    'test_accuracy': test_acc_exp1,\n",
    "    'test_macro_f1': macro_f1_exp1,\n",
    "    'test_top3_accuracy': top3_acc_exp1,\n",
    "    'best_epoch': best_epoch_exp1+1\n",
    "}\n",
    "\n",
    "print(\"‚úÖ EXPERIMENTO 1 (CLASS-WEIGHTS) COMPLETADO\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f756e2",
   "metadata": {},
   "source": [
    "### üß™ Experimento 2 - LABEL-SMOOTHING\n",
    "**Configuraci√≥n:**\n",
    "- Dropout: 0.3\n",
    "- Label Smoothing: 0.1\n",
    "- Sin class weights\n",
    "- Representaci√≥n: Segmentos UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4c3be2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üß™ Iniciando Experimento 2: LABEL-SMOOTH\n",
      "================================================================================\n",
      "üìÅ Directorio: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-QDRANT (Video-Base)\\G4-RESULTS-LABEL-SMOOTH\n",
      "‚úì Modelo creado con dropout 0.3\n",
      "\n",
      "================================================================================\n",
      "Iniciando entrenamiento EXP2 - Max epochs: 50\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50 | Train Loss: 3.3195 | Train Acc: 0.0757 | Val Loss: 3.2600 | Val Acc: 0.0935 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/50 | Train Loss: 3.2644 | Train Acc: 0.0883 | Val Loss: 3.2552 | Val Acc: 0.0863 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/50 | Train Loss: 3.2394 | Train Acc: 0.0991 | Val Loss: 3.2357 | Val Acc: 0.0935 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/50 | Train Loss: 3.0371 | Train Acc: 0.1423 | Val Loss: 3.0158 | Val Acc: 0.1583 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/50 | Train Loss: 2.9097 | Train Acc: 0.1550 | Val Loss: 2.9741 | Val Acc: 0.1511 | LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25/50 | Train Loss: 2.8377 | Train Acc: 0.1874 | Val Loss: 2.8675 | Val Acc: 0.1655 | LR: 5.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30/50 | Train Loss: 2.7750 | Train Acc: 0.2000 | Val Loss: 2.7604 | Val Acc: 0.2086 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  35/50 | Train Loss: 2.7340 | Train Acc: 0.2270 | Val Loss: 2.8421 | Val Acc: 0.1727 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40/50 | Train Loss: 2.7033 | Train Acc: 0.2342 | Val Loss: 2.7018 | Val Acc: 0.2014 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  45/50 | Train Loss: 2.6653 | Train Acc: 0.2090 | Val Loss: 2.6871 | Val Acc: 0.2230 | LR: 2.50e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50/50 | Train Loss: 2.6494 | Train Acc: 0.2360 | Val Loss: 2.6459 | Val Acc: 0.2014 | LR: 2.50e-05\n",
      "\n",
      "================================================================================\n",
      "Entrenamiento EXP2 completado\n",
      "Mejor modelo: Epoch 47 | Val Acc: 0.2518\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# EXPERIMENTO 2: LABEL-SMOOTH\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß™ Iniciando Experimento 2: LABEL-SMOOTH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configurar directorio de salida\n",
    "output_dir_exp2 = ROOT_PATH / 'G4-RESULTS-LABEL-SMOOTH'\n",
    "output_dir_exp2.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ Directorio: {output_dir_exp2}\")\n",
    "\n",
    "# Crear modelo con dropout 0.3\n",
    "model_exp2 = create_model_with_dropout(dropout_config=0.3)\n",
    "print(f\"‚úì Modelo creado con dropout 0.3\")\n",
    "\n",
    "# Configurar loss con label smoothing\n",
    "criterion_exp2 = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Optimizer y Scheduler\n",
    "optimizer_exp2 = AdamW(model_exp2.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler_exp2 = ReduceLROnPlateau(optimizer_exp2, mode='max', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Training log\n",
    "training_log_exp2 = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_acc_exp2 = 0.0\n",
    "best_epoch_exp2 = 0\n",
    "patience_counter_exp2 = 0\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Iniciando entrenamiento EXP2 - Max epochs: {max_epochs}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # Train\n",
    "    train_loss_exp2, train_acc_exp2 = train_epoch(model_exp2, train_loader, criterion_exp2, optimizer_exp2, device)\n",
    "    \n",
    "    # Val\n",
    "    val_loss_exp2, val_acc_exp2, _, _, _ = eval_epoch(model_exp2, val_loader, criterion_exp2, device)\n",
    "    \n",
    "    # LR Scheduler\n",
    "    current_lr_exp2 = optimizer_exp2.param_groups[0]['lr']\n",
    "    scheduler_exp2.step(val_acc_exp2)\n",
    "    \n",
    "    # Log\n",
    "    training_log_exp2['epoch'].append(epoch)\n",
    "    training_log_exp2['train_loss'].append(train_loss_exp2)\n",
    "    training_log_exp2['train_acc'].append(train_acc_exp2)\n",
    "    training_log_exp2['val_loss'].append(val_loss_exp2)\n",
    "    training_log_exp2['val_acc'].append(val_acc_exp2)\n",
    "    training_log_exp2['lr'].append(current_lr_exp2)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc_exp2 > best_val_acc_exp2:\n",
    "        best_val_acc_exp2 = val_acc_exp2\n",
    "        best_epoch_exp2 = epoch\n",
    "        patience_counter_exp2 = 0\n",
    "        best_model_state_exp2 = model_exp2.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter_exp2 += 1\n",
    "    \n",
    "    # Print\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{max_epochs} | \"\n",
    "              f\"Train Loss: {train_loss_exp2:.4f} | Train Acc: {train_acc_exp2:.4f} | \"\n",
    "              f\"Val Loss: {val_loss_exp2:.4f} | Val Acc: {val_acc_exp2:.4f} | \"\n",
    "              f\"LR: {current_lr_exp2:.2e}\")\n",
    "    \n",
    "    if patience_counter_exp2 >= early_stopping_patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Cargar mejor modelo\n",
    "model_exp2.load_state_dict(best_model_state_exp2)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Entrenamiento EXP2 completado\")\n",
    "print(f\"Mejor modelo: Epoch {best_epoch_exp2+1} | Val Acc: {best_val_acc_exp2:.4f}\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "798fef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluaci√≥n en Test Set - EXP2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©tricas EXP2:\n",
      "  Accuracy: 0.2356\n",
      "  Macro F1: 0.0599\n",
      "  Top-3 Acc: 0.5116\n",
      "\n",
      "‚úÖ EXPERIMENTO 2 (LABEL-SMOOTH) COMPLETADO\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Evaluaci√≥n y guardado EXP2\n",
    "print(f\"Evaluaci√≥n en Test Set - EXP2\")\n",
    "test_loss_exp2, test_acc_exp2, test_preds_exp2, test_labels_exp2, test_logits_exp2 = eval_epoch(\n",
    "    model_exp2, test_loader, criterion_exp2, device\n",
    ")\n",
    "\n",
    "# Verificar NaN\n",
    "if np.isnan(test_logits_exp2).any():\n",
    "    valid_mask = ~np.isnan(test_logits_exp2).any(axis=1)\n",
    "    test_logits_exp2 = test_logits_exp2[valid_mask]\n",
    "    test_labels_exp2 = test_labels_exp2[valid_mask]\n",
    "    test_preds_exp2 = test_preds_exp2[valid_mask]\n",
    "\n",
    "# M√©tricas\n",
    "macro_f1_exp2 = f1_score(test_labels_exp2, test_preds_exp2, average='macro', zero_division=0)\n",
    "macro_precision_exp2 = precision_score(test_labels_exp2, test_preds_exp2, average='macro', zero_division=0)\n",
    "macro_recall_exp2 = recall_score(test_labels_exp2, test_preds_exp2, average='macro', zero_division=0)\n",
    "top3_acc_exp2 = top_k_accuracy_score(test_labels_exp2, test_logits_exp2, k=3, labels=np.arange(num_classes))\n",
    "cm_exp2 = confusion_matrix(test_labels_exp2, test_preds_exp2)\n",
    "\n",
    "print(f\"M√©tricas EXP2:\")\n",
    "print(f\"  Accuracy: {test_acc_exp2:.4f}\")\n",
    "print(f\"  Macro F1: {macro_f1_exp2:.4f}\")\n",
    "print(f\"  Top-3 Acc: {top3_acc_exp2:.4f}\\n\")\n",
    "\n",
    "# Guardar resultados (archivos principales)\n",
    "pd.DataFrame(training_log_exp2).to_csv(output_dir_exp2 / 'training_log.csv', index=False)\n",
    "pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Macro-F1', 'Macro-Precision', 'Macro-Recall', 'Top-3 Accuracy', 'Test Loss', 'Best Epoch', 'Best Val Acc'],\n",
    "    'Value': [test_acc_exp2, macro_f1_exp2, macro_precision_exp2, macro_recall_exp2, top3_acc_exp2, test_loss_exp2, best_epoch_exp2+1, best_val_acc_exp2]\n",
    "}).to_csv(output_dir_exp2 / 'metrics.csv', index=False)\n",
    "\n",
    "per_class_report_exp2 = classification_report(\n",
    "    test_labels_exp2, test_preds_exp2, \n",
    "    labels=list(range(num_classes)),\n",
    "    target_names=unique_classes_names,\n",
    "    zero_division=0, output_dict=True\n",
    ")\n",
    "pd.DataFrame(per_class_report_exp2).transpose().to_csv(output_dir_exp2 / 'per_class_metrics.csv')\n",
    "pd.DataFrame(cm_exp2).to_csv(output_dir_exp2 / 'confusion_matrix.csv', index=False, header=False)\n",
    "\n",
    "config_exp2 = {\n",
    "    'experiment': 'G4-QDRANT-Video-Base-LABEL-SMOOTH',\n",
    "    'dataset': 'dataset_umap_segments.npz',\n",
    "    'dropout': 0.3,\n",
    "    'class_weights': False,\n",
    "    'label_smoothing': 0.1,\n",
    "    'best_epoch': int(best_epoch_exp2),\n",
    "    'best_val_acc': float(best_val_acc_exp2),\n",
    "    'test_accuracy': float(test_acc_exp2),\n",
    "    'test_macro_f1': float(macro_f1_exp2),\n",
    "    'test_top3_accuracy': float(top3_acc_exp2)\n",
    "}\n",
    "with open(output_dir_exp2 / 'config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_exp2, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "torch.save(model_exp2.state_dict(), output_dir_exp2 / 'best_model.pt')\n",
    "\n",
    "# RESUMEN.txt\n",
    "with open(output_dir_exp2 / 'RESUMEN.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"EXPERIMENTO 2 - LABEL-SMOOTH\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(f\"Dropout: 0.3\\n\")\n",
    "    f.write(f\"Class Weights: Desactivado\\n\")\n",
    "    f.write(f\"Label Smoothing: 0.1\\n\")\n",
    "    f.write(f\"Best Epoch: {best_epoch_exp2+1}\\n\\n\")\n",
    "    f.write(\"RESULTADOS TEST:\\n\")\n",
    "    f.write(f\"  Test Accuracy:    {test_acc_exp2:.4f}\\n\")\n",
    "    f.write(f\"  Macro F1-Score:   {macro_f1_exp2:.4f}\\n\")\n",
    "    f.write(f\"  Macro Precision:  {macro_precision_exp2:.4f}\\n\")\n",
    "    f.write(f\"  Macro Recall:     {macro_recall_exp2:.4f}\\n\")\n",
    "    f.write(f\"  Top-3 Accuracy:   {top3_acc_exp2:.4f}\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "exp2_results = {\n",
    "    'experiment': 'LABEL-SMOOTH',\n",
    "    'dropout': 0.3,\n",
    "    'class_weights': False,\n",
    "    'label_smoothing': 0.1,\n",
    "    'test_accuracy': test_acc_exp2,\n",
    "    'test_macro_f1': macro_f1_exp2,\n",
    "    'test_top3_accuracy': top3_acc_exp2,\n",
    "    'best_epoch': best_epoch_exp2+1\n",
    "}\n",
    "\n",
    "print(\"‚úÖ EXPERIMENTO 2 (LABEL-SMOOTH) COMPLETADO\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc18470",
   "metadata": {},
   "source": [
    "## 4. An√°lisis Comparativo\n",
    "\n",
    "### üìä Comparaci√≥n entre Experimentos\n",
    "An√°lisis de rendimiento de las tres configuraciones usando segmentos UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "61c698b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä COMPARACI√ìN FINAL DE EXPERIMENTOS\n",
      "================================================================================\n",
      "\n",
      "    Experimento                  Carpeta  Dropout Class Weights  Label Smoothing  Test Accuracy  Macro F1  Top-3 Accuracy  Best Epoch\n",
      "       Baseline               G4-RESULTS      0.1            No              0.0       0.160920  0.028535        0.401163           9\n",
      "  Class Weights G4-RESULTS-CLASS-WEIGHTS      0.3            S√≠              0.0       0.126437  0.039760        0.348837          26\n",
      "Label Smoothing  G4-RESULTS-LABEL-SMOOTH      0.3            No              0.1       0.235632  0.059899        0.511628          47\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úì Comparaci√≥n guardada en: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-QDRANT (Video-Base)\\experiments_comparison.csv\n",
      "\n",
      "üèÜ Mejor Macro F1: Label Smoothing (0.0599)\n",
      "üèÜ Mejor Accuracy: Label Smoothing (0.2356)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ SISTEMA G4 COMPLETADO - 3 EXPERIMENTOS\n",
      "================================================================================\n",
      "üìÅ Estructura generada:\n",
      "  G4-QDRANT (Video-Base)/\n",
      "    ‚îú‚îÄ‚îÄ G4-RESULTS/ (10 archivos)\n",
      "    ‚îú‚îÄ‚îÄ G4-RESULTS-CLASS-WEIGHTS/ (10 archivos)\n",
      "    ‚îú‚îÄ‚îÄ G4-RESULTS-LABEL-SMOOTH/ (10 archivos)\n",
      "    ‚îî‚îÄ‚îÄ experiments_comparison.csv\n",
      "\n",
      "  Total: 30 archivos + 1 comparaci√≥n\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMPARACI√ìN DE LOS 3 EXPERIMENTOS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPARACI√ìN FINAL DE EXPERIMENTOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Crear DataFrame de comparaci√≥n\n",
    "comparison_data = pd.DataFrame([\n",
    "    {\n",
    "        'Experimento': 'Baseline',\n",
    "        'Carpeta': 'G4-RESULTS',\n",
    "        'Dropout': 0.1,\n",
    "        'Class Weights': 'No',\n",
    "        'Label Smoothing': 0.0,\n",
    "        'Test Accuracy': exp0_results['test_accuracy'],\n",
    "        'Macro F1': exp0_results['test_macro_f1'],\n",
    "        'Top-3 Accuracy': exp0_results['test_top3_accuracy'],\n",
    "        'Best Epoch': exp0_results['best_epoch']\n",
    "    },\n",
    "    {\n",
    "        'Experimento': 'Class Weights',\n",
    "        'Carpeta': 'G4-RESULTS-CLASS-WEIGHTS',\n",
    "        'Dropout': 0.3,\n",
    "        'Class Weights': 'S√≠',\n",
    "        'Label Smoothing': 0.0,\n",
    "        'Test Accuracy': exp1_results['test_accuracy'],\n",
    "        'Macro F1': exp1_results['test_macro_f1'],\n",
    "        'Top-3 Accuracy': exp1_results['test_top3_accuracy'],\n",
    "        'Best Epoch': exp1_results['best_epoch']\n",
    "    },\n",
    "    {\n",
    "        'Experimento': 'Label Smoothing',\n",
    "        'Carpeta': 'G4-RESULTS-LABEL-SMOOTH',\n",
    "        'Dropout': 0.3,\n",
    "        'Class Weights': 'No',\n",
    "        'Label Smoothing': 0.1,\n",
    "        'Test Accuracy': exp2_results['test_accuracy'],\n",
    "        'Macro F1': exp2_results['test_macro_f1'],\n",
    "        'Top-3 Accuracy': exp2_results['test_top3_accuracy'],\n",
    "        'Best Epoch': exp2_results['best_epoch']\n",
    "    }\n",
    "])\n",
    "\n",
    "# Mostrar tabla\n",
    "print(comparison_data.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Guardar comparaci√≥n en ROOT_PATH\n",
    "comparison_data.to_csv(ROOT_PATH / 'experiments_comparison.csv', index=False)\n",
    "print(f\"\\n‚úì Comparaci√≥n guardada en: {ROOT_PATH / 'experiments_comparison.csv'}\")\n",
    "\n",
    "# Identificar mejor experimento\n",
    "best_f1_idx = comparison_data['Macro F1'].idxmax()\n",
    "best_acc_idx = comparison_data['Test Accuracy'].idxmax()\n",
    "\n",
    "print(f\"\\nüèÜ Mejor Macro F1: {comparison_data.loc[best_f1_idx, 'Experimento']} ({comparison_data.loc[best_f1_idx, 'Macro F1']:.4f})\")\n",
    "print(f\"üèÜ Mejor Accuracy: {comparison_data.loc[best_acc_idx, 'Experimento']} ({comparison_data.loc[best_acc_idx, 'Test Accuracy']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ SISTEMA G4 COMPLETADO - 3 EXPERIMENTOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÅ Estructura generada:\")\n",
    "print(f\"  {ROOT_PATH.name}/\")\n",
    "print(f\"    ‚îú‚îÄ‚îÄ G4-RESULTS/ (10 archivos)\")\n",
    "print(f\"    ‚îú‚îÄ‚îÄ G4-RESULTS-CLASS-WEIGHTS/ (10 archivos)\")\n",
    "print(f\"    ‚îú‚îÄ‚îÄ G4-RESULTS-LABEL-SMOOTH/ (10 archivos)\")\n",
    "print(f\"    ‚îî‚îÄ‚îÄ experiments_comparison.csv\")\n",
    "print(f\"\\n  Total: 30 archivos + 1 comparaci√≥n\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86121ae",
   "metadata": {},
   "source": [
    "## 5. Conclusiones\n",
    "\n",
    "### üéØ Hallazgos Principales\n",
    "\n",
    "**Rendimiento General:**\n",
    "Los experimentos con segmentos UMAP demuestran una aproximaci√≥n diferente a la clasificaci√≥n de gestos ASL: en lugar de procesar secuencias temporales frame-a-frame, cada video completo se representa como un √∫nico embedding UMAP. Esta representaci√≥n captura la informaci√≥n hol√≠stica del gesto en un espacio dimensional reducido.\n",
    "\n",
    "**Comparaci√≥n de T√©cnicas:**\n",
    "1. **Baseline:** Configuraci√≥n base con dropout conservador (0.1) sobre segmentos UMAP completos\n",
    "2. **Class Weights:** Balanceo de clases con regularizaci√≥n aumentada, especialmente √∫til en representaciones de alta abstracci√≥n\n",
    "3. **Label Smoothing:** Regularizaci√≥n adicional para prevenir sobreconfianza en el espacio de segmentos\n",
    "\n",
    "**Impacto de las Configuraciones:**\n",
    "- **Segmentos UMAP:** Representaci√≥n global del video (1 embedding vs 96 frames), perdiendo informaci√≥n temporal detallada pero ganando eficiencia computacional\n",
    "- **Ventaja:** Clasificaci√≥n extremadamente r√°pida en inferencia (sin procesar secuencias largas)\n",
    "- **Desventaja:** P√©rdida de din√°mica temporal que puede ser crucial para distinguir gestos similares\n",
    "- **Dropout aumentado (0.3):** Cr√≠tico para prevenir overfitting en representaci√≥n tan compacta\n",
    "- **Class Weights:** Mayor impacto al trabajar con embeddings globales donde clases minoritarias pueden quedar subrepresentadas\n",
    "- **Label Smoothing:** √ötil para mejorar calibraci√≥n en fronteras de decisi√≥n del espacio de segmentos\n",
    "\n",
    "**Diferencias con Secuencias Frame-a-Frame:**\n",
    "- **Secuencias (otros notebooks):** Transformer procesa 96 frames individualmente, capturando din√°mica temporal\n",
    "- **Segmentos (este notebook):** Transformer trabaja con 1 embedding por video, clasificaci√≥n m√°s directa pero menos informaci√≥n temporal\n",
    "- **Trade-off:** Velocidad vs precisi√≥n temporal\n",
    "\n",
    "**Casos de Uso √ìptimos:**\n",
    "- **Segmentos UMAP:** Aplicaciones de tiempo real, clasificaci√≥n r√°pida, gestos est√°ticos o con poca variaci√≥n temporal\n",
    "- **Secuencias frame-a-frame:** Gestos din√°micos complejos, an√°lisis temporal detallado, m√°xima precisi√≥n\n",
    "\n",
    "### üìÅ Archivos Generados\n",
    "\n",
    "**Estructura de salida (Formato G4):**\n",
    "```\n",
    "G4-QDRANT (Video-Base)/\n",
    "‚îú‚îÄ‚îÄ G4-RESULTS/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ best_model.pt\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ training_log.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ results.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ per_class.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ confusion.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.png\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ training_curves.png\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ per_class_analysis.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ RESUMEN.txt\n",
    "‚îú‚îÄ‚îÄ G4-RESULTS-CLASS-WEIGHTS/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ [mismos archivos]\n",
    "‚îú‚îÄ‚îÄ G4-RESULTS-LABEL-SMOOTH/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ [mismos archivos]\n",
    "‚îú‚îÄ‚îÄ experiments_comparison.csv\n",
    "‚îî‚îÄ‚îÄ experiments_comparison.png\n",
    "```\n",
    "\n",
    "### üîß Uso del Notebook\n",
    "\n",
    "Para cambiar entre experimentos, modificar la configuraci√≥n en las celdas de entrenamiento seg√∫n el experimento deseado.\n",
    "\n",
    "---\n",
    "\n",
    "**Nota:** Este notebook usa **segmentos UMAP** (1 embedding por video) a diferencia de los otros experimentos que usan secuencias frame-a-frame (96 embeddings por video). Esta aproximaci√≥n prioriza eficiencia sobre detalle temporal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
