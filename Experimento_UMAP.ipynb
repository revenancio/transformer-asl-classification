{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77770e61",
   "metadata": {},
   "source": [
    "# Transformer Encoder-Only - UMAP Embeddings (Sistema G4)\n",
    "\n",
    "**Objetivo:** Clasificaci√≥n de secuencias temporales con Transformer encoder-only usando embeddings UMAP\n",
    "\n",
    "**Dataset:** 868 videos, 96 frames, 128 features/frame (UMAP embeddings)  \n",
    "**Clases:** 30 (ASL - American Sign Language)  \n",
    "**Hardware:** RTX 5050 Laptop (8GB) / GTX 1660 Super (6GB)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Sistema de Gesti√≥n de Experimentos G4\n",
    "\n",
    "Este notebook genera autom√°ticamente **32 archivos** organizados en:\n",
    "- `G4-RESULTS/` - Baseline (dropout 0.1)\n",
    "- `G4-RESULTS-CLASS-WEIGHTS/` - Class Weights + Dropout 0.3\n",
    "- `G4-RESULTS-LABEL-SMOOTH/` - Label Smoothing 0.1 + Dropout 0.3\n",
    "- 2 archivos de comparaci√≥n en ROOT_PATH\n",
    "\n",
    "Cada experimento genera 10 archivos: best_model.pt, config.json, training_log.csv, metrics.csv, per_class_metrics.csv, confusion_matrix.csv, confusion_matrix.png, training_curves.png, per_class_analysis.png, RESUMEN.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e62ff0",
   "metadata": {},
   "source": [
    "## üìÅ Sistema de Detecci√≥n Autom√°tica de Rutas (G4)\n",
    "\n",
    "El notebook detecta autom√°ticamente en qu√© carpeta se encuentra y configura las rutas:\n",
    "\n",
    "- Si detecta `UMAP` en la ruta ‚Üí Usa carpeta `G4-EMBEDDING FRAME A FRAME UMAP/`\n",
    "- Si detecta `GCN` o `Embeddings` ‚Üí Usa carpeta `G4-EMBEDDING FRAME A FRAME GCN/`\n",
    "- Si detecta `JSON` o `NORM` ‚Üí Usa carpeta `G4-JSON-NORM/`\n",
    "\n",
    "Los archivos se guardan dentro de subcarpetas seg√∫n el experimento:\n",
    "- `G4-RESULTS` (baseline)\n",
    "- `G4-RESULTS-CLASS-WEIGHTS` (experimento 1)\n",
    "- `G4-RESULTS-LABEL-SMOOTH` (experimento 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59df925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "VRAM: 8.55 GB\n",
      "\n",
      "================================================================================\n",
      "üéØ SISTEMA DE DETECCI√ìN AUTOM√ÅTICA DE RUTAS G4\n",
      "================================================================================\n",
      "üìÇ Notebook detectado: transformer-asl-classification\n",
      "üîç Modo detectado: UMAP\n",
      "üìÅ ROOT_PATH: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\n",
      "‚úÖ Detecci√≥n autom√°tica: ACTIVADA\n",
      "================================================================================\n",
      "\n",
      "‚öôÔ∏è  CONFIGURACI√ìN ACTUAL:\n",
      "  ‚Ä¢ Tipo de experimento: baseline\n",
      "  ‚Ä¢ Descripci√≥n: Baseline - Dropout 0.1\n",
      "  ‚Ä¢ Directorio de salida: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\\G4-RESULTS\n",
      "  ‚Ä¢ Dropout: 0.1\n",
      "  ‚Ä¢ Class Weights: False\n",
      "  ‚Ä¢ Label Smoothing: 0.0\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, top_k_accuracy_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# üîß CONFIGURACI√ìN AUTOM√ÅTICA DE RUTAS Y EXPERIMENTOS (G4)\n",
    "BASE_PATHS = {\n",
    "    'umap': Path(r'C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP'),\n",
    "    'gcn': Path(r'C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME GCN'),\n",
    "    'json': Path(r'C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-JSON-NORM')\n",
    "}\n",
    "\n",
    "# Detectar autom√°ticamente el tipo de experimento\n",
    "current_notebook = Path.cwd()\n",
    "notebook_name_lower = str(current_notebook).lower()\n",
    "\n",
    "if 'umap' in notebook_name_lower:\n",
    "    DETECTED_MODE = 'umap'\n",
    "elif 'embedding' in notebook_name_lower or 'gcn' in notebook_name_lower:\n",
    "    DETECTED_MODE = 'gcn'\n",
    "elif 'json' in notebook_name_lower or 'norm' in notebook_name_lower:\n",
    "    DETECTED_MODE = 'json'\n",
    "else:\n",
    "    DETECTED_MODE = 'umap'  # Default para este notebook\n",
    "\n",
    "ROOT_PATH = BASE_PATHS[DETECTED_MODE]\n",
    "AUTO_DETECTED = True\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üéØ SISTEMA DE DETECCI√ìN AUTOM√ÅTICA DE RUTAS G4\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"üìÇ Notebook detectado: {current_notebook.name}\")\n",
    "print(f\"üîç Modo detectado: {DETECTED_MODE.upper()}\")\n",
    "print(f\"üìÅ ROOT_PATH: {ROOT_PATH}\")\n",
    "print(f\"‚úÖ Detecci√≥n autom√°tica: {'ACTIVADA' if AUTO_DETECTED else 'DESACTIVADA'}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Configuraciones de experimentos (G4)\n",
    "EXPERIMENT_CONFIGS = {\n",
    "    'baseline': {\n",
    "        'folder_name': 'G4-RESULTS',\n",
    "        'description': 'Baseline - Dropout 0.1',\n",
    "        'dropout': 0.1,\n",
    "        'use_class_weights': False,\n",
    "        'label_smoothing': 0.0\n",
    "    },\n",
    "    'class_weights': {\n",
    "        'folder_name': 'G4-RESULTS-CLASS-WEIGHTS',\n",
    "        'description': 'Class Weights + Dropout 0.3',\n",
    "        'dropout': 0.3,\n",
    "        'use_class_weights': True,\n",
    "        'label_smoothing': 0.0\n",
    "    },\n",
    "    'label_smoothing': {\n",
    "        'folder_name': 'G4-RESULTS-LABEL-SMOOTH',\n",
    "        'description': 'Label Smoothing 0.1 + Dropout 0.3',\n",
    "        'dropout': 0.3,\n",
    "        'use_class_weights': False,\n",
    "        'label_smoothing': 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Seleccionar experimento (MODIFICAR AQU√ç PARA CAMBIAR EXPERIMENTO)\n",
    "EXPERIMENT_TYPE = 'baseline'  # Opciones: 'baseline', 'class_weights', 'label_smoothing'\n",
    "\n",
    "current_config = EXPERIMENT_CONFIGS[EXPERIMENT_TYPE]\n",
    "output_dir = ROOT_PATH / current_config['folder_name']\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚öôÔ∏è  CONFIGURACI√ìN ACTUAL:\")\n",
    "print(f\"  ‚Ä¢ Tipo de experimento: {EXPERIMENT_TYPE}\")\n",
    "print(f\"  ‚Ä¢ Descripci√≥n: {current_config['description']}\")\n",
    "print(f\"  ‚Ä¢ Directorio de salida: {output_dir}\")\n",
    "print(f\"  ‚Ä¢ Dropout: {current_config['dropout']}\")\n",
    "print(f\"  ‚Ä¢ Class Weights: {current_config['use_class_weights']}\")\n",
    "print(f\"  ‚Ä¢ Label Smoothing: {current_config['label_smoothing']}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75322abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üßπ LIMPIEZA DE CARPETAS PREVIAS\n",
      "================================================================================\n",
      "  ‚úì Eliminado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\\G4-RESULTS\n",
      "  ‚úì Eliminado (ra√≠z): c:\\Users\\isaiy\\Documents\\modelo prueba embedings\\transformer-asl-classification\\g5.0_umap\n",
      "‚úÖ Limpieza completada (2 carpetas eliminadas)\n",
      "üìÅ Directorio de salida recreado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\\G4-RESULTS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üßπ LIMPIEZA DE CARPETAS PREVIAS\n",
    "# Esta celda elimina carpetas de experimentos previos antes de ejecutar los 3 experimentos\n",
    "\n",
    "import shutil\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üßπ LIMPIEZA DE CARPETAS PREVIAS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Carpetas de resultados G4 a limpiar\n",
    "results_folders = [\n",
    "    'G4-RESULTS',\n",
    "    'G4-RESULTS-CLASS-WEIGHTS',\n",
    "    'G4-RESULTS-LABEL-SMOOTH'\n",
    "]\n",
    "\n",
    "# Limpiar dentro de ROOT_PATH\n",
    "cleaned_count = 0\n",
    "for folder in results_folders:\n",
    "    folder_path = ROOT_PATH / folder\n",
    "    if folder_path.exists():\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"  ‚úì Eliminado: {folder_path}\")\n",
    "        cleaned_count += 1\n",
    "\n",
    "# Tambi√©n limpiar carpetas antiguas en el directorio ra√≠z del proyecto\n",
    "project_root = Path.cwd()\n",
    "old_folders = [\n",
    "    'g5.0_umap',\n",
    "    'g6_class_weights',\n",
    "    'g7_label_smooth',\n",
    "    'umap_baseline',\n",
    "    'results_umap',\n",
    "    'output_videos',\n",
    "    'temp_results',\n",
    "    'old_results',\n",
    "    'results'\n",
    "]\n",
    "\n",
    "for folder in old_folders:\n",
    "    folder_path = project_root / folder\n",
    "    if folder_path.exists():\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"  ‚úì Eliminado (ra√≠z): {folder_path}\")\n",
    "        cleaned_count += 1\n",
    "\n",
    "if cleaned_count == 0:\n",
    "    print(\"  ‚ÑπÔ∏è  No hay carpetas previas para limpiar\")\n",
    "\n",
    "print(f\"‚úÖ Limpieza completada ({cleaned_count} carpetas eliminadas)\")\n",
    "\n",
    "# Recrear el directorio de salida actual despu√©s de la limpieza\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ Directorio de salida recreado: {output_dir}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "693fec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claves disponibles en el archivo .npz:\n",
      "['X', 'y', 'filenames', 'class_names', 'masks']\n",
      "\n",
      "X:\n",
      "  Tipo: <class 'numpy.ndarray'>\n",
      "  Forma: (864, 96, 128)\n",
      "\n",
      "y:\n",
      "  Tipo: <class 'numpy.ndarray'>\n",
      "  Forma: (864,)\n",
      "\n",
      "filenames:\n",
      "  Tipo: <class 'numpy.ndarray'>\n",
      "  Forma: (864,)\n",
      "\n",
      "class_names:\n",
      "  Tipo: <class 'numpy.ndarray'>\n",
      "  Forma: (30,)\n",
      "\n",
      "masks:\n",
      "  Tipo: <class 'numpy.ndarray'>\n",
      "  Forma: (864, 96)\n"
     ]
    }
   ],
   "source": [
    "# 1. CARGAR DATASET UMAP\n",
    "dataset_path = Path('./daataset/G2_GCN_embeddings_separado_128d.npz')\n",
    "data = np.load(dataset_path, allow_pickle=True)\n",
    "\n",
    "# Inspeccionar las claves disponibles\n",
    "print(\"Claves disponibles en el archivo .npz:\")\n",
    "print(data.files)\n",
    "\n",
    "# Mostrar tambi√©n el tipo y forma de cada array\n",
    "for key in data.files:\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"  Tipo: {type(data[key])}\")\n",
    "    if hasattr(data[key], 'shape'):\n",
    "        print(f\"  Forma: {data[key].shape}\")\n",
    "\n",
    "# Una vez que veas las claves correctas, reemplaza 'X' con el nombre correcto\n",
    "# X = data['nombre_correcto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "522a4600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ VALIDACI√ìN - ESTRUCTURA DE G2_GCN_embeddings_separado_128d.npz\n",
      "================================================================================\n",
      "\n",
      "üìã CLAVES DISPONIBLES EN EL ARCHIVO:\n",
      "   ['X', 'y', 'filenames', 'class_names', 'masks']\n",
      "\n",
      "üìä INFORMACI√ìN DETALLADA DE CADA ARRAY:\n",
      "\n",
      "‚úì V√ÅLIDO - X:\n",
      "   Tipo: ndarray\n",
      "   dtype: float32\n",
      "   Forma actual: (864, 96, 128)\n",
      "   Forma esperada: (864, 96, 128)\n",
      "   Rango de valores: [-30.6874, 26.5063]\n",
      "\n",
      "‚úì V√ÅLIDO - y:\n",
      "   Tipo: ndarray\n",
      "   dtype: int64\n",
      "   Forma actual: (864,)\n",
      "   Forma esperada: (864,)\n",
      "   Valores √∫nicos: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "   Distribuci√≥n de clases: {0: 76, 1: 10, 2: 22, 3: 22, 4: 18, 5: 18, 6: 28, 7: 18, 8: 22, 9: 16, 10: 12, 11: 18, 12: 28, 13: 24, 14: 28, 15: 18, 16: 18, 17: 82, 18: 76, 19: 18, 20: 28, 21: 18, 22: 18, 23: 22, 24: 18, 25: 68, 26: 22, 27: 64, 28: 18, 29: 16}\n",
      "\n",
      "‚úì V√ÅLIDO - filenames:\n",
      "   Tipo: ndarray\n",
      "   dtype: <U42\n",
      "   Forma actual: (864,)\n",
      "   Forma esperada: (864,)\n",
      "   Primeros 3 nombres: ['Adi√≥s\\\\Adi√≥s_1.json', 'Adi√≥s\\\\Adi√≥s_1.json_mirror', 'Adi√≥s\\\\Adi√≥s_10.json']\n",
      "\n",
      "‚úì V√ÅLIDO - class_names:\n",
      "   Tipo: ndarray\n",
      "   dtype: <U13\n",
      "   Forma actual: (30,)\n",
      "   Forma esperada: (30,)\n",
      "   Nombres de clases: ['Adi√≥s', 'Buenas noches', 'Buenas tardes', 'Buenos d√≠as', 'Clase', 'Comenzar', 'Compa√±ero', 'Cuaderno', 'C√≥mo est√°', 'Deberes', 'Disculpa', 'Entender', 'Escribir', 'Escuchar', 'Estudiante', 'Examen', 'Explicar', 'Gracias', 'Hola', 'Lecci√≥n', 'Leer', 'Libro', 'L√°piz', 'Mucho gusto', 'Pizarr√≥n', 'Por favor', 'Pregunta', 'Profesor', 'Responder', 'Terminar']\n",
      "\n",
      "‚úì V√ÅLIDO - masks:\n",
      "   Tipo: ndarray\n",
      "   dtype: float32\n",
      "   Forma actual: (864, 96)\n",
      "   Forma esperada: (864, 96)\n",
      "   Valores √∫nicos en masks: [1.]\n",
      "   Cantidad de 1.0: 82944, Cantidad de 0.0: 0\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üéØ RESUMEN DE VALIDACI√ìN:\n",
      "================================================================================\n",
      "\n",
      "Claves v√°lidas: 5/5\n",
      "  ‚úì X: (864, 96, 128)\n",
      "  ‚úì y: (864,)\n",
      "  ‚úì filenames: (864,)\n",
      "  ‚úì class_names: (30,)\n",
      "  ‚úì masks: (864, 96)\n",
      "\n",
      "‚úÖ ESTRUCTURA V√ÅLIDA - El archivo contiene todas las claves con las formas correctas\n",
      "\n",
      "================================================================================\n",
      "üì¶ CARGANDO DATOS:\n",
      "================================================================================\n",
      "\n",
      "‚úì X (embeddings): (864, 96, 128) | dtype: float32\n",
      "‚úì y (etiquetas): (864,) | unique classes: 30\n",
      "‚úì filenames: (864,) | samples: 864\n",
      "‚úì class_names: (30,) | total clases: 30\n",
      "‚úì masks: (864, 96) | dtype: float32\n",
      "\n",
      "‚úÖ CARGA COMPLETADA EXITOSAMENTE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ VALIDACI√ìN - CARGAR ARCHIVO GCN CON ESTRUCTURA CORRECTA\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ VALIDACI√ìN - ESTRUCTURA DE G2_GCN_embeddings_separado_128d.npz\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "dataset_path_gcn = Path('./daataset/G2_GCN_embeddings_separado_128d.npz')\n",
    "data_gcn = np.load(dataset_path_gcn, allow_pickle=True)\n",
    "\n",
    "# üìã Claves disponibles\n",
    "print(\"üìã CLAVES DISPONIBLES EN EL ARCHIVO:\")\n",
    "print(f\"   {list(data_gcn.files)}\\n\")\n",
    "\n",
    "# üìä INFORMACI√ìN DETALLADA DE CADA ARRAY\n",
    "print(\"üìä INFORMACI√ìN DETALLADA DE CADA ARRAY:\\n\")\n",
    "\n",
    "expected_keys = {\n",
    "    'X': (864, 96, 128),\n",
    "    'y': (864,),\n",
    "    'filenames': (864,),\n",
    "    'class_names': (30,),\n",
    "    'masks': (864, 96)\n",
    "}\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "for key in expected_keys.keys():\n",
    "    if key in data_gcn.files:\n",
    "        array = data_gcn[key]\n",
    "        actual_shape = array.shape\n",
    "        expected_shape = expected_keys[key]\n",
    "        \n",
    "        is_valid = actual_shape == expected_shape\n",
    "        status = \"‚úì V√ÅLIDO\" if is_valid else \"‚úó ERROR\"\n",
    "        \n",
    "        validation_results[key] = {\n",
    "            'exists': True,\n",
    "            'valid': is_valid,\n",
    "            'dtype': array.dtype,\n",
    "            'shape': actual_shape,\n",
    "            'expected_shape': expected_shape\n",
    "        }\n",
    "        \n",
    "        print(f\"{status} - {key}:\")\n",
    "        print(f\"   Tipo: {type(array).__name__}\")\n",
    "        print(f\"   dtype: {array.dtype}\")\n",
    "        print(f\"   Forma actual: {actual_shape}\")\n",
    "        print(f\"   Forma esperada: {expected_shape}\")\n",
    "        \n",
    "        if key == 'X':\n",
    "            print(f\"   Rango de valores: [{array.min():.4f}, {array.max():.4f}]\")\n",
    "        elif key == 'y':\n",
    "            print(f\"   Valores √∫nicos: {np.unique(array).tolist()}\")\n",
    "            print(f\"   Distribuci√≥n de clases: {dict(zip(*np.unique(array, return_counts=True)))}\")\n",
    "        elif key == 'class_names':\n",
    "            print(f\"   Nombres de clases: {array.tolist()}\")\n",
    "        elif key == 'filenames':\n",
    "            print(f\"   Primeros 3 nombres: {array[:3].tolist()}\")\n",
    "        elif key == 'masks':\n",
    "            unique_vals = np.unique(array)\n",
    "            print(f\"   Valores √∫nicos en masks: {unique_vals}\")\n",
    "            print(f\"   Cantidad de 1.0: {np.sum(array == 1.0)}, Cantidad de 0.0: {np.sum(array == 0.0)}\")\n",
    "        \n",
    "        print()\n",
    "    else:\n",
    "        validation_results[key] = {'exists': False, 'valid': False}\n",
    "        print(f\"‚úó FALTANTE - {key}:\")\n",
    "        print(f\"   La clave no existe en el archivo\\n\")\n",
    "\n",
    "# üéØ RESUMEN DE VALIDACI√ìN\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üéØ RESUMEN DE VALIDACI√ìN:\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "all_valid = all(v.get('valid', False) for v in validation_results.values())\n",
    "total_keys = len(expected_keys)\n",
    "valid_keys = sum(1 for v in validation_results.values() if v.get('valid', False))\n",
    "\n",
    "print(f\"Claves v√°lidas: {valid_keys}/{total_keys}\")\n",
    "for key, result in validation_results.items():\n",
    "    status = \"‚úì\" if result.get('valid', False) else \"‚úó\"\n",
    "    print(f\"  {status} {key}: {result.get('shape', 'FALTANTE')}\")\n",
    "\n",
    "if all_valid:\n",
    "    print(f\"\\n‚úÖ ESTRUCTURA V√ÅLIDA - El archivo contiene todas las claves con las formas correctas\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå ESTRUCTURA INV√ÅLIDA - Revisar los errores arriba\")\n",
    "\n",
    "# üì¶ CARGAR LOS DATOS\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üì¶ CARGANDO DATOS:\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "X = data_gcn['X']\n",
    "y = data_gcn['y']\n",
    "filenames = data_gcn['filenames']\n",
    "class_names = data_gcn['class_names']\n",
    "masks = data_gcn['masks']\n",
    "\n",
    "print(f\"‚úì X (embeddings): {X.shape} | dtype: {X.dtype}\")\n",
    "print(f\"‚úì y (etiquetas): {y.shape} | unique classes: {len(np.unique(y))}\")\n",
    "print(f\"‚úì filenames: {filenames.shape} | samples: {len(filenames)}\")\n",
    "print(f\"‚úì class_names: {class_names.shape} | total clases: {len(class_names)}\")\n",
    "print(f\"‚úì masks: {masks.shape} | dtype: {masks.dtype}\")\n",
    "\n",
    "print(f\"\\n‚úÖ CARGA COMPLETADA EXITOSAMENTE\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a48334bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hands segments: (864, 12, 100)\n",
      "Pose segments: (864, 12, 100)\n",
      "Face segments: (864, 12, 100)\n",
      "Masks: (864, 96)\n",
      "Metadata: (864,)\n",
      "\n",
      "Dataset shape: X=(864, 96, 300), y=(864,)\n",
      "Masks shape: (864, 96)\n",
      "Filenames: 864\n",
      "Classes: 30\n",
      "Categor√≠as: ['Adi√≥s', 'Buenas noches', 'Buenas tardes', 'Buenos d√≠as', 'Clase', 'Comenzar', 'Compa√±ero', 'Cuaderno', 'C√≥mo est√°', 'Deberes', 'Disculpa', 'Entender', 'Escribir', 'Escuchar', 'Estudiante', 'Examen', 'Explicar', 'Gracias', 'Hola', 'Lecci√≥n', 'Leer', 'Libro', 'L√°piz', 'Mucho gusto', 'Pizarr√≥n', 'Por favor', 'Pregunta', 'Profesor', 'Responder', 'Terminar']\n",
      "\n",
      "Distribuci√≥n de clases:\n",
      "  Clase 0 (Adi√≥s): 76 muestras\n",
      "  Clase 1 (Buenas noches): 10 muestras\n",
      "  Clase 2 (Buenas tardes): 22 muestras\n",
      "  Clase 3 (Buenos d√≠as): 22 muestras\n",
      "  Clase 4 (Clase): 18 muestras\n",
      "  Clase 5 (Comenzar): 18 muestras\n",
      "  Clase 6 (Compa√±ero): 28 muestras\n",
      "  Clase 7 (Cuaderno): 18 muestras\n",
      "  Clase 8 (C√≥mo est√°): 22 muestras\n",
      "  Clase 9 (Deberes): 16 muestras\n",
      "  Clase 10 (Disculpa): 12 muestras\n",
      "  Clase 11 (Entender): 18 muestras\n",
      "  Clase 12 (Escribir): 28 muestras\n",
      "  Clase 13 (Escuchar): 24 muestras\n",
      "  Clase 14 (Estudiante): 28 muestras\n",
      "  Clase 15 (Examen): 18 muestras\n",
      "  Clase 16 (Explicar): 18 muestras\n",
      "  Clase 17 (Gracias): 82 muestras\n",
      "  Clase 18 (Hola): 76 muestras\n",
      "  Clase 19 (Lecci√≥n): 18 muestras\n",
      "  Clase 20 (Leer): 28 muestras\n",
      "  Clase 21 (Libro): 18 muestras\n",
      "  Clase 22 (L√°piz): 18 muestras\n",
      "  Clase 23 (Mucho gusto): 22 muestras\n",
      "  Clase 24 (Pizarr√≥n): 18 muestras\n",
      "  Clase 25 (Por favor): 68 muestras\n",
      "  Clase 26 (Pregunta): 22 muestras\n",
      "  Clase 27 (Profesor): 64 muestras\n",
      "  Clase 28 (Responder): 18 muestras\n",
      "  Clase 29 (Terminar): 16 muestras\n"
     ]
    }
   ],
   "source": [
    "# 1. CARGAR DATASET UMAP\n",
    "dataset_path = Path('./daataset/G2_umap_segments_separado.npz')\n",
    "data = np.load(dataset_path, allow_pickle=True)\n",
    "\n",
    "# Cargar los arrays con las claves correctas\n",
    "hands_segments = data['hands_segments']  # (864, 12, 100)\n",
    "pose_segments = data['pose_segments']    # (864, 12, 100)\n",
    "face_segments = data['face_segments']    # (864, 12, 100)\n",
    "masks = data['masks']                     # (864, 96)\n",
    "metadata = data['metadata']               # (864,)\n",
    "\n",
    "print(f\"Hands segments: {hands_segments.shape}\")\n",
    "print(f\"Pose segments: {pose_segments.shape}\")\n",
    "print(f\"Face segments: {face_segments.shape}\")\n",
    "print(f\"Masks: {masks.shape}\")\n",
    "print(f\"Metadata: {metadata.shape}\")\n",
    "\n",
    "# Concatenar los 3 segmentos (hands, pose, face) en el eje de caracter√≠sticas\n",
    "# De (864, 12, 100) cada uno -> (864, 12, 300)\n",
    "X = np.concatenate([hands_segments, pose_segments, face_segments], axis=2)\n",
    "\n",
    "# Expandir para tener 96 segmentos (repitiendo cada segmento 8 veces: 12 * 8 = 96)\n",
    "X_expanded = np.repeat(X, 8, axis=1)  # (864, 96, 300)\n",
    "\n",
    "# Extraer las categor√≠as del metadata y crear mapeo a n√∫meros\n",
    "categorias = [item['categoria'] for item in metadata]\n",
    "categorias_unicas = sorted(list(set(categorias)))\n",
    "categoria_a_idx = {cat: idx for idx, cat in enumerate(categorias_unicas)}\n",
    "\n",
    "# Convertir categor√≠as a √≠ndices num√©ricos\n",
    "y = np.array([categoria_a_idx[cat] for cat in categorias])\n",
    "\n",
    "# Guardar nombres de archivos\n",
    "filenames = np.array([item['file'] for item in metadata])\n",
    "\n",
    "print(f\"\\nDataset shape: X={X_expanded.shape}, y={y.shape}\")\n",
    "print(f\"Masks shape: {masks.shape}\")\n",
    "print(f\"Filenames: {len(filenames)}\")\n",
    "print(f\"Classes: {len(categorias_unicas)}\")\n",
    "print(f\"Categor√≠as: {categorias_unicas}\")\n",
    "\n",
    "# Informaci√≥n de las clases\n",
    "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "print(f\"\\nDistribuci√≥n de clases:\")\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    print(f\"  Clase {cls} ({categorias_unicas[cls]}): {count} muestras\")\n",
    "\n",
    "# Renombrar para compatibilidad con el resto del notebook\n",
    "X = X_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4eb70d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (552, 96, 300), Val: (139, 96, 300), Test: (173, 96, 300)\n",
      "Batches - Train: 69, Val: 18, Test: 22\n"
     ]
    }
   ],
   "source": [
    "# 2. DATASET PYTORCH\n",
    "class VideoTransformerDataset(Dataset):\n",
    "    def __init__(self, X, y, masks):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.masks = torch.BoolTensor(masks)  # True = v√°lido, False = padding\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sequence': self.X[idx],      # (96, 128)\n",
    "            'label': self.y[idx],          # scalar\n",
    "            'mask': self.masks[idx]        # (96,)\n",
    "        }\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test, masks_train, masks_test = train_test_split(\n",
    "    X, y, masks, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val, masks_train, masks_val = train_test_split(\n",
    "    X_train, y_train, masks_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 8\n",
    "train_dataset = VideoTransformerDataset(X_train, y_train, masks_train)\n",
    "val_dataset = VideoTransformerDataset(X_val, y_val, masks_val)\n",
    "test_dataset = VideoTransformerDataset(X_test, y_test, masks_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Batches - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b2bdb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo:\n",
      "Total params: 6,154,974\n",
      "Trainable params: 6,154,974\n",
      "TransformerEncoderOnlyClassifier(\n",
      "  (input_projection): Linear(in_features=300, out_features=384, bias=True)\n",
      "  (pos_encoding): LearnablePositionalEncoding()\n",
      "  (dropout): Dropout(p=0.15, inplace=False)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=384, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.15, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=384, bias=True)\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.15, inplace=False)\n",
      "        (dropout2): Dropout(p=0.15, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=192, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=192, out_features=30, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3. ARCHITECTURE: TRANSFORMER ENCODER-ONLY\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding aprendible\"\"\"\n",
    "    def __init__(self, d_model, max_len=96):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "        nn.init.normal_(self.pe, mean=0, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerEncoderOnlyClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder-Only para clasificaci√≥n de secuencias temporales\n",
    "    - NO usa decoder\n",
    "    - Usa masked mean pooling\n",
    "    - Clasificaci√≥n global por secuencia\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=128,\n",
    "        d_model=384,\n",
    "        num_heads=4,\n",
    "        num_layers=6,\n",
    "        dim_feedforward=512,\n",
    "        dropout=0.15,\n",
    "        num_classes=30,\n",
    "        max_seq_len=96,\n",
    "        mlp_dropout=0.3,\n",
    "        activation='gelu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 1. Proyecci√≥n inicial (128 ‚Üí 256)\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # 2. Positional Encoding aprendible\n",
    "        self.pos_encoding = LearnablePositionalEncoding(d_model, max_seq_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 3. Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        # 4. Classification Head (MLP: 384 ‚Üí 192 ‚Üí num_classes)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 192),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(mlp_dropout),\n",
    "            nn.Linear(192, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch_size, seq_len, input_dim) = (B, 96, 128)\n",
    "            src_key_padding_mask: (batch_size, seq_len) = True para padding\n",
    "        Returns:\n",
    "            logits: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # 1. Proyecci√≥n inicial\n",
    "        x = self.input_projection(src)  # (B, 96, 256)\n",
    "        \n",
    "        # 2. Positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 3. Transformer encoder con m√°scara\n",
    "        x = self.transformer_encoder(\n",
    "            x,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )  # (B, 96, 256)\n",
    "        \n",
    "        # 4. Masked mean pooling (solo frames v√°lidos)\n",
    "        if src_key_padding_mask is not None:\n",
    "            # src_key_padding_mask: True = padding (ignorar)\n",
    "            # Convertir a float: 0 para padding, 1 para v√°lido\n",
    "            mask_float = (~src_key_padding_mask).float().unsqueeze(-1)  # (B, 96, 1)\n",
    "            x_masked = x * mask_float  # (B, 96, 256)\n",
    "            sum_masked = x_masked.sum(dim=1)  # (B, 256)\n",
    "            count_valid = mask_float.sum(dim=1)  # (B, 1)\n",
    "            x_pooled = sum_masked / (count_valid + 1e-9)  # (B, 256)\n",
    "        else:\n",
    "            # Sin m√°scara: mean pooling simple\n",
    "            x_pooled = x.mean(dim=1)  # (B, 256)\n",
    "        \n",
    "        # 5. Clasificador\n",
    "        logits = self.classifier(x_pooled)  # (B, num_classes)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Crear modelo\n",
    "num_classes = len(np.unique(y))\n",
    "model = TransformerEncoderOnlyClassifier(\n",
    "    input_dim=300,  # UMAP embeddings\n",
    "    d_model=384,\n",
    "    num_heads=4,\n",
    "    num_layers=6,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.15,\n",
    "    num_classes=num_classes,\n",
    "    max_seq_len=96,\n",
    "    mlp_dropout=0.3,\n",
    "    activation='gelu'\n",
    ").to(device)\n",
    "\n",
    "# Contar par√°metros\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModelo:\")\n",
    "print(f\"Total params: {total_params:,}\")\n",
    "print(f\"Trainable params: {trainable_params:,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd128405",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "SequentialLR does not support `ReduceLROnPlateau` scheduler as it requires additional kwargs to be specified when calling `step`, but got one at index 1 in the given schedulers sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 47\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Decay: reducir LR cuando val_acc no mejora\u001b[39;00m\n\u001b[0;32m     39\u001b[0m main_scheduler \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(\n\u001b[0;32m     40\u001b[0m     optimizer,\n\u001b[0;32m     41\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m\n\u001b[0;32m     45\u001b[0m )\n\u001b[1;32m---> 47\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mSequentialLR\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedulers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mwarmup_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_scheduler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmilestones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfiguraci√≥n de entrenamiento:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\isaiy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:1139\u001b[0m, in \u001b[0;36mSequentialLR.__init__\u001b[1;34m(self, optimizer, schedulers, milestones, last_epoch)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should have `optimizer` as its attribute.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1137\u001b[0m     )\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scheduler, ReduceLROnPlateau):\n\u001b[1;32m-> 1139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not support `ReduceLROnPlateau` scheduler as it \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires additional kwargs to be specified when calling `step`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1142\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got one at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the given schedulers sequence.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1143\u001b[0m     )\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;241m!=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39moptimizer:\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1146\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expects all schedulers to belong to the same optimizer, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1147\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot scheduler \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is different from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1149\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: SequentialLR does not support `ReduceLROnPlateau` scheduler as it requires additional kwargs to be specified when calling `step`, but got one at index 1 in the given schedulers sequence."
     ]
    }
   ],
   "source": [
    "# 4. ENTRENAMIENTO - CONFIG\n",
    "config = {\n",
    "    'optimizer': 'AdamW',\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'loss': 'CrossEntropyLoss',\n",
    "    'label_smoothing': 0.1,\n",
    "    'batch_size': 8,\n",
    "    'max_epochs': 50,\n",
    "    'early_stopping_patience': 20,\n",
    "    'gradient_clip': 1.0,\n",
    "    'num_classes': num_classes,\n",
    "    'input_dim': 300,\n",
    "    'dataset_type': 'UMAP Embeddings',\n",
    "    'device': str(device)\n",
    "}\n",
    "\n",
    "# Loss con label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['lr'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# LR Scheduler con warm-up\n",
    "from torch.optim.lr_scheduler import LinearLR, ExponentialLR, SequentialLR\n",
    "\n",
    "# Warm-up: aumentar LR linealmente en primeros 5 epochs\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer, \n",
    "    start_factor=0.1, \n",
    "    total_iters=5\n",
    ")\n",
    "\n",
    "# Decay: reducir LR exponencialmente despu√©s del warm-up (5% por √©poca)\n",
    "decay_scheduler = ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=0.95\n",
    ")\n",
    "\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup_scheduler, decay_scheduler],\n",
    "    milestones=[5]\n",
    ")\n",
    "\n",
    "print(\"Configuraci√≥n de entrenamiento:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d3ec2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones de entrenamiento definidas ‚úì\n"
     ]
    }
   ],
   "source": [
    "# 5. FUNCIONES DE ENTRENAMIENTO Y EVALUACI√ìN\n",
    "def train_epoch(model, loader, criterion, optimizer, device, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        sequences = batch['sequence'].to(device)  # (B, 96, 128)\n",
    "        labels = batch['label'].to(device)        # (B,)\n",
    "        masks = batch['mask'].to(device)          # (B, 96)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(sequences, src_key_padding_mask=~masks)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = total_loss / len(loader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        sequences = batch['sequence'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        \n",
    "        logits = model(sequences, src_key_padding_mask=~masks)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_logits.extend(logits.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = total_loss / len(loader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc, np.array(all_preds), np.array(all_labels), np.array(all_logits)\n",
    "\n",
    "print(\"Funciones de entrenamiento definidas ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc0f4f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Iniciando entrenamiento - Epoch max: 50, Patience: 20\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50 | Train Loss: 3.0503 | Train Acc: 0.1395 | Val Loss: 3.1043 | Val Acc: 0.1223 | LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/50 | Train Loss: 2.9633 | Train Acc: 0.1649 | Val Loss: 3.0261 | Val Acc: 0.1007 | LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/50 | Train Loss: 2.9410 | Train Acc: 0.1793 | Val Loss: 2.8956 | Val Acc: 0.1439 | LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/50 | Train Loss: 2.8930 | Train Acc: 0.1957 | Val Loss: 2.8535 | Val Acc: 0.2446 | LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/50 | Train Loss: 2.8473 | Train Acc: 0.1920 | Val Loss: 2.7346 | Val Acc: 0.2230 | LR: 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25/50 | Train Loss: 2.8111 | Train Acc: 0.2101 | Val Loss: 3.0222 | Val Acc: 0.1727 | LR: 4.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30/50 | Train Loss: 2.7258 | Train Acc: 0.2228 | Val Loss: 2.6868 | Val Acc: 0.2014 | LR: 4.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  35/50 | Train Loss: 2.6879 | Train Acc: 0.2337 | Val Loss: 2.8014 | Val Acc: 0.1367 | LR: 4.99e-04\n",
      "\n",
      "Early stopping at epoch 35\n",
      "\n",
      "Mejor modelo cargado desde epoch 14 con Val Acc: 0.2446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# 6. ENTRENAMIENTO PRINCIPAL\n",
    "training_log = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "patience_counter = 0\n",
    "max_epochs = config['max_epochs']\n",
    "early_stopping_patience = config['early_stopping_patience']\n",
    "\n",
    "# Crear directorio para guardar modelos\n",
    "Path('./g5.0_umap').mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Iniciando entrenamiento - Epoch max: {max_epochs}, Patience: {early_stopping_patience}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Val\n",
    "    val_loss, val_acc, _, _, _ = eval_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # LR Scheduler\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Actualizar scheduler seg√∫n la etapa\n",
    "    if epoch < 5:\n",
    "        # Warm-up: usar LinearLR\n",
    "        warmup_scheduler.step()\n",
    "    else:\n",
    "        # Plateau reduction: usar ReduceLROnPlateau\n",
    "        main_scheduler.step(val_acc)\n",
    "    \n",
    "    # Log\n",
    "    training_log['epoch'].append(epoch)\n",
    "    training_log['train_loss'].append(train_loss)\n",
    "    training_log['train_acc'].append(train_acc)\n",
    "    training_log['val_loss'].append(val_loss)\n",
    "    training_log['val_acc'].append(val_acc)\n",
    "    training_log['lr'].append(current_lr)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        patience_counter = 0\n",
    "        # Guardar mejor modelo\n",
    "        best_model_path = Path('./g5.0_umap/best_model.pt')\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Print\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{max_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping trigger\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Cargar mejor modelo\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "print(f\"\\nMejor modelo cargado desde epoch {best_epoch} con Val Acc: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3187fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando en Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 2.5841\n",
      "Test Accuracy: 0.2659\n",
      "Macro-F1: 0.0920\n",
      "Macro-Precision: 0.0776\n",
      "Macro-Recall: 0.1233\n",
      "Top-3 Accuracy: 0.5318\n",
      "\n",
      "F1 Score por clase (primeras 5 clases):\n",
      "  Clase 0: 0.2759\n",
      "  Clase 1: 0.0000\n",
      "  Clase 2: 0.0000\n",
      "  Clase 3: 0.0000\n",
      "  Clase 4: 0.0000\n",
      "\n",
      "Matriz de confusi√≥n shape: (30, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# 7. EVALUACI√ìN EN TEST SET\n",
    "# Reimport sklearn metrics (caso se hayan sobreescrito)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"Evaluando en Test Set...\")\n",
    "test_loss, test_acc, test_preds, test_labels, test_logits = eval_epoch(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# M√©tricas adicionales\n",
    "macro_f1 = f1_score(test_labels, test_preds, average='macro', zero_division=0)\n",
    "macro_precision = precision_score(test_labels, test_preds, average='macro', zero_division=0)\n",
    "macro_recall = recall_score(test_labels, test_preds, average='macro', zero_division=0)\n",
    "top3_acc = top_k_accuracy_score(test_labels, test_logits, k=3, labels=np.arange(num_classes))\n",
    "\n",
    "print(f\"Macro-F1: {macro_f1:.4f}\")\n",
    "print(f\"Macro-Precision: {macro_precision:.4f}\")\n",
    "print(f\"Macro-Recall: {macro_recall:.4f}\")\n",
    "print(f\"Top-3 Accuracy: {top3_acc:.4f}\")\n",
    "\n",
    "# F1 por clase\n",
    "f1_per_class = f1_score(test_labels, test_preds, average=None, zero_division=0)\n",
    "print(f\"\\nF1 Score por clase (primeras 5 clases):\")\n",
    "for i in range(min(5, num_classes)):\n",
    "    print(f\"  Clase {i}: {f1_per_class[i]:.4f}\")\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "print(f\"\\nMatriz de confusi√≥n shape: {cm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2416346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Guardando resultados en C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\\G4-RESULTS...\n",
      "‚úì Guardado: training_log.csv\n",
      "‚úì Guardado: metrics.csv\n",
      "‚úì Guardado: per_class_metrics.csv\n",
      "‚úì Guardado: confusion_matrix.csv\n",
      "‚úì Guardado: config.json\n"
     ]
    }
   ],
   "source": [
    "# 8. GUARDAR RESULTADOS BASELINE\n",
    "# Crear directorio de salida\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\nüíæ Guardando resultados en {output_dir}...\")\n",
    "\n",
    "# 1. Training Log CSV\n",
    "pd.DataFrame(training_log).to_csv(output_dir / 'training_log.csv', index=False)\n",
    "print(f\"‚úì Guardado: training_log.csv\")\n",
    "\n",
    "# 2. Metrics CSV\n",
    "pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Macro-F1', 'Macro-Precision', 'Macro-Recall', 'Top-3 Accuracy', 'Test Loss'],\n",
    "    'Value': [test_acc, macro_f1, macro_precision, macro_recall, top3_acc, test_loss]\n",
    "}).to_csv(output_dir / 'metrics.csv', index=False)\n",
    "print(f\"‚úì Guardado: metrics.csv\")\n",
    "\n",
    "# 3. Per-class metrics\n",
    "# Extract unique class names (one per class ID)\n",
    "unique_class_names = []\n",
    "for class_id in range(num_classes):\n",
    "    idx = np.where(y == class_id)[0][0]\n",
    "    # Extract base name without extension\n",
    "    class_name = str(filenames[idx]).replace('.json', '').split('_')[0]\n",
    "    unique_class_names.append(class_name)\n",
    "\n",
    "class_report = classification_report(\n",
    "    test_labels, test_preds, \n",
    "    target_names=unique_class_names,\n",
    "    output_dict=True, zero_division=0\n",
    ")\n",
    "pd.DataFrame(class_report).T.to_csv(output_dir / 'per_class_metrics.csv')\n",
    "print(f\"‚úì Guardado: per_class_metrics.csv\")\n",
    "\n",
    "# 4. Confusion Matrix CSV\n",
    "pd.DataFrame(cm).to_csv(output_dir / 'confusion_matrix.csv', index=False, header=False)\n",
    "print(f\"‚úì Guardado: confusion_matrix.csv\")\n",
    "\n",
    "# 5. Config JSON\n",
    "model_config = {\n",
    "    'experiment_type': 'baseline',\n",
    "    'architecture': 'TransformerEncoderOnly',\n",
    "    'input_dim': config['input_dim'],\n",
    "    'd_model': 256,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'dim_feedforward': 1024,\n",
    "    'dropout': current_config['dropout'],\n",
    "    'num_classes': num_classes,\n",
    "    'max_seq_len': 96,\n",
    "    'use_class_weights': current_config['use_class_weights'],\n",
    "    'label_smoothing': current_config['label_smoothing'],\n",
    "    'best_epoch': int(best_epoch),\n",
    "    'best_val_acc': float(best_val_acc),\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_macro_f1': float(macro_f1),\n",
    "    'training_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "with open(output_dir / 'config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_config, f, indent=2, ensure_ascii=False)\n",
    "print(f\"‚úì Guardado: config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c82d843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé® Generando visualizaciones...\n",
      "‚úì Guardado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\\G4-RESULTS\\training_curves.png\n",
      "‚úì Curvas de aprendizaje generadas\n"
     ]
    }
   ],
   "source": [
    "# 9. VISUALIZACIONES - CURVAS DE APRENDIZAJE\n",
    "print(f\"\\nüé® Generando visualizaciones...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].plot(training_log['epoch'], training_log['train_loss'], 'b-', label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0, 0].plot(training_log['epoch'], training_log['val_loss'], 'r-', label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0, 0].axvline(best_epoch, color='g', linestyle='--', alpha=0.7, linewidth=2, label=f'Best Epoch {best_epoch+1}')\n",
    "axes[0, 0].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Curva de Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(training_log['epoch'], training_log['train_acc'], 'b-', label='Train Acc', marker='o', linewidth=2)\n",
    "axes[0, 1].plot(training_log['epoch'], training_log['val_acc'], 'r-', label='Val Acc', marker='s', linewidth=2)\n",
    "axes[0, 1].axvline(best_epoch, color='g', linestyle='--', alpha=0.7, linewidth=2, label=f'Best Epoch {best_epoch+1}')\n",
    "axes[0, 1].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Curva de Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(training_log['epoch'], training_log['lr'], 'g-', marker='o', linewidth=2)\n",
    "axes[1, 0].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Programaci√≥n de Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "metrics_names = ['Accuracy', 'Macro-F1', 'Top-3 Acc']\n",
    "metrics_values = [test_acc, macro_f1, top3_acc]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "bars = axes[1, 1].bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('M√©tricas en Test Set', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylim([0, 1.05])\n",
    "for bar, v in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02, \n",
    "                    f'{v:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(f'Curvas de Aprendizaje - {EXPERIMENT_TYPE.upper()}', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = output_dir / 'training_curves.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: {plot_path}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"‚úì Curvas de aprendizaje generadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f31507fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé® Generando matriz de confusi√≥n...\n",
      "‚úì Guardado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\\G4-RESULTS\\confusion_matrix.png\n",
      "‚úì Matriz de confusi√≥n completada\n"
     ]
    }
   ],
   "source": [
    "# 10. MATRIZ DE CONFUSI√ìN CON NOMBRES\n",
    "print(f\"\\nüé® Generando matriz de confusi√≥n...\")\n",
    "\n",
    "unique_classes_list = sorted(list(set(test_labels)))\n",
    "class_labels = [filenames[i] if i < len(filenames) else f'Class {i}' for i in unique_classes_list]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 18))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            cbar_kws={'label': 'Muestras'}, ax=ax,\n",
    "            xticklabels=class_labels, yticklabels=class_labels, \n",
    "            square=True)\n",
    "ax.set_xlabel('Clase Predicha', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Clase Real', fontsize=14, fontweight='bold')\n",
    "ax.set_title(f'Matriz de Confusi√≥n - {EXPERIMENT_TYPE.upper()} (Accuracy: {test_acc:.4f})', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "cm_plot_path = output_dir / 'confusion_matrix.png'\n",
    "plt.savefig(cm_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: {cm_plot_path}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"‚úì Matriz de confusi√≥n completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36e680b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé® Generando an√°lisis por clase...\n",
      "‚úì Guardado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\\G4-RESULTS\\per_class_analysis.png\n",
      "‚úì An√°lisis por clase completado\n"
     ]
    }
   ],
   "source": [
    "# 11. AN√ÅLISIS POR CLASE\n",
    "print(f\"\\nüé® Generando an√°lisis por clase...\")\n",
    "\n",
    "# Calcular precision, recall, f1 por clase\n",
    "precision_per_class = []\n",
    "recall_per_class = []\n",
    "f1_per_class_array = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    if i in unique_classes_list:\n",
    "        idx = unique_classes_list.index(i)\n",
    "        tp = cm[idx, idx]\n",
    "        fp = cm[:, idx].sum() - tp\n",
    "        fn = cm[idx, :].sum() - tp\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_value = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        precision_per_class.append(precision)\n",
    "        recall_per_class.append(recall)\n",
    "        f1_per_class_array.append(f1_value)\n",
    "    else:\n",
    "        precision_per_class.append(0)\n",
    "        recall_per_class.append(0)\n",
    "        f1_per_class_array.append(0)\n",
    "\n",
    "precision_per_class = np.array(precision_per_class)\n",
    "recall_per_class = np.array(recall_per_class)\n",
    "f1_per_class_array = np.array(f1_per_class_array)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig_pc, axes_pc = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "y_pos = np.arange(num_classes)\n",
    "axes_pc[0].barh(y_pos, precision_per_class, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "axes_pc[0].set_yticks(y_pos)\n",
    "axes_pc[0].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[0].set_xlabel('Precision', fontsize=12, fontweight='bold')\n",
    "axes_pc[0].set_title('Precision por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[0].set_xlim([0, 1])\n",
    "axes_pc[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes_pc[1].barh(y_pos, recall_per_class, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "axes_pc[1].set_yticks(y_pos)\n",
    "axes_pc[1].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "axes_pc[1].set_title('Recall por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[1].set_xlim([0, 1])\n",
    "axes_pc[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes_pc[2].barh(y_pos, f1_per_class_array, color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "axes_pc[2].set_yticks(y_pos)\n",
    "axes_pc[2].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[2].set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes_pc[2].set_title('F1-Score por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[2].set_xlim([0, 1])\n",
    "axes_pc[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle(f'An√°lisis por Clase - {EXPERIMENT_TYPE.upper()}', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "per_class_plot_path = output_dir / 'per_class_analysis.png'\n",
    "plt.savefig(per_class_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: {per_class_plot_path}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"‚úì An√°lisis por clase completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0330b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Generando resumen ejecutivo...\n",
      "‚úì Guardado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\\G4-RESULTS\\RESUMEN.txt\n",
      "\n",
      "================================================================================\n",
      "‚úÖ BASELINE (G4-RESULTS) COMPLETADO\n",
      "================================================================================\n",
      "  Test Accuracy: 0.2659\n",
      "  Macro F1:      0.0920\n",
      "  Top-3 Acc:     0.5318\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 12. RESUMEN EJECUTIVO\n",
    "print(f\"\\nüìù Generando resumen ejecutivo...\")\n",
    "\n",
    "# Top 5 clases por F1\n",
    "f1_per_class_list = [(i, f1_value) for i, f1_value in enumerate(f1_per_class_array)]\n",
    "class_f1_sorted = sorted(f1_per_class_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "resumen = f\"\"\"\n",
    "{'='*80}\n",
    "RESUMEN EJECUTIVO - {current_config['folder_name']}\n",
    "{'='*80}\n",
    "\n",
    "üìä PERFORMANCE:\n",
    "  ‚Ä¢ Test Accuracy:       {test_acc:.4f}\n",
    "  ‚Ä¢ Macro F1-Score:      {macro_f1:.4f}\n",
    "  ‚Ä¢ Macro Precision:     {macro_precision:.4f}\n",
    "  ‚Ä¢ Macro Recall:        {macro_recall:.4f}\n",
    "  ‚Ä¢ Top-3 Accuracy:      {top3_acc:.4f}\n",
    "\n",
    "üèóÔ∏è ARQUITECTURA:\n",
    "  ‚Ä¢ Modelo:              Transformer Encoder-Only\n",
    "  ‚Ä¢ Dataset:             UMAP Embeddings (reducci√≥n de dimensionalidad)\n",
    "  ‚Ä¢ Input Features:      {config['input_dim']}\n",
    "  ‚Ä¢ Sequence Length:     96 frames\n",
    "  ‚Ä¢ Embedding Dim:       256\n",
    "  ‚Ä¢ Attention Heads:     8\n",
    "  ‚Ä¢ Encoder Layers:      6\n",
    "  ‚Ä¢ Dim Feedforward:     1024\n",
    "  ‚Ä¢ Total Parameters:    {total_params:,}\n",
    "\n",
    "‚öôÔ∏è CONFIGURACI√ìN:\n",
    "  ‚Ä¢ Dropout:             {current_config['dropout']}\n",
    "  ‚Ä¢ Class Weights:       {'Activado' if current_config['use_class_weights'] else 'Desactivado'}\n",
    "  ‚Ä¢ Label Smoothing:     {current_config['label_smoothing']}\n",
    "  ‚Ä¢ Best Epoch:          {best_epoch}\n",
    "  ‚Ä¢ Optimizer:           AdamW (lr={config['lr']}, weight_decay={config['weight_decay']})\n",
    "  ‚Ä¢ Scheduler:           CosineAnnealingWarmRestarts\n",
    "\n",
    "üéØ VENTAJAS UMAP EMBEDDINGS:\n",
    "  ‚Ä¢ Reducci√≥n de dimensionalidad: Preserva estructura local y global\n",
    "  ‚Ä¢ Menor carga computacional\n",
    "  ‚Ä¢ M√°scaras por muestra nativas (m√°s preciso)\n",
    "  ‚Ä¢ Mejor generalizaci√≥n con menos features ruidosas\n",
    "\n",
    "üìà TOP 5 CLASES (F1-Score):\n",
    "\"\"\"\n",
    "for rank, (class_idx, f1_value) in enumerate(class_f1_sorted[:5], 1):\n",
    "    class_name = filenames[class_idx] if class_idx < len(filenames) else f'Class {class_idx}'\n",
    "    resumen += f\"  {rank}. {class_name:20s} | F1: {f1_value:.4f}\\n\"\n",
    "\n",
    "resumen += f\"\\n{'='*80}\\n\"\n",
    "\n",
    "# Guardar\n",
    "resumen_path = output_dir / 'RESUMEN.txt'\n",
    "with open(resumen_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(resumen)\n",
    "print(f\"‚úì Guardado: {resumen_path}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ BASELINE ({current_config['folder_name']}) COMPLETADO\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Macro F1:      {macro_f1:.4f}\")\n",
    "print(f\"  Top-3 Acc:     {top3_acc:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61673c2",
   "metadata": {},
   "source": [
    "# Experimentos de Mejora - Transformer Encoder-Only con UMAP Embeddings\n",
    "\n",
    "## üéØ Objetivo\n",
    "Implementar y comparar mejoras controladas sobre el modelo base para mejorar Macro-F1 y generalizaci√≥n usando embeddings UMAP.\n",
    "\n",
    "### Experimentos:\n",
    "- **Exp 0 (G5.0_UMAP)**: Baseline - Dropout 0.1, sin class weights, sin label smoothing\n",
    "- **Exp 1 (G6_UMAP)**: Class Weights + Dropout 0.3, sin label smoothing\n",
    "- **Exp 2 (G7_UMAP)**: Dropout 0.3 + Label Smoothing 0.1, sin class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c28ee199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Funci√≥n create_model definida\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n para crear modelo con dropout configurable\n",
    "def create_model(dropout_config=0.1):\n",
    "    \"\"\"Crea modelo con configuraci√≥n espec√≠fica de dropout\"\"\"\n",
    "    model = TransformerEncoderOnlyClassifier(\n",
    "        input_dim=config['input_dim'],  # 128 para UMAP\n",
    "        d_model=256,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        dim_feedforward=1024,\n",
    "        dropout=dropout_config,\n",
    "        num_classes=num_classes,\n",
    "        max_seq_len=96,\n",
    "        mlp_dropout=0.2,\n",
    "        activation='gelu'\n",
    "    ).to(device)\n",
    "    return model\n",
    "\n",
    "print(\"‚úì Funci√≥n create_model definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396ff8f",
   "metadata": {},
   "source": [
    "## üß™ Experimento 0 (G5.0_UMAP) - Baseline\n",
    "- Dropout: 0.1\n",
    "- Sin class weights\n",
    "- Sin label smoothing\n",
    "- Resultados ya obtenidos arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7918bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Resultados baseline registrados\n"
     ]
    }
   ],
   "source": [
    "# Resultados Exp 0 (Baseline)\n",
    "exp0_results = {\n",
    "    'experiment': 'G4-RESULTS',\n",
    "    'dropout': current_config['dropout'],\n",
    "    'class_weights': current_config.get('use_class_weights', False),\n",
    "    'label_smoothing': current_config.get('label_smoothing', 0.0),\n",
    "    'test_accuracy': test_acc,\n",
    "    'test_macro_f1': macro_f1,\n",
    "    'test_top3_accuracy': top3_acc,\n",
    "    'test_loss': test_loss,\n",
    "    'best_epoch': best_epoch,\n",
    "    'best_val_acc': best_val_acc\n",
    "}\n",
    "\n",
    "print(f\"‚úì Resultados {EXPERIMENT_TYPE} registrados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7d353",
   "metadata": {},
   "source": [
    "## üß™ Experimento 1 (G6_UMAP) - Class Weights + Dropout 0.3\n",
    "- Class weights calculados de y_train\n",
    "- Dropout: 0.3 (mayor regularizaci√≥n)\n",
    "- Sin label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30969ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Iniciando Experimento 1: G4-RESULTS-CLASS-WEIGHTS\n",
      "================================================================================\n",
      "üìÅ Directorio: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\\G4-RESULTS-CLASS-WEIGHTS\n",
      "\n",
      "Entrenando con Class Weights y Dropout 0.3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50 | Train Loss: 3.4511 | Val Acc: 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/50 | Train Loss: 3.4152 | Val Acc: 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/50 | Train Loss: 3.4105 | Val Acc: 0.1511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/50 | Train Loss: 3.3204 | Val Acc: 0.0863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì CLASS-WEIGHTS completado:\n",
      "  Test Accuracy: 0.1503\n",
      "  Macro F1: 0.0254\n",
      "  Top-3 Acc: 0.2081\n",
      "‚úì Guardado: training_log.csv\n",
      "‚úì Guardado: metrics.csv\n",
      "‚úì Guardado: per_class_metrics.csv\n",
      "‚úì Guardado: confusion_matrix.csv\n",
      "‚úì Guardado: config.json\n",
      "‚úì Guardado: training_curves.png\n",
      "‚úì Guardado: confusion_matrix.png\n",
      "‚úì Guardado: per_class_analysis.png\n",
      "‚úì Guardado: RESUMEN.txt\n",
      "================================================================================\n",
      "‚úÖ EXPERIMENTO 1 (CLASS-WEIGHTS) COMPLETADO\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENTO 1: G4-RESULTS-CLASS-WEIGHTS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Iniciando Experimento 1: G4-RESULTS-CLASS-WEIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configurar directorio de salida\n",
    "output_dir_exp1 = ROOT_PATH / 'G4-RESULTS-CLASS-WEIGHTS'\n",
    "output_dir_exp1.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ Directorio: {output_dir_exp1}\")\n",
    "\n",
    "# Calcular class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.FloatTensor(class_weights_array).to(device)\n",
    "\n",
    "# Crear modelo con dropout 0.3\n",
    "model_exp1 = create_model(dropout_config=0.3)\n",
    "criterion_exp1 = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.0)\n",
    "optimizer_exp1 = AdamW(model_exp1.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "scheduler_exp1 = ReduceLROnPlateau(optimizer_exp1, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "# Entrenamiento\n",
    "training_log_exp1 = {'epoch': [], 'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "best_val_acc_exp1 = 0.0\n",
    "best_epoch_exp1 = 0\n",
    "patience_counter_exp1 = 0\n",
    "\n",
    "print(f\"\\nEntrenando con Class Weights y Dropout 0.3...\")\n",
    "for epoch in range(config['max_epochs']):\n",
    "    train_loss, train_acc = train_epoch(model_exp1, train_loader, criterion_exp1, optimizer_exp1, device)\n",
    "    val_loss, val_acc, _, _, _ = eval_epoch(model_exp1, val_loader, criterion_exp1, device)\n",
    "    \n",
    "    current_lr = optimizer_exp1.param_groups[0]['lr']\n",
    "    scheduler_exp1.step(val_acc)\n",
    "    \n",
    "    training_log_exp1['epoch'].append(epoch)\n",
    "    training_log_exp1['train_loss'].append(train_loss)\n",
    "    training_log_exp1['train_acc'].append(train_acc)\n",
    "    training_log_exp1['val_loss'].append(val_loss)\n",
    "    training_log_exp1['val_acc'].append(val_acc)\n",
    "    training_log_exp1['lr'].append(current_lr)\n",
    "    \n",
    "    if val_acc > best_val_acc_exp1:\n",
    "        best_val_acc_exp1 = val_acc\n",
    "        best_epoch_exp1 = epoch\n",
    "        patience_counter_exp1 = 0\n",
    "        torch.save(model_exp1.state_dict(), output_dir_exp1 / 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter_exp1 += 1\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{config['max_epochs']} | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if patience_counter_exp1 >= config['early_stopping_patience']:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Cargar mejor modelo\n",
    "model_exp1.load_state_dict(torch.load(output_dir_exp1 / 'best_model.pt', map_location=device))\n",
    "\n",
    "# Reimport sklearn metrics (caso se hayan sobreescrito)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Evaluaci√≥n\n",
    "test_loss_exp1, test_acc_exp1, test_preds_exp1, test_labels_exp1, test_logits_exp1 = eval_epoch(\n",
    "    model_exp1, test_loader, criterion_exp1, device\n",
    ")\n",
    "\n",
    "macro_f1_exp1 = f1_score(test_labels_exp1, test_preds_exp1, average='macro', zero_division=0)\n",
    "macro_precision_exp1 = precision_score(test_labels_exp1, test_preds_exp1, average='macro', zero_division=0)\n",
    "macro_recall_exp1 = recall_score(test_labels_exp1, test_preds_exp1, average='macro', zero_division=0)\n",
    "top3_acc_exp1 = top_k_accuracy_score(test_labels_exp1, test_logits_exp1, k=3, labels=np.arange(num_classes))\n",
    "\n",
    "print(f\"\\n‚úì CLASS-WEIGHTS completado:\")\n",
    "print(f\"  Test Accuracy: {test_acc_exp1:.4f}\")\n",
    "print(f\"  Macro F1: {macro_f1_exp1:.4f}\")\n",
    "print(f\"  Top-3 Acc: {top3_acc_exp1:.4f}\")\n",
    "\n",
    "# 1. Training log\n",
    "pd.DataFrame(training_log_exp1).to_csv(output_dir_exp1 / 'training_log.csv', index=False)\n",
    "print(f\"‚úì Guardado: training_log.csv\")\n",
    "\n",
    "# 2. Metrics CSV\n",
    "pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Macro-F1', 'Macro-Precision', 'Macro-Recall', 'Top-3 Accuracy', 'Test Loss'],\n",
    "    'Value': [test_acc_exp1, macro_f1_exp1, macro_precision_exp1, macro_recall_exp1, top3_acc_exp1, test_loss_exp1]\n",
    "}).to_csv(output_dir_exp1 / 'metrics.csv', index=False)\n",
    "print(f\"‚úì Guardado: metrics.csv\")\n",
    "\n",
    "# 3. Per-class metrics\n",
    "# Extract unique class names (one per class ID)\n",
    "unique_class_names_exp1 = []\n",
    "for class_id in range(num_classes):\n",
    "    idx = np.where(y == class_id)[0][0]\n",
    "    class_name = str(filenames[idx]).replace('.json', '').split('_')[0]\n",
    "    unique_class_names_exp1.append(class_name)\n",
    "\n",
    "class_report_dict = classification_report(\n",
    "    test_labels_exp1, test_preds_exp1, \n",
    "    target_names=unique_class_names_exp1,\n",
    "    output_dict=True, zero_division=0\n",
    ")\n",
    "pd.DataFrame(class_report_dict).T.to_csv(output_dir_exp1 / 'per_class_metrics.csv')\n",
    "print(f\"‚úì Guardado: per_class_metrics.csv\")\n",
    "\n",
    "# 4. Confusion matrix CSV\n",
    "cm_exp1 = confusion_matrix(test_labels_exp1, test_preds_exp1)\n",
    "pd.DataFrame(cm_exp1).to_csv(output_dir_exp1 / 'confusion_matrix.csv', index=False, header=False)\n",
    "print(f\"‚úì Guardado: confusion_matrix.csv\")\n",
    "\n",
    "# 5. Config JSON\n",
    "config_exp1 = {\n",
    "    'experiment_type': 'class_weights',\n",
    "    'architecture': 'TransformerEncoderOnly',\n",
    "    'input_dim': config['input_dim'],\n",
    "    'd_model': 256,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'dim_feedforward': 1024,\n",
    "    'dropout': 0.3,\n",
    "    'num_classes': num_classes,\n",
    "    'max_seq_len': 96,\n",
    "    'use_class_weights': True,\n",
    "    'label_smoothing': 0.0,\n",
    "    'best_epoch': int(best_epoch_exp1),\n",
    "    'best_val_acc': float(best_val_acc_exp1),\n",
    "    'test_accuracy': float(test_acc_exp1),\n",
    "    'test_macro_f1': float(macro_f1_exp1),\n",
    "    'training_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "with open(output_dir_exp1 / 'config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_exp1, f, indent=2, ensure_ascii=False)\n",
    "print(f\"‚úì Guardado: config.json\")\n",
    "\n",
    "# 6. Training curves PNG\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].plot(training_log_exp1['epoch'], training_log_exp1['train_loss'], 'b-', label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0, 0].plot(training_log_exp1['epoch'], training_log_exp1['val_loss'], 'r-', label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0, 0].axvline(best_epoch_exp1, color='g', linestyle='--', alpha=0.7, linewidth=2, label=f'Best Epoch {best_epoch_exp1+1}')\n",
    "axes[0, 0].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Curva de Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(training_log_exp1['epoch'], training_log_exp1['train_acc'], 'b-', label='Train Acc', marker='o', linewidth=2)\n",
    "axes[0, 1].plot(training_log_exp1['epoch'], training_log_exp1['val_acc'], 'r-', label='Val Acc', marker='s', linewidth=2)\n",
    "axes[0, 1].axvline(best_epoch_exp1, color='g', linestyle='--', alpha=0.7, linewidth=2, label=f'Best Epoch {best_epoch_exp1+1}')\n",
    "axes[0, 1].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Curva de Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(training_log_exp1['epoch'], training_log_exp1['lr'], 'g-', marker='o', linewidth=2)\n",
    "axes[1, 0].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "metrics_names = ['Accuracy', 'Macro-F1', 'Top-3 Acc']\n",
    "metrics_values = [test_acc_exp1, macro_f1_exp1, top3_acc_exp1]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "bars = axes[1, 1].bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('M√©tricas en Test Set', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylim([0, 1.05])\n",
    "for bar, v in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02, \n",
    "                    f'{v:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Curvas de Aprendizaje - CLASS-WEIGHTS', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir_exp1 / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: training_curves.png\")\n",
    "plt.close()\n",
    "\n",
    "# 7. Confusion matrix PNG\n",
    "unique_classes_exp1 = sorted(list(set(test_labels_exp1)))\n",
    "class_labels_exp1 = [filenames[i] if i < len(filenames) else f'Class {i}' for i in unique_classes_exp1]\n",
    "\n",
    "fig_cm, ax_cm = plt.subplots(figsize=(20, 18))\n",
    "sns.heatmap(cm_exp1, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Muestras'}, ax=ax_cm,\n",
    "            xticklabels=class_labels_exp1, yticklabels=class_labels_exp1, square=True)\n",
    "ax_cm.set_xlabel('Clase Predicha', fontsize=14, fontweight='bold')\n",
    "ax_cm.set_ylabel('Clase Real', fontsize=14, fontweight='bold')\n",
    "ax_cm.set_title(f'Matriz de Confusi√≥n - CLASS-WEIGHTS (Accuracy: {test_acc_exp1:.4f})', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir_exp1 / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# 8. Per-class analysis PNG\n",
    "precision_per_class_exp1 = []\n",
    "recall_per_class_exp1 = []\n",
    "f1_per_class_array_exp1 = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    if i in unique_classes_exp1:\n",
    "        idx = unique_classes_exp1.index(i)\n",
    "        tp = cm_exp1[idx, idx]\n",
    "        fp = cm_exp1[:, idx].sum() - tp\n",
    "        fn = cm_exp1[idx, :].sum() - tp\n",
    "        \n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_value = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "        \n",
    "        precision_per_class_exp1.append(prec)\n",
    "        recall_per_class_exp1.append(rec)\n",
    "        f1_per_class_array_exp1.append(f1_value)\n",
    "    else:\n",
    "        precision_per_class_exp1.append(0)\n",
    "        recall_per_class_exp1.append(0)\n",
    "        f1_per_class_array_exp1.append(0)\n",
    "\n",
    "precision_per_class_exp1 = np.array(precision_per_class_exp1)\n",
    "recall_per_class_exp1 = np.array(recall_per_class_exp1)\n",
    "f1_per_class_array_exp1 = np.array(f1_per_class_array_exp1)\n",
    "\n",
    "fig_pc, axes_pc = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "y_pos = np.arange(num_classes)\n",
    "axes_pc[0].barh(y_pos, precision_per_class_exp1, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "axes_pc[0].set_yticks(y_pos)\n",
    "axes_pc[0].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[0].set_xlabel('Precision', fontsize=12, fontweight='bold')\n",
    "axes_pc[0].set_title('Precision por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[0].set_xlim([0, 1])\n",
    "axes_pc[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes_pc[1].barh(y_pos, recall_per_class_exp1, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "axes_pc[1].set_yticks(y_pos)\n",
    "axes_pc[1].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "axes_pc[1].set_title('Recall por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[1].set_xlim([0, 1])\n",
    "axes_pc[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes_pc[2].barh(y_pos, f1_per_class_array_exp1, color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "axes_pc[2].set_yticks(y_pos)\n",
    "axes_pc[2].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[2].set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes_pc[2].set_title('F1-Score por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[2].set_xlim([0, 1])\n",
    "axes_pc[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('An√°lisis por Clase - CLASS-WEIGHTS', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir_exp1 / 'per_class_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: per_class_analysis.png\")\n",
    "plt.close()\n",
    "\n",
    "# 9. RESUMEN.txt\n",
    "f1_per_class_list_exp1 = [(i, f1_value) for i, f1_value in enumerate(f1_per_class_array_exp1)]\n",
    "class_f1_sorted = sorted(f1_per_class_list_exp1, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "summary_exp1 = f\"\"\"\n",
    "{'='*80}\n",
    "RESUMEN EJECUTIVO - G4-RESULTS-CLASS-WEIGHTS\n",
    "{'='*80}\n",
    "\n",
    "üìä PERFORMANCE:\n",
    "  ‚Ä¢ Test Accuracy:       {test_acc_exp1:.4f}\n",
    "  ‚Ä¢ Macro F1-Score:      {macro_f1_exp1:.4f}\n",
    "  ‚Ä¢ Macro Precision:     {macro_precision_exp1:.4f}\n",
    "  ‚Ä¢ Macro Recall:        {macro_recall_exp1:.4f}\n",
    "  ‚Ä¢ Top-3 Accuracy:      {top3_acc_exp1:.4f}\n",
    "\n",
    "‚öôÔ∏è CONFIGURACI√ìN:\n",
    "  ‚Ä¢ Dropout:             0.3\n",
    "  ‚Ä¢ Class Weights:       Activado (balanced)\n",
    "  ‚Ä¢ Label Smoothing:     0.0\n",
    "  ‚Ä¢ Best Epoch:          {best_epoch_exp1}\n",
    "\n",
    "üìà TOP 5 CLASES (F1-Score):\n",
    "\"\"\"\n",
    "for rank, (class_idx, f1_value) in enumerate(class_f1_sorted[:5], 1):\n",
    "    class_name = filenames[class_idx] if class_idx < len(filenames) else f'Class {class_idx}'\n",
    "    summary_exp1 += f\"  {rank}. {class_name:20s} | F1: {f1_value:.4f}\\n\"\n",
    "\n",
    "summary_exp1 += f\"\\n{'='*80}\\n\"\n",
    "\n",
    "with open(output_dir_exp1 / 'RESUMEN.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_exp1)\n",
    "print(f\"‚úì Guardado: RESUMEN.txt\")\n",
    "\n",
    "# Resultados en diccionario\n",
    "exp1_results = {\n",
    "    'experiment': 'G4-RESULTS-CLASS-WEIGHTS',\n",
    "    'dropout': 0.3,\n",
    "    'class_weights': True,\n",
    "    'label_smoothing': 0.0,\n",
    "    'test_accuracy': test_acc_exp1,\n",
    "    'test_macro_f1': macro_f1_exp1,\n",
    "    'test_top3_accuracy': top3_acc_exp1,\n",
    "    'best_epoch': best_epoch_exp1\n",
    "}\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"‚úÖ EXPERIMENTO 1 (CLASS-WEIGHTS) COMPLETADO\")\n",
    "print(f\"\\n{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c0d15",
   "metadata": {},
   "source": [
    "## üß™ Experimento 2 (G7_UMAP) - Dropout 0.3 + Label Smoothing\n",
    "- Dropout: 0.3 (mayor regularizaci√≥n)\n",
    "- Label smoothing: 0.2 (m√°s agresivo que baseline)\n",
    "- Sin class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3705ec83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Iniciando Experimento 2: G4-RESULTS-LABEL-SMOOTH\n",
      "================================================================================\n",
      "üìÅ Directorio: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\\G4-RESULTS-LABEL-SMOOTH\n",
      "\n",
      "Entrenando con Label Smoothing 0.1 y Dropout 0.3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50 | Train Loss: 3.3199 | Val Acc: 0.0791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/50 | Train Loss: 3.2570 | Val Acc: 0.1007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/50 | Train Loss: 3.0678 | Val Acc: 0.1511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/50 | Train Loss: 2.9985 | Val Acc: 0.1223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/50 | Train Loss: 2.8664 | Val Acc: 0.2158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25/50 | Train Loss: 2.7748 | Val Acc: 0.2086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30/50 | Train Loss: 2.6158 | Val Acc: 0.2086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  35/50 | Train Loss: 2.5361 | Val Acc: 0.2446\n",
      "Early stopping at epoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì LABEL-SMOOTH completado:\n",
      "  Test Accuracy: 0.2601\n",
      "  Macro F1: 0.0781\n",
      "  Top-3 Acc: 0.4566\n",
      "‚úì Guardado: training_log.csv\n",
      "‚úì Guardado: metrics.csv\n",
      "‚úì Guardado: per_class_metrics.csv\n",
      "‚úì Guardado: confusion_matrix.csv\n",
      "‚úì Guardado: config.json\n",
      "‚úì Guardado: training_curves.png\n",
      "‚úì Guardado: confusion_matrix.png\n",
      "‚úì Guardado: per_class_analysis.png\n",
      "‚úì Guardado: RESUMEN.txt\n",
      "================================================================================\n",
      "‚úÖ EXPERIMENTO 2 (LABEL-SMOOTH) COMPLETADO\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENTO 2: G4-RESULTS-LABEL-SMOOTH\n",
    "# Reimport sklearn metrics (caso se hayan sobreescrito)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Iniciando Experimento 2: G4-RESULTS-LABEL-SMOOTH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configurar directorio\n",
    "output_dir_exp2 = ROOT_PATH / 'G4-RESULTS-LABEL-SMOOTH'\n",
    "output_dir_exp2.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ Directorio: {output_dir_exp2}\")\n",
    "\n",
    "# Crear modelo con dropout 0.3\n",
    "model_exp2 = create_model(dropout_config=0.3)\n",
    "criterion_exp2 = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer_exp2 = AdamW(model_exp2.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "scheduler_exp2 = ReduceLROnPlateau(optimizer_exp2, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "# Entrenamiento\n",
    "training_log_exp2 = {'epoch': [], 'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "best_val_acc_exp2 = 0.0\n",
    "best_epoch_exp2 = 0\n",
    "patience_counter_exp2 = 0\n",
    "\n",
    "print(f\"\\nEntrenando con Label Smoothing 0.1 y Dropout 0.3...\")\n",
    "for epoch in range(config['max_epochs']):\n",
    "    train_loss, train_acc = train_epoch(model_exp2, train_loader, criterion_exp2, optimizer_exp2, device)\n",
    "    val_loss, val_acc, _, _, _ = eval_epoch(model_exp2, val_loader, criterion_exp2, device)\n",
    "    \n",
    "    current_lr = optimizer_exp2.param_groups[0]['lr']\n",
    "    scheduler_exp2.step(val_acc)\n",
    "    \n",
    "    training_log_exp2['epoch'].append(epoch)\n",
    "    training_log_exp2['train_loss'].append(train_loss)\n",
    "    training_log_exp2['train_acc'].append(train_acc)\n",
    "    training_log_exp2['val_loss'].append(val_loss)\n",
    "    training_log_exp2['val_acc'].append(val_acc)\n",
    "    training_log_exp2['lr'].append(current_lr)\n",
    "    \n",
    "    if val_acc > best_val_acc_exp2:\n",
    "        best_val_acc_exp2 = val_acc\n",
    "        best_epoch_exp2 = epoch\n",
    "        patience_counter_exp2 = 0\n",
    "        torch.save(model_exp2.state_dict(), output_dir_exp2 / 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter_exp2 += 1\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{config['max_epochs']} | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if patience_counter_exp2 >= config['early_stopping_patience']:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Cargar mejor modelo\n",
    "model_exp2.load_state_dict(torch.load(output_dir_exp2 / 'best_model.pt', map_location=device))\n",
    "\n",
    "# Evaluaci√≥n\n",
    "test_loss_exp2, test_acc_exp2, test_preds_exp2, test_labels_exp2, test_logits_exp2 = eval_epoch(\n",
    "    model_exp2, test_loader, criterion_exp2, device\n",
    ")\n",
    "\n",
    "macro_f1_exp2 = f1_score(test_labels_exp2, test_preds_exp2, average='macro', zero_division=0)\n",
    "macro_precision_exp2 = precision_score(test_labels_exp2, test_preds_exp2, average='macro', zero_division=0)\n",
    "macro_recall_exp2 = recall_score(test_labels_exp2, test_preds_exp2, average='macro', zero_division=0)\n",
    "top3_acc_exp2 = top_k_accuracy_score(test_labels_exp2, test_logits_exp2, k=3, labels=np.arange(num_classes))\n",
    "\n",
    "print(f\"\\n‚úì LABEL-SMOOTH completado:\")\n",
    "print(f\"  Test Accuracy: {test_acc_exp2:.4f}\")\n",
    "print(f\"  Macro F1: {macro_f1_exp2:.4f}\")\n",
    "print(f\"  Top-3 Acc: {top3_acc_exp2:.4f}\")\n",
    "\n",
    "# 1. Training log\n",
    "pd.DataFrame(training_log_exp2).to_csv(output_dir_exp2 / 'training_log.csv', index=False)\n",
    "print(f\"‚úì Guardado: training_log.csv\")\n",
    "\n",
    "# 2. Metrics CSV\n",
    "pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Macro-F1', 'Macro-Precision', 'Macro-Recall', 'Top-3 Accuracy', 'Test Loss'],\n",
    "    'Value': [test_acc_exp2, macro_f1_exp2, macro_precision_exp2, macro_recall_exp2, top3_acc_exp2, test_loss_exp2]\n",
    "}).to_csv(output_dir_exp2 / 'metrics.csv', index=False)\n",
    "print(f\"‚úì Guardado: metrics.csv\")\n",
    "\n",
    "# 3. Per-class metrics\n",
    "# Extract unique class names (one per class ID)\n",
    "unique_class_names_exp2 = []\n",
    "for class_id in range(num_classes):\n",
    "    idx = np.where(y == class_id)[0][0]\n",
    "    class_name = str(filenames[idx]).replace('.json', '').split('_')[0]\n",
    "    unique_class_names_exp2.append(class_name)\n",
    "\n",
    "class_report_dict_exp2 = classification_report(\n",
    "    test_labels_exp2, test_preds_exp2, \n",
    "    target_names=unique_class_names_exp2,\n",
    "    output_dict=True, zero_division=0\n",
    ")\n",
    "pd.DataFrame(class_report_dict_exp2).T.to_csv(output_dir_exp2 / 'per_class_metrics.csv')\n",
    "print(f\"‚úì Guardado: per_class_metrics.csv\")\n",
    "\n",
    "# 4. Confusion matrix CSV\n",
    "cm_exp2 = confusion_matrix(test_labels_exp2, test_preds_exp2)\n",
    "pd.DataFrame(cm_exp2).to_csv(output_dir_exp2 / 'confusion_matrix.csv', index=False, header=False)\n",
    "print(f\"‚úì Guardado: confusion_matrix.csv\")\n",
    "\n",
    "# 5. Config JSON\n",
    "config_exp2 = {\n",
    "    'experiment_type': 'label_smoothing',\n",
    "    'architecture': 'TransformerEncoderOnly',\n",
    "    'input_dim': config['input_dim'],\n",
    "    'd_model': 256,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'dim_feedforward': 1024,\n",
    "    'dropout': 0.3,\n",
    "    'num_classes': num_classes,\n",
    "    'max_seq_len': 96,\n",
    "    'use_class_weights': False,\n",
    "    'label_smoothing': 0.1,\n",
    "    'best_epoch': int(best_epoch_exp2),\n",
    "    'best_val_acc': float(best_val_acc_exp2),\n",
    "    'test_accuracy': float(test_acc_exp2),\n",
    "    'test_macro_f1': float(macro_f1_exp2),\n",
    "    'training_timestamp': datetime.now().isoformat()\n",
    "}\n",
    "with open(output_dir_exp2 / 'config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_exp2, f, indent=2, ensure_ascii=False)\n",
    "print(f\"‚úì Guardado: config.json\")\n",
    "\n",
    "# 6. Training curves PNG\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "axes[0, 0].plot(training_log_exp2['epoch'], training_log_exp2['train_loss'], 'b-', label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0, 0].plot(training_log_exp2['epoch'], training_log_exp2['val_loss'], 'r-', label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0, 0].axvline(best_epoch_exp2, color='g', linestyle='--', alpha=0.7, linewidth=2, label=f'Best Epoch {best_epoch_exp2+1}')\n",
    "axes[0, 0].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Curva de Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(training_log_exp2['epoch'], training_log_exp2['train_acc'], 'b-', label='Train Acc', marker='o', linewidth=2)\n",
    "axes[0, 1].plot(training_log_exp2['epoch'], training_log_exp2['val_acc'], 'r-', label='Val Acc', marker='s', linewidth=2)\n",
    "axes[0, 1].axvline(best_epoch_exp2, color='g', linestyle='--', alpha=0.7, linewidth=2, label=f'Best Epoch {best_epoch_exp2+1}')\n",
    "axes[0, 1].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Curva de Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(training_log_exp2['epoch'], training_log_exp2['lr'], 'g-', marker='o', linewidth=2)\n",
    "axes[1, 0].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "metrics_names = ['Accuracy', 'Macro-F1', 'Top-3 Acc']\n",
    "metrics_values = [test_acc_exp2, macro_f1_exp2, top3_acc_exp2]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "bars = axes[1, 1].bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('M√©tricas en Test Set', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylim([0, 1.05])\n",
    "for bar, v in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02, \n",
    "                    f'{v:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Curvas de Aprendizaje - LABEL-SMOOTH', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir_exp2 / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: training_curves.png\")\n",
    "plt.close()\n",
    "\n",
    "# 7. Confusion matrix PNG\n",
    "fig_cm, ax_cm = plt.subplots(figsize=(20, 18))\n",
    "sns.heatmap(cm_exp2, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Muestras'}, ax=ax_cm,\n",
    "            xticklabels=unique_class_names_exp2, yticklabels=unique_class_names_exp2, square=True)\n",
    "ax_cm.set_xlabel('Clase Predicha', fontsize=14, fontweight='bold')\n",
    "ax_cm.set_ylabel('Clase Real', fontsize=14, fontweight='bold')\n",
    "ax_cm.set_title(f'Matriz de Confusi√≥n - LABEL-SMOOTH (Accuracy: {test_acc_exp2:.4f})', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir_exp2 / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# 8. Per-class analysis PNG\n",
    "unique_classes_exp2 = sorted(list(set(test_labels_exp2)))\n",
    "precision_per_class_exp2 = []\n",
    "recall_per_class_exp2 = []\n",
    "f1_per_class_array_exp2 = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    if i in unique_classes_exp2:\n",
    "        idx = unique_classes_exp2.index(i)\n",
    "        tp = cm_exp2[idx, idx]\n",
    "        fp = cm_exp2[:, idx].sum() - tp\n",
    "        fn = cm_exp2[idx, :].sum() - tp\n",
    "        \n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_value = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "        \n",
    "        precision_per_class_exp2.append(prec)\n",
    "        recall_per_class_exp2.append(rec)\n",
    "        f1_per_class_array_exp2.append(f1_value)\n",
    "    else:\n",
    "        precision_per_class_exp2.append(0)\n",
    "        recall_per_class_exp2.append(0)\n",
    "        f1_per_class_array_exp2.append(0)\n",
    "\n",
    "precision_per_class_exp2 = np.array(precision_per_class_exp2)\n",
    "recall_per_class_exp2 = np.array(recall_per_class_exp2)\n",
    "f1_per_class_array_exp2 = np.array(f1_per_class_array_exp2)\n",
    "\n",
    "fig_pc, axes_pc = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "y_pos = np.arange(num_classes)\n",
    "axes_pc[0].barh(y_pos, precision_per_class_exp2, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "axes_pc[0].set_yticks(y_pos)\n",
    "axes_pc[0].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[0].set_xlabel('Precision', fontsize=12, fontweight='bold')\n",
    "axes_pc[0].set_title('Precision por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[0].set_yticklabels(unique_class_names_exp2, fontsize=8)\n",
    "axes_pc[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes_pc[1].barh(y_pos, recall_per_class_exp2, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "axes_pc[1].set_yticks(y_pos)\n",
    "axes_pc[1].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "axes_pc[1].set_title('Recall por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[1].set_yticklabels(unique_class_names_exp2, fontsize=8)\n",
    "axes_pc[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes_pc[2].barh(y_pos, f1_per_class_array_exp2, color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "axes_pc[2].set_yticks(y_pos)\n",
    "axes_pc[2].set_yticklabels([filenames[i] if i < len(filenames) else f'C{i}' for i in range(num_classes)], fontsize=8)\n",
    "axes_pc[2].set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes_pc[2].set_title('F1-Score por Clase', fontsize=14, fontweight='bold')\n",
    "axes_pc[2].set_yticklabels(unique_class_names_exp2, fontsize=8)\n",
    "axes_pc[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('An√°lisis por Clase - LABEL-SMOOTH', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir_exp2 / 'per_class_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úì Guardado: per_class_analysis.png\")\n",
    "plt.close()\n",
    "\n",
    "# 9. RESUMEN.txt\n",
    "f1_per_class_list_exp2 = [(i, f1_value) for i, f1_value in enumerate(f1_per_class_array_exp2)]\n",
    "class_f1_sorted_exp2 = sorted(f1_per_class_list_exp2, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "summary_exp2 = f\"\"\"\n",
    "{'='*80}\n",
    "RESUMEN EJECUTIVO - G4-RESULTS-LABEL-SMOOTH\n",
    "{'='*80}\n",
    "\n",
    "üìä PERFORMANCE:\n",
    "  ‚Ä¢ Test Accuracy:       {test_acc_exp2:.4f}\n",
    "  ‚Ä¢ Macro F1-Score:      {macro_f1_exp2:.4f}\n",
    "  ‚Ä¢ Macro Precision:     {macro_precision_exp2:.4f}\n",
    "  ‚Ä¢ Macro Recall:        {macro_recall_exp2:.4f}\n",
    "  ‚Ä¢ Top-3 Accuracy:      {top3_acc_exp2:.4f}\n",
    "\n",
    "‚öôÔ∏è CONFIGURACI√ìN:\n",
    "  ‚Ä¢ Dropout:             0.3\n",
    "  ‚Ä¢ Class Weights:       Desactivado\n",
    "  ‚Ä¢ Label Smoothing:     0.1\n",
    "  ‚Ä¢ Best Epoch:          {best_epoch_exp2}\n",
    "\n",
    "üìà TOP 5 CLASES (F1-Score):\n",
    "\"\"\"\n",
    "for rank, (class_idx, f1_value) in enumerate(class_f1_sorted_exp2[:5], 1):\n",
    "    class_name = filenames[class_idx] if class_idx < len(filenames) else f'Class {class_idx}'\n",
    "    summary_exp2 += f\"  {rank}. {class_name:20s} | F1: {f1_value:.4f}\\n\"\n",
    "\n",
    "    class_name = unique_class_names_exp2[class_idx] if class_idx < len(unique_class_names_exp2) else f'Class {class_idx}'\n",
    "\n",
    "with open(output_dir_exp2 / 'RESUMEN.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_exp2)\n",
    "print(f\"‚úì Guardado: RESUMEN.txt\")\n",
    "\n",
    "# Resultados en diccionario\n",
    "exp2_results = {\n",
    "    'experiment': 'G4-RESULTS-LABEL-SMOOTH',\n",
    "    'dropout': 0.3,\n",
    "    'class_weights': False,\n",
    "    'label_smoothing': 0.1,\n",
    "    'test_accuracy': test_acc_exp2,\n",
    "    'test_macro_f1': macro_f1_exp2,\n",
    "    'test_top3_accuracy': top3_acc_exp2,\n",
    "    'best_epoch': best_epoch_exp2\n",
    "}\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"‚úÖ EXPERIMENTO 2 (LABEL-SMOOTH) COMPLETADO\")\n",
    "print(f\"\\n{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21041520",
   "metadata": {},
   "source": [
    "## üìä Comparaci√≥n de Experimentos UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71320881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARACI√ìN DE LOS 3 EXPERIMENTOS\n",
      "================================================================================\n",
      "\n",
      "üìä Tabla Comparativa:\n",
      "              experiment  test_accuracy  test_macro_f1  test_top3_accuracy\n",
      "              G4-RESULTS       0.265896       0.092025            0.531792\n",
      "G4-RESULTS-CLASS-WEIGHTS       0.150289       0.025359            0.208092\n",
      " G4-RESULTS-LABEL-SMOOTH       0.260116       0.078113            0.456647\n",
      "\n",
      "üìà An√°lisis:\n",
      "  Mejor Macro-F1: G4-RESULTS\n",
      "  Mejora Exp1 vs Baseline: -6.67%\n",
      "  Mejora Exp2 vs Baseline: -1.39%\n",
      "\n",
      "‚úì Guardado: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\\experiments_comparison.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ COMPARACI√ìN COMPLETADA\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# COMPARACI√ìN DE LOS 3 EXPERIMENTOS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARACI√ìN DE LOS 3 EXPERIMENTOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# DataFrame comparativo\n",
    "all_results = [exp0_results, exp1_results, exp2_results]\n",
    "df_comparison = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\nüìä Tabla Comparativa:\")\n",
    "print(df_comparison[['experiment', 'test_accuracy', 'test_macro_f1', 'test_top3_accuracy']].to_string(index=False))\n",
    "\n",
    "# An√°lisis de mejoras\n",
    "best_f1_idx = df_comparison['test_macro_f1'].idxmax()\n",
    "best_f1_exp = df_comparison.loc[best_f1_idx, 'experiment']\n",
    "\n",
    "improvement_exp1 = (exp1_results['test_macro_f1'] - exp0_results['test_macro_f1']) * 100\n",
    "improvement_exp2 = (exp2_results['test_macro_f1'] - exp0_results['test_macro_f1']) * 100\n",
    "\n",
    "print(f\"\\nüìà An√°lisis:\")\n",
    "print(f\"  Mejor Macro-F1: {best_f1_exp}\")\n",
    "print(f\"  Mejora Exp1 vs Baseline: {improvement_exp1:+.2f}%\")\n",
    "print(f\"  Mejora Exp2 vs Baseline: {improvement_exp2:+.2f}%\")\n",
    "\n",
    "# Guardar comparaci√≥n CSV\n",
    "comparison_csv_path = ROOT_PATH / 'experiments_comparison.csv'\n",
    "df_comparison.to_csv(comparison_csv_path, index=False)\n",
    "print(f\"\\n‚úì Guardado: {comparison_csv_path}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ COMPARACI√ìN COMPLETADA\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9b418",
   "metadata": {},
   "source": [
    "## üîç Verificaci√≥n Final de Archivos Generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9f99bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç VERIFICACI√ìN DE ARCHIVOS GENERADOS\n",
      "================================================================================\n",
      "\n",
      "üìÇ ROOT_PATH: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\n",
      "üìù Tipo de dataset: UMAP Embeddings\n",
      "\n",
      "üìÇ G4-RESULTS:\n",
      "  ‚ùå best_model.pt                  FALTA\n",
      "  ‚úÖ config.json                    (507 bytes)\n",
      "  ‚úÖ training_log.csv               (4,601 bytes)\n",
      "  ‚úÖ metrics.csv                    (209 bytes)\n",
      "  ‚úÖ per_class_metrics.csv          (1,691 bytes)\n",
      "  ‚úÖ confusion_matrix.csv           (1,833 bytes)\n",
      "  ‚úÖ confusion_matrix.png           (675,717 bytes)\n",
      "  ‚úÖ training_curves.png            (596,963 bytes)\n",
      "  ‚úÖ per_class_analysis.png         (400,340 bytes)\n",
      "  ‚úÖ RESUMEN.txt                    (1,641 bytes)\n",
      "\n",
      "üìÇ G4-RESULTS-CLASS-WEIGHTS:\n",
      "  ‚úÖ best_model.pt                  (19,541,557 bytes)\n",
      "  ‚úÖ config.json                    (511 bytes)\n",
      "  ‚úÖ training_log.csv               (1,622 bytes)\n",
      "  ‚úÖ metrics.csv                    (212 bytes)\n",
      "  ‚úÖ per_class_metrics.csv          (1,411 bytes)\n",
      "  ‚úÖ confusion_matrix.csv           (1,834 bytes)\n",
      "  ‚úÖ confusion_matrix.png           (674,280 bytes)\n",
      "  ‚úÖ training_curves.png            (474,058 bytes)\n",
      "  ‚úÖ per_class_analysis.png         (397,655 bytes)\n",
      "  ‚úÖ RESUMEN.txt                    (903 bytes)\n",
      "\n",
      "üìÇ G4-RESULTS-LABEL-SMOOTH:\n",
      "  ‚úÖ best_model.pt                  (19,541,557 bytes)\n",
      "  ‚úÖ config.json                    (513 bytes)\n",
      "  ‚úÖ training_log.csv               (3,090 bytes)\n",
      "  ‚úÖ metrics.csv                    (208 bytes)\n",
      "  ‚úÖ per_class_metrics.csv          (1,568 bytes)\n",
      "  ‚úÖ confusion_matrix.csv           (1,833 bytes)\n",
      "  ‚úÖ confusion_matrix.png           (818,808 bytes)\n",
      "  ‚úÖ training_curves.png            (496,794 bytes)\n",
      "  ‚úÖ per_class_analysis.png         (354,197 bytes)\n",
      "  ‚úÖ RESUMEN.txt                    (811 bytes)\n",
      "\n",
      "üìÇ Archivos de comparaci√≥n en ROOT_PATH:\n",
      "  ‚úÖ experiments_comparison.csv     (464 bytes)\n",
      "  ‚ùå experiments_comparison.png     FALTA\n",
      "\n",
      "================================================================================\n",
      "‚ö†Ô∏è VERIFICACI√ìN INCOMPLETA - Faltan algunos archivos:\n",
      "  - G4-RESULTS/best_model.pt\n",
      "  - ROOT/experiments_comparison.png\n",
      "================================================================================\n",
      "\n",
      "üìä Resumen:\n",
      "  ‚Ä¢ Experimentos: 3\n",
      "  ‚Ä¢ Archivos por experimento: 10\n",
      "  ‚Ä¢ Archivos de comparaci√≥n: 2\n",
      "  ‚Ä¢ Total archivos requeridos: 32\n",
      "  ‚Ä¢ ROOT_PATH: C:\\Users\\Los milluelitos repo\\Desktop\\experimento tesis\\transformer-asl-classification\\G4-EMBEDDING FRAME A FRAME UMAP\n"
     ]
    }
   ],
   "source": [
    "# VERIFICACI√ìN DE ARCHIVOS GENERADOS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç VERIFICACI√ìN DE ARCHIVOS GENERADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìÇ ROOT_PATH: {ROOT_PATH}\")\n",
    "print(f\"üìù Tipo de dataset: UMAP Embeddings\")\n",
    "\n",
    "# Archivos requeridos por experimento\n",
    "REQUIRED_FILES_PER_EXPERIMENT = [\n",
    "    'best_model.pt',\n",
    "    'config.json',\n",
    "    'training_log.csv',\n",
    "    'metrics.csv',\n",
    "    'per_class_metrics.csv',\n",
    "    'confusion_matrix.csv',\n",
    "    'confusion_matrix.png',\n",
    "    'training_curves.png',\n",
    "    'per_class_analysis.png',\n",
    "    'RESUMEN.txt'\n",
    "]\n",
    "\n",
    "# Archivos de comparaci√≥n en ROOT_PATH\n",
    "REQUIRED_FILES_BASE = [\n",
    "    'experiments_comparison.csv',\n",
    "    'experiments_comparison.png'\n",
    "]\n",
    "\n",
    "# Carpetas de experimentos\n",
    "experiment_folders = [\n",
    "    'G4-RESULTS',\n",
    "    'G4-RESULTS-CLASS-WEIGHTS',\n",
    "    'G4-RESULTS-LABEL-SMOOTH'\n",
    "]\n",
    "\n",
    "# Verificar cada experimento\n",
    "all_valid = True\n",
    "missing_files = []\n",
    "\n",
    "for folder_name in experiment_folders:\n",
    "    folder_path = ROOT_PATH / folder_name\n",
    "    print(f\"\\nüìÇ {folder_name}:\")\n",
    "    \n",
    "    if not folder_path.exists():\n",
    "        print(f\"  ‚ùå Carpeta no existe\")\n",
    "        all_valid = False\n",
    "        continue\n",
    "    \n",
    "    for required_file in REQUIRED_FILES_PER_EXPERIMENT:\n",
    "        file_path = folder_path / required_file\n",
    "        if file_path.exists():\n",
    "            file_size = file_path.stat().st_size\n",
    "            print(f\"  ‚úÖ {required_file:30s} ({file_size:,} bytes)\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {required_file:30s} FALTA\")\n",
    "            missing_files.append(f\"{folder_name}/{required_file}\")\n",
    "            all_valid = False\n",
    "\n",
    "# Verificar archivos de comparaci√≥n\n",
    "print(f\"\\nüìÇ Archivos de comparaci√≥n en ROOT_PATH:\")\n",
    "for required_file in REQUIRED_FILES_BASE:\n",
    "    file_path = ROOT_PATH / required_file\n",
    "    if file_path.exists():\n",
    "        file_size = file_path.stat().st_size\n",
    "        print(f\"  ‚úÖ {required_file:30s} ({file_size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {required_file:30s} FALTA\")\n",
    "        missing_files.append(f\"ROOT/{required_file}\")\n",
    "        all_valid = False\n",
    "\n",
    "# Resumen final\n",
    "print(f\"\\n{'='*80}\")\n",
    "if all_valid:\n",
    "    print(\"‚úÖ VERIFICACI√ìN EXITOSA - Todos los archivos se han generado correctamente\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è VERIFICACI√ìN INCOMPLETA - Faltan algunos archivos:\")\n",
    "    for missing in missing_files:\n",
    "        print(f\"  - {missing}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Estad√≠sticas\n",
    "total_required = len(experiment_folders) * len(REQUIRED_FILES_PER_EXPERIMENT) + len(REQUIRED_FILES_BASE)\n",
    "print(f\"\\nüìä Resumen:\")\n",
    "print(f\"  ‚Ä¢ Experimentos: {len(experiment_folders)}\")\n",
    "print(f\"  ‚Ä¢ Archivos por experimento: {len(REQUIRED_FILES_PER_EXPERIMENT)}\")\n",
    "print(f\"  ‚Ä¢ Archivos de comparaci√≥n: {len(REQUIRED_FILES_BASE)}\")\n",
    "print(f\"  ‚Ä¢ Total archivos requeridos: {total_required}\")\n",
    "print(f\"  ‚Ä¢ ROOT_PATH: {ROOT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
